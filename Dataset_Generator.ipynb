{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVnwUkYAl2S5",
        "outputId": "025e3c7d-35cc-4cc0-cd0e-f251b1a89c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fitz in /home/yasiru/anaconda3/lib/python3.12/site-packages (0.0.1.dev2)\n",
            "Requirement already satisfied: configobj in /home/yasiru/anaconda3/lib/python3.12/site-packages (from fitz) (5.0.9)\n",
            "Requirement already satisfied: configparser in /home/yasiru/anaconda3/lib/python3.12/site-packages (from fitz) (7.2.0)\n",
            "Requirement already satisfied: httplib2 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /home/yasiru/anaconda3/lib/python3.12/site-packages (from fitz) (5.3.2)\n",
            "Requirement already satisfied: nipype in /home/yasiru/anaconda3/lib/python3.12/site-packages (from fitz) (1.9.2)\n",
            "Requirement already satisfied: numpy in /home/yasiru/anaconda3/lib/python3.12/site-packages (from fitz) (1.26.4)\n",
            "Requirement already satisfied: pandas in /home/yasiru/anaconda3/lib/python3.12/site-packages (from fitz) (2.2.2)\n",
            "Requirement already satisfied: pyxnat in /home/yasiru/anaconda3/lib/python3.12/site-packages (from fitz) (1.6.3)\n",
            "Requirement already satisfied: scipy in /home/yasiru/anaconda3/lib/python3.12/site-packages (from fitz) (1.13.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from httplib2->fitz) (3.1.2)\n",
            "Requirement already satisfied: packaging>=20 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nibabel->fitz) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nibabel->fitz) (4.12.2)\n",
            "Requirement already satisfied: click>=6.6.0 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nipype->fitz) (8.1.8)\n",
            "Requirement already satisfied: networkx>=2.5 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nipype->fitz) (3.3)\n",
            "Requirement already satisfied: prov>=1.5.2 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nipype->fitz) (2.0.1)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nipype->fitz) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nipype->fitz) (2.9.0.post0)\n",
            "Requirement already satisfied: rdflib>=5.0.0 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nipype->fitz) (6.3.2)\n",
            "Requirement already satisfied: simplejson>=3.8.0 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nipype->fitz) (3.20.1)\n",
            "Requirement already satisfied: traits>=6.2 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nipype->fitz) (7.0.2)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nipype->fitz) (3.13.1)\n",
            "Requirement already satisfied: acres in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: etelemetry>=0.3.1 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nipype->fitz) (0.3.1)\n",
            "Requirement already satisfied: looseversion!=1.2 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nipype->fitz) (1.3.0)\n",
            "Requirement already satisfied: puremagic in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nipype->fitz) (1.28)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from pandas->fitz) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from pandas->fitz) (2023.3)\n",
            "Requirement already satisfied: lxml>=4.3 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from pyxnat->fitz) (5.3.0)\n",
            "Requirement already satisfied: requests>=2.20 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from pyxnat->fitz) (2.32.3)\n",
            "Requirement already satisfied: pathlib>=1.0 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: ci-info>=0.2 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from etelemetry>=0.3.1->nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.2->nipype->fitz) (1.16.0)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from requests>=2.20->pyxnat->fitz) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from requests>=2.20->pyxnat->fitz) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from requests>=2.20->pyxnat->fitz) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from requests>=2.20->pyxnat->fitz) (2024.8.30)\n",
            "Requirement already satisfied: pymupdf in /home/yasiru/anaconda3/lib/python3.12/site-packages (1.25.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install fitz\n",
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "t1kkhIFFnZ_s"
      },
      "outputs": [],
      "source": [
        "import fitz  \n",
        "import glob\n",
        "import fitz  \n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import markdown\n",
        "import re\n",
        "# Extract text from PDFs, using OCR if necessary\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        page_text = page.get_text(\"text\")\n",
        "        if page_text.strip():  \n",
        "            text += page_text + \"\\n\"\n",
        "        else:  \n",
        "            images = convert_from_path(pdf_path)  # OCR fallback\n",
        "            for image in images:\n",
        "                text += pytesseract.image_to_string(image) + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "def extract_text_from_md(md_path):\n",
        "    with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "    text = markdown.markdown(content)\n",
        "    # Replace unwanted characters (apply same replacements as PDF)\n",
        "    text = re.sub(r\"</?ol>\", \"\", text)\n",
        "    text = re.sub(r\"</?p>\", \"\", text)\n",
        "    text = re.sub(r\"<h1>.*?</h1>\", \"\", text)\n",
        "    return text\n",
        "\n",
        "# Load all PDFs and Markdown files\n",
        "pdf_files = glob.glob(\"data/*.pdf\")\n",
        "md_files = glob.glob(\"data/*.md\")\n",
        "\n",
        "documents = []\n",
        "for file in pdf_files:\n",
        "    documents.append(extract_text_from_pdf(file))\n",
        "for file in md_files:\n",
        "    documents.append(extract_text_from_md(file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPxG9Reun9H9",
        "outputId": "cd03c5d0-dc87-4a1e-dfe0-341e09d2e678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /home/yasiru/anaconda3/lib/python3.12/site-packages (3.9.1)\n",
            "Requirement already satisfied: click in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /home/yasiru/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TKg4duY9oIno"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /home/yasiru/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download the Punkt tokenizer model for sentence tokenization\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Define the function to split documents into smaller chunks\n",
        "def split_documents(documents, max_length=256):\n",
        "    chunks = []\n",
        "    for doc in documents:\n",
        "        sentences = sent_tokenize(doc)  # Tokenize the document into sentences\n",
        "        chunk = \"\"\n",
        "        for sentence in sentences:\n",
        "            if len(chunk) + len(sentence) <= max_length:\n",
        "                chunk += sentence + \" \"\n",
        "            else:\n",
        "                chunks.append(chunk.strip())\n",
        "                chunk = sentence + \" \"\n",
        "        if chunk:\n",
        "            chunks.append(chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "def preprocess_chunk(chunk):\n",
        "    chunk = chunk.strip()                    \n",
        "    chunk = chunk.replace(\"\\n\", \" \")    \n",
        "    return chunk\n",
        "\n",
        "chunks = split_documents(documents)\n",
        "\n",
        "#for i, chunk in enumerate(chunks):\n",
        "#    print(f\"Chunk {i + 1}: {chunk}\")\n",
        "\n",
        "chunks = [preprocess_chunk(chunk) for chunk in chunks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Questions and Answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TUAjGnMsp-eW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/yasiru/anaconda3/lib/python3.12/site-packages (4.49.0)\n",
            "Requirement already satisfied: filelock in /home/yasiru/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from transformers) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /home/yasiru/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/yasiru/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HIsWJMAQquNU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: What are the first-generation reasoning models?\n",
            "A: DeepSeek-R1-Zero and DeepSeek-R1\n",
            "Q: What is the model trained via large-scale reinforcement learning?\n",
            "A: DeepSeek-R1-Zero\n",
            "Q: What does DeepSeek-R1-Zero naturally emerge with?\n",
            "A: powerful and intriguing reasoning behaviors\n",
            "Q: What does DeepSeek-R1 do?\n",
            "A: incorporates multi-stage training and cold-start data before RL\n",
            "Q: What are the names of the models that we open-source?\n",
            "A: DeepSeek-R1-Zero, DeepSeek-R1\n",
            "Q: What is the percentage of accuracy of DeepSeek-R1?\n",
            "A: 42.0\n",
            "Q: What is the name of the cs.CL database?\n",
            "A: arXiv:2501.12948v1\n",
            "Q: What is the base model of DeepSeek?\n",
            "A: Reinforcement Learning on the Base Model\n",
            "Q: What is the name of the training template?\n",
            "A: Template\n",
            "Q: What are the main topics of DeepSeek-R1-Zero?\n",
            "A: Self-evolution Process\n",
            "Q: What is the name of the program that is used to teach Reinforcement Learning?\n",
            "A: Rejection Sampling and Supervised Fine-Tuning\n",
            "Q: What is the name of the experiment?\n",
            "A: DeepSeek-R1 Evaluation\n",
            "Q: What is the difference between Distillation and Reinforcement Learning?\n",
            "A: 14 4 Discussion 14 4.1\n",
            "Q: What are the main points of the work?\n",
            "A: A Contributions and Acknowledgments 20 2  1\n",
            "Q: What are the most common questions that can be asked from LLMs?\n",
            "A: Large Language Models\n",
            "Q: What are the most common questions that are asked after training?\n",
            "A: post-training\n",
            "Q: What does the software generate?\n",
            "A: accuracy\n",
            "Q: What did OpenAI generate?\n",
            "A: inference-time scaling\n",
            "Q: What is the most common question that researchers can ask?\n",
            "A: test-time scaling\n",
            "Q: What are some of the approaches that have been explored?\n",
            "A: process-based reward models\n",
            "Q: What is the first step toward improving language model reasoning capabilities?\n",
            "A: pure reinforcement learning (RL)\n",
            "Q: What are the questions that we want to ask?\n",
            "A: their self-evolution\n",
            "Q: What is the base model used for?\n",
            "A: DeepSeek-V3-Base\n",
            "Q: What did DeepSeek-R1-Zero generate?\n",
            "A: reasoning behaviors\n",
            "Q: What is the pass@1 score on AIME 2024?\n",
            "A: 15.6% to 71.0%\n",
            "Q: What is the main feature of DeepSeek-R1-Zero?\n",
            "A: poor readability, and language mixing\n",
            "Q: What is the name of the DeepSeek-R1?\n",
            "A: DeepSeek-R1\n",
            "Q: What do we collect to refine DeepSeek-V3-Base model?\n",
            "A: thousands of cold-start data\n",
            "Q: What do we generate questions from?\n",
            "A: DeepSeek-V3\n",
            "Q: What are the new data generated from?\n",
            "A: all scenarios\n",
            "Q: What did we generate from DeepSeek-R1?\n",
            "A: distillation\n",
            "Q: What is the base model for DeepSeek-R1?\n",
            "A: Qwen2.5- 32B\n",
            "Q: What are the Qwen and Llama series of books based on?\n",
            "A: distilled\n",
            "Q: What did the distilled 14B model outperform the open-source QwQ-32\n",
            "A: open-source QwQ-32B-Preview\n",
            "Q: What are the questions that you can ask from the post-training?\n",
            "A: supervised fine-tuning\n",
            "Q: What is the model able to generate questions from?\n",
            "A: chain-of-thought (CoT)\n",
            "Q: DeepSeek- R1-Zero demonstrates capabilities such as self-verification,\n",
            "A: DeepSeek- R1-Zero\n",
            "Q: What is the first open research to validate that reasoning capabilities of LLMs can be incen\n",
            "A: RL\n",
            "Q: • We introduce our pipeline to develop DeepSeek-R1:\n",
            "A: We introduce our pipeline to develop DeepSeek-R1\n",
            "Q: What does the pipeline generate?\n",
            "A: two RL stages\n",
            "Q: What do you think the pipeline will benefit the industry by creating better models?\n",
            "A: the pipeline will benefit the industry by creating better models\n",
            "Q: What do we demonstrate that larger models can be distilled into smaller models?\n",
            "A: reasoning patterns\n",
            "Q: What will DeepSeek-R1 help the research community to distill better smaller models?\n",
            "A: open source\n",
            "Q: What did DeepSeek-R1 generate?\n",
            "A: reasoning data\n",
            "Q: What does DeepSeek-R1-Distill-Qwen-7B score on\n",
            "A: 55.5%\n",
            "Q: What did we open-source to the community?\n",
            "A: 1.2\n",
            "Q: What are the questions that DeepSeek-R1 generate?\n",
            "A: OpenAI-o1-1217\n",
            "Q: What questions does MATH-500 generate?\n",
            "A: significantly outperforming other models\n",
            "Q: DeepSeek-R1 demonstrates expert level in code competition tasks.\n",
            "A: (2) On coding-related tasks\n",
            "Q: DeepSeek-R1 performs slightly better than DeepSeek-V3 for engineering tasks\n",
            "A: DeepSeek-V3\n",
            "Q: What does DeepSeek- R1 achieve outstanding results on?\n",
            "A: benchmarks\n",
            "Q: What is the name of the OpenAI-o1-1217?\n",
            "A: DeepSeek-R1\n",
            "Q: DeepSeek-R1 outperforms DeepSeek-V3 on the factual\n",
            "A: SimpleQA\n",
            "Q: What is the most common question that DeepSeek-R1 can generate?\n",
            "A: general question answering\n",
            "Q: What is the best way to generate questions from?\n",
            "A: non-exam-oriented queries\n",
            "Q: What does DeepSeek-R1 do well on long-context benchmarks?\n",
            "A: outstanding performance\n",
            "Q: What are the questions that you can generate from?\n",
            "A: supervised data\n",
            "Q: What are the most common questions that can be asked?\n",
            "A: reasoning capabilities\n",
            "Q: What can be further enhanced with the inclusion of a small amount of cold-start data?\n",
            "A: performance\n",
            "Q: What does DeepSeek-R1-Zero do?\n",
            "A: applies RL directly to the base model without any SFT data\n",
            "Q: How do you generate questions from DeepSeek-R1?\n",
            "A: Distill the reasoning capability\n",
            "Q: What is the DeepSeek-R1-Zero?\n",
            "A: Reinforcement Learning\n",
            "Q: What kind of data are supervised works able to generate?\n",
            "A: supervised data, which are time-intensive\n",
            "Q: What are the questions that we ask in this section?\n",
            "A: the potential of LLMs to develop reasoning capabilities without any supervised data\n",
            "Q: What are the first questions that the community can ask?\n",
            "A: RL algorithm\n",
            "Q: What is the GRPO?\n",
            "A: Group Relative Policy Optimization\n",
            "Q: What does GRPO generate for each question q?\n",
            "A: a group of outputs\n",
            "Q: What is the output of each group?\n",
            "A: 𝐴𝑖= 𝑟𝑖−m𝑒𝑎𝑛\n",
            "Q: What does the assistant generate?\n",
            "A: the answer\n",
            "Q: What are the reasoning process and answer enclosed within?\n",
            "A: </think> and <answer> </answer> tags\n",
            "Q: What will be replaced with a specific reasoning question during training?\n",
            "A: prompt\n",
            "Q: What are the rewards for DeepSeek-R1-Zero?\n",
            "A: Accuracy rewards\n",
            "Q: What is the model required to provide the final answer in a specified format?\n",
            "A: within a box\n",
            "Q: What can a compiler generate questions from?\n",
            "A: predefined test cases\n",
            "Q: What does the format reward model enforce?\n",
            "A: to put its thinking process\n",
            "Q: What do we not use in developing DeepSeek-R1-Zero?\n",
            "A: the outcome or process neural reward model\n",
            "Q: What are the questions that you can generate from?\n",
            "A: specified instructions\n",
            "Q: What does DeepSeek-R1-Zero generate questions from?\n",
            "A: reasoning process, followed by the final answer\n",
            "Q: What are the constraints of the model?\n",
            "A: mandating reflective reasoning or promoting particular problem-solving strate- gies\n",
            "Q: What are the questions that you can generate from the wiki?\n",
            "A: Performance, Self-evolution Process\n",
            "Q: What is the DeepSeek-R1-Zero training program?\n",
            "A: a steady and consistent enhancement in performance\n",
            "Q: What is the average pass@1 score on AIME 2024?\n",
            "A: 15.6%\n",
            "Q: What is the most important thing to do with the RL algorithm?\n",
            "A: optimizing the model’s performance over time\n",
            "Q: What are the questions generated from DeepSeek-R1-Zero?\n",
            "A: reasoning-related benchmarks\n",
            "Q: What did the findings reveal about RL empower?\n",
            "A: 6  Model AIME\n",
            "Q: How accurate is DeepSeek-R1-Zo?\n",
            "A: AIME\n",
            "Q: What did DeepSeek-R1-Zero achieve?\n",
            "A: robust reasoning capabilities\n",
            "Q: What can be generated from the DeepSeek- R1-Zero?\n",
            "A: majority voting\n",
            "Q: What is the most common question that DeepSeek-R1-Zero gets asked?\n",
            "A: 86.7%\n",
            "Q: What is the name of the game that DeepSeek-R1-Zero can generate\n",
            "A: DeepSeek-R1-Zero\n",
            "Q: What is the self-evolution process of DeepSeek-R1-Zero?\n",
            "A: RL can drive a model to improve its reasoning capabilities autonomously\n",
            "Q: What can we use to initiate RL directly from the base model?\n",
            "A: supervised fine-tuning stage\n",
            "Q: What does the model generate?\n",
            "A: complex reasoning tasks\n",
            "Q: What is the thinking time of DeepSeek-R1-Zero?\n",
            "A: Figure 3\n",
            "Q: What does DeepSeek-R1-Zero naturally learn to solve reasoning tasks with more\n",
            "A: thinking time\n",
            "Q: What does DeepSeek-R1-Zero generate questions from?\n",
            "A: test-time compu- tation\n",
            "Q: What can the model generate?\n",
            "A: reasoning tokens\n",
            "Q: What are the most remarkable aspects of self-evolution?\n",
            "A: the emergence of sophisticated behaviors\n",
            "Q: What do you generate questions from?\n",
            "A: reflection\n",
            "Q: What do the models interact with the reinforcement learning environment?\n",
            "A: behaviors\n",
            "Q: What does the spontaneous development of DeepSeek-R1-Zero allow for?\n",
            "A: to tackle more challenging tasks with greater efficiency and accuracy\n",
            "Q: What is the Aha Moment of DeepSeek-R1-Zero?\n",
            "A: the occurrence of an “aha moment”\n",
            "Q: What does DeepSeek-R1-Zero learn to allocate more thinking time to \n",
            "A: by reevaluating its initial approach\n",
            "Q: What is the model's reasoning ability?\n",
            "A: growing\n",
            "Q: What are the researchers looking at in the moment?\n",
            "A: its behavior\n",
            "Q: What is the most powerful aspect of reinforcement learning?\n",
            "A: beauty\n",
            "Q: What is the aha moment?\n",
            "A: a powerful reminder of the potential of RL to unlock new levels of intelligence\n",
            "Q: What are the real solutions of a a+ x= x equal to\n",
            "A: Response\n",
            "Q: What is the inner square root term?\n",
            "A: 2\n",
            "Q: What is the correct sum?\n",
            "A: · · ·\n",
            "Q: What is the model's answer to the aha moment?\n",
            "A: allowing us to witness the power and beauty of reinforcement learning\n",
            "Q: What is the drawback of DeepSeek-R1-Zero?\n",
            "A: strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors\n",
            "Q: What is the name of the application that struggles with language mixing?\n",
            "A: DeepSeek-R1-Zero\n",
            "Q: What are the questions that DeepSeek-R1 uses?\n",
            "A: human-friendly cold-start data\n",
            "Q: What is the name of the DeepSeek-R1?\n",
            "A: Reinforcement Learning with Cold Start\n",
            "Q: What are the main questions we want to answer?\n",
            "A: DeepSeek-R1\n",
            "Q: What is the first step of the pipeline?\n",
            "A: 2.3.1\n",
            "Q: What are the cold start questions?\n",
            "A: to prevent the early unstable cold start phase of RL training\n",
            "Q: How do we collect data from DeepSeek?\n",
            "A: in a readable format\n",
            "Q: What is the starting point for RL?\n",
            "A: DeepSeek-V3-Base\n",
            "Q: What are the advantages of cold start data 9?\n",
            "A: its content is often not suitable for reading\n",
            "Q: What can users use to generate questions?\n",
            "A: Responses may mix multiple languages or lack markdown formatting\n",
            "Q: What is the name of the data generated by DeepSeek-R1?\n",
            "A: cold-start data\n",
            "Q: What is the output format of the query?\n",
            "A: |special_token\n",
            "Q: What is the potential of DeepSeek-R1-Zero?\n",
            "A: better performance\n",
            "Q: What is the purpose of DeepSeek-V3-Base?\n",
            "A: apply the same large-scale reinforcement learning training process\n",
            "Q: What is the first phase of the model?\n",
            "A: reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning\n",
            "Q: What do we use to train CoT?\n",
            "A: language mixing\n",
            "Q: What are the language consistency rewards?\n",
            "A: the proportion of target language words in the CoT\n",
            "Q: What is the reward for a human's ablation?\n",
            "A: preferences\n",
            "Q: What do we generate questions from?\n",
            "A: accuracy of reasoning tasks and the reward for language consistency\n",
            "Q: What do we use to collect SFT data for the subsequent round?\n",
            "A: checkpoint\n",
            "Q: What does the model incorporate data from other domains?\n",
            "A: initial cold-start data\n",
            "Q: What do we generate questions from?\n",
            "A: the data\n",
            "Q: What did we use to generate questions?\n",
            "A: rule-based rewards\n",
            "Q: What is the dataset used to generate questions from?\n",
            "A: DeepSeek-V3\n",
            "Q: What are the most common questions in the model?\n",
            "A: the correct ones\n",
            "Q: How many training samples are collected?\n",
            "A: 600k\n",
            "Q: What is the SFT dataset used for?\n",
            "A: DeepSeek-V3\n",
            "Q: What does DeepSeek-V3 generate questions from?\n",
            "A: a potential chain-of-thought\n",
            "Q: What did we collect?\n",
            "A: a total of approximately 200k training samples\n",
            "Q: What is the primary reinforcement learning stage?\n",
            "A: improving the model’s helpfulness and harmlessness\n",
            "Q: What do we train the model using?\n",
            "A: a combination of reward signals and diverse prompt distributions\n",
            "Q: What do we use to generate questions?\n",
            "A: rule-based rewards\n",
            "Q: What do we use to generate questions from?\n",
            "A: DeepSeek-V3 pipeline\n",
            "Q: What do we use to determine the usefulness of the final summary?\n",
            "A: assessment\n",
            "Q: What is the model's harmlessness?\n",
            "A: to identify and mitigate any potential risks, biases, or harmful content\n",
            "Q: What do you want to learn about the model?\n",
            "A: reasoning\n",
            "Q: What did DeepSeek-R1 enable?\n",
            "A: reasoning capabilities\n",
            "Q: What did the distillation method help to generate?\n",
            "A: reasoning abilities\n",
            "Q: What are the base models we use?\n",
            "A: Qwen2.5-Math-1.5B\n",
            "Q: What do we use for distilled models?\n",
            "A: SFT\n",
            "Q: What are the primary goals of the distillation technique?\n",
            "A: to demonstrate the effectiveness\n",
            "Q: What are the benchmarks for the MMLU?\n",
            "A: Experiment Benchmarks\n",
            "Q: What do we use as judges?\n",
            "A: LLMs\n",
            "Q: What are the original configurations of AlpacaEval 2.0 and Arena-Hard?\n",
            "A: Li et al., 2024\n",
            "Q: What are the most common questions that are used in this section?\n",
            "A: AIME 2024, MATH-500, GPQA Diamond\n",
            "Q: What are the benchmarks used to evaluate?\n",
            "A: GPQA Diamond, and SimpleQA\n",
            "Q: What are the first questions that are generated by MMLU-Redux?\n",
            "A: the original prompts are few-shot\n",
            "Q: What is the CoT in few-shot that may hurt the performance of DeepSeek-R\n",
            "A: DeepSeek-R1\n",
            "Q: What is the HumanEval-Mul dataset?\n",
            "A: eight mainstream programming languages\n",
            "Q: What are the most common questions that are generated from the model?\n",
            "A: Model performance\n",
            "Q: What is the Codeforces dataset used for?\n",
            "A: evaluated using problems from 10 Div.2 contests\n",
            "Q: What are the questions generated from?\n",
            "A: agentless framework\n",
            "Q: What are the most common questions that are generated from the DeepSeek-V3?\n",
            "A: Claude-Sonnet-3.5-1022\n",
            "Q: What are the questions that we generate from the OpenAI-o1-1217 API?\n",
            "A: perfor- mance based on official reports\n",
            "Q: How long is the maximum generation length?\n",
            "A: 32,768 tokens\n",
            "Q: What is the default value for pass@kevaluation?\n",
            "A: 𝑘evaluation\n",
            "Q: What temperature is used to generate k responses for each question?\n",
            "A: sampling temperature of 0.6 and a top-𝑝value of 0.95\n",
            "Q: What is the pass@1 calculated as?\n",
            "A: pass@1 = 1\n",
            "Q: What are the results of the consensus vote for AIME 2024?\n",
            "A: 64 samples\n",
            "Q: What is the name of the test that is used to determine the performance of DeepSeek-R\n",
            "A: Evaluation\n",
            "Q: What is the name of the game that generates questions from?\n",
            "A: Aider-Polyglot\n",
            "Q: What is the name of the test that is generated from the questions in the test?\n",
            "A: Math AIME 2024\n",
            "Q: What does DeepSeek-R1 demonstrate superior performance against?\n",
            "A: DeepSeek-V3\n",
            "Q: What is the most important question that can be generated?\n",
            "A: STEM\n",
            "Q: DeepSeek-R1 excels on FRAMES, a long-context-\n",
            "A: DeepSeek-R1 excels on FRAMES\n",
            "Q: DeepSeek-R1 outperforms DeepSeek-V3 on the factual\n",
            "A: SimpleQA\n",
            "Q: What is the Chinese SimpleQA benchmark?\n",
            "A: DeepSeek-V3\n",
            "Q: What does DeepSeek-R1 generate questions from?\n",
            "A: IF-Eval\n",
            "Q: What can be linked to the inclusion of instruction-following data during the final stages of supervised\n",
            "A: improvements\n",
            "Q: DeepSeek-R1 is able to write tasks and open-domain question answering.\n",
            "A: AlpacaEval2.0 and ArenaHard\n",
            "Q: What is the name of the application that is able to generate questions from?\n",
            "A: DeepSeek-V3\n",
            "Q: What is the average summary length on ArenaHard?\n",
            "A: 689 tokens\n",
            "Q: What does 13 DeepSeek-R1 avoid introducing length bias during GPT-based evaluation\n",
            "A: further solidifying its robustness across multiple tasks\n",
            "Q: DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217 on\n",
            "A: On math tasks\n",
            "Q: What do the benchmarks LiveCodeBench and Codeforces use?\n",
            "A: reasoning-focused models\n",
            "Q: What does OpenAI-o1-1217 outperform DeepSeek-R1 on Aid\n",
            "A: Aider\n",
            "Q: What are the most common questions that DeepSeek-R1 can ask?\n",
            "A: RL training data\n",
            "Q: What is the name of the model that is used to evaluate a distilled model?\n",
            "A: Comparison of DeepSeek-R1\n",
            "Q: What does the DeepSeek-R1-7B outperform other models?\n",
            "A: GPT-4o-0513\n",
            "Q: What does DeepSeek-R1-14B surpass on all evaluation metrics?\n",
            "A: QwQ-32B- Preview\n",
            "Q: What did we find about applying RL to the distilled models?\n",
            "A: significant further gains\n",
            "Q: What is the name of the model that can be used to help us learn?\n",
            "A: DeepSeek-R1\n",
            "Q: What is the first question that can be asked about the model?\n",
            "A: can the model achieve comparable performance\n",
            "Q: What is the deep-seek-r1-zero-Qwen-32B\n",
            "A: DeepSeek-R1-Zero-Qwen-32B\n",
            "Q: What is the 32B base model?\n",
            "A: 14  Model AIME 2024\n",
            "Q: What is the name of the test that results in the best performance?\n",
            "A: RL training\n",
            "Q: What are the results of distilling more powerful models into smaller ones?\n",
            "A: excellent results\n",
            "Q: What are the most important questions to ask?\n",
            "A: distillation strategies are both economical and effective\n",
            "Q: What did DeepSeek-R1 fail to generate?\n",
            "A: Unsuccessful Attempts\n",
            "Q: What are the failures of the approaches discussed in this article?\n",
            "A: incapable of developing effective reasoning models\n",
            "Q: What is the PRM?\n",
            "A: Process Reward Model\n",
            "Q: What is the first limitation of PRM?\n",
            "A: it is challenging to explicitly define a fine-grain step in general reasoning\n",
            "Q: What are the first two steps of an annotation?\n",
            "A: current intermediate step is correct\n",
            "Q: What is the main reason for the need for additional training resources?\n",
            "A: reward hacking\n",
            "Q: What is the name of the model that PRM can help you with?\n",
            "A: guided search\n",
            "Q: What did we use to enhance test-time compute scalability?\n",
            "A: Monte Carlo Tree Search (MCTS)\n",
            "Q: What is the first step in generating questions from a model?\n",
            "A: breaking answers into smaller parts\n",
            "Q: How do we find answers to questions?\n",
            "A: MCTS\n",
            "Q: What do we use to train both the actor model and the value model?\n",
            "A: question-answer pairs\n",
            "Q: What is the search space in token generation?\n",
            "A: 15  exponentially larger search space\n",
            "Q: What does the value model directly influence the quality of generation?\n",
            "A: it guides each step of the search process\n",
            "Q: What are the most common questions you can ask from a model?\n",
            "A: fine-grained value model\n",
            "Q: What is the name of the tokens that AlphaGo uses?\n",
            "A: value model\n",
            "Q: What can be generated from MCTS?\n",
            "A: pre-trained value model\n",
            "Q: What are the conclusions of this work?\n",
            "A: we share our journey in enhancing model reasoning abilities through reinforcement learning\n",
            "Q: What is the deep-seek-r1-zero approach?\n",
            "A: pure RL approach without relying on cold-start data\n",
            "Q: DeepSeek-R1 can generate questions from what?\n",
            "A: small dense models\n",
            "Q: What is the teacher model used for?\n",
            "A: to generate 800K training samples\n",
            "Q: What is the name of the test?\n",
            "A: DeepSeek-R1-Distill-Qwen\n",
            "Q: What are the main questions for DeepSeek-R1?\n",
            "A: research across the following directions\n",
            "Q: What is the general capability of DeepSeek-R1?\n",
            "A: General Capability\n",
            "Q: What are the first questions we want to ask?\n",
            "A: how long CoT can be leveraged to enhance tasks in these fields\n",
            "Q: What languages are supported in DeepSeek-R1?\n",
            "A: Chinese and English\n",
            "Q: What language might DeepSeek-R1 use?\n",
            "A: English\n",
            "Q: What does DeepSeek-R1 use to generate questions?\n",
            "A: Few-shot prompting\n",
            "Q: What are the most common questions that users can ask?\n",
            "A: output format using a zero-shot setting for optimal results\n",
            "Q: What is the most common type of RL used in software engineering tasks?\n",
            "A: large-scale\n",
            "Q: What does DeepSeek-R1 not show a huge improvement over DeepSeek-V\n",
            "A: software engineering benchmarks\n",
            "Q: What will be generated from the AI@Meta. Llama 3.1 model card?\n",
            "A: 16  References\n",
            "Q: What is the name of the Claude 3.5 sonnet?\n",
            "A: claude\n",
            "Q: What are some of the questions that you can ask?\n",
            "A: H. P. de Oliveira Pinto\n",
            "Q: What is the name of the author of the book?\n",
            "A: D. Cummings\n",
            "Q: What is the name of the paper that was published in 2021?\n",
            "A: CoRR\n",
            "Q: What is the name of the preprint of the book?\n",
            "A: arXiv\n",
            "Q: What is the name of the tree-search that can help with large language model decoding and training\n",
            "A: Alphazero\n",
            "Q: What are the questions that you can ask from the axiv database?\n",
            "A: Scaling laws for reward model overoptimization\n",
            "Q: What is the name of the next-generation Gemini model?\n",
            "A: Gemini 1.5, 2024\n",
            "Q: What is the name of the next generation model?\n",
            "A: february-2024\n",
            "Q: What is the name of the preprint for the book?\n",
            "A: arXiv\n",
            "Q: What is the name of the preprint of C-Eval?\n",
            "A: arXiv:2305.08322, 2023\n",
            "Q: What is the name of the project that was created by N. Jain?\n",
            "A: Livecodebench\n",
            "Q: What is the URL of the augmented generation?\n",
            "A: unified evaluation\n",
            "Q: What is the abs/2409.12941?\n",
            "A: CoRR, abs/2409.12941, 2024\n",
            "Q: What is the name of the preprint of the book?\n",
            "A: arXiv\n",
            "Q: What is the name of the preprint of CMMLU?\n",
            "A: arXiv\n",
            "Q: What is the name of the preprint of arXiv?\n",
            "A: 2406.11939, 2024\n",
            "Q: What is the name of the preprint for the book?\n",
            "A: arXiv:2305.20050, 2023\n",
            "Q: What is the name of the American Invitational Mathematics Examination?\n",
            "A: AIME 2024\n",
            "Q: What is the name of the book that opens a new question from OpenAI?\n",
            "A: SimpleQA\n",
            "Q: What are the questions that you can ask from the Introducing SWE-bench?\n",
            "A: human-validated subset\n",
            "Q: What is the name of the party of foundation models?\n",
            "A: A party\n",
            "Q: What is the name of the benchmark that is google proof?\n",
            "A: q&a\n",
            "Q: What is the name of the preprint of arXiv?\n",
            "A: arXiv:2402.03300, 2024\n",
            "Q: What is the abs/1712.01815?\n",
            "A: URL\n",
            "Q: What is the name of the author of the book?\n",
            "A: D. Hassabis\n",
            "Q: What is the name of the study that was published in 2017b?\n",
            "A: Nat., 550\n",
            "Q: What are the most common questions that can be asked about scaling llm test-time?\n",
            "A: without human demonstrations\n",
            "Q: What is the name of the book that was published in 2022?\n",
            "A: Nature\n",
            "Q: What is the name of the preprint of Math-sheerd?\n",
            "A: arXiv:2312.08935, 2023\n",
            "Q: What is the name of the preprint of arXiv?\n",
            "A: 2203.11171, 2022\n",
            "Q: What is the name of the newest version of Mmlu-pro?\n",
            "A: W. Chen\n",
            "Q: What is the name of the book that is based on the agentless method?\n",
            "A: arXiv preprint\n",
            "Q: What is the name of the version of Deepseek-prover?\n",
            "A: v1.5\n",
            "Q: What is the name of the preprint of arXiv?\n",
            "A: Appendix A\n",
            "Q: What are the core contributors?\n",
            "A: Contributions and Acknowledgments\n",
            "Q: Who contributed to Wu Zhibin Gou?\n",
            "A: Gao Contributors\n",
            "Q: What is the name of the person who asked the question?\n",
            "A: Cai Jiaqi Ni Jian\n",
            "Q: Who is the Jin 20 Ruyi Chen Shanghao Lu Shangyan Zhou\n",
            "A: Jin 20  Ruyi Chen Shanghao Lu Shangyan\n",
            "Q: What are the names of the questions that you can ask?\n",
            "A: Su Xuheng Lin X.Q\n",
            "Q: What are the names of the people who are in the Xinnan Song Xin\n",
            "A: Jin Xiaojin Shen Xiaosha Chen Xiaowen Sun Xiaoxiang\n",
            "Q: What are the names of the people who are in the Yin Yang family?\n",
            "A: Wei Yang Zhang Yanhong Xu Yao Li Yao Zhao Yaofeng Sun Yaohui\n",
            "Q: What are the names of the people who can help you?\n",
            "A: Ma Ying Tang Yukun Zha Yuting Yan Z.Z\n",
            "Q: Who is the author of the role?\n",
            "A: the first name\n",
            "Q: What are the names of the individuals who have departed from our team?\n",
            "A: Names marked with *\n",
            "Q: What is the name of the algorithm that created the DeepSeek-V3 Technical Report?\n",
            "A: DualPipe\n",
            "Q: What are the questions that you can generate from the profile data?\n",
            "A: computation-communication overlap\n",
            "Q: What are the names of the bubbles and memory usage comparison?\n",
            "A: Pipeline Bubbles and Memory Usage Comparison\n",
            "Q: What are the main questions that you can ask about DualPipe?\n",
            "A: communication-computation overlap strategies and low-level implementation details\n",
            "Q: What did you generate questions from?\n",
            "A: PyTorch Profiler\n",
            "Q: What does the training profile data show?\n",
            "A: our overlapping strategy\n",
            "Q: What are the 4 layers of MoE?\n",
            "A: Mixture of Experts\n",
            "Q: What does DeepSeek generate questions from?\n",
            "A: online deployment\n",
            "Q: What are the questions that are generated from?\n",
            "A: micro-batches\n",
            "Q: What does the profile employ for decoding?\n",
            "A: EP128, TP1\n",
            "Q: Decoding leverages two micro-batches for computation and all-to-all communication.\n",
            "A: decoding\n",
            "Q: What does the system wait for after the computation has finished?\n",
            "A: all-to-all communication\n",
            "Q: What are the experts assigned to different GPUs?\n",
            "A: expert parallelism\n",
            "Q: What are the most common questions that you can ask?\n",
            "A: different experts may vary depending on the current workload\n",
            "Q: What is the name of the paper that describes the strategy that is used to duplicate heavy-loaded experts\n",
            "A: DeepSeek-V3\n",
            "Q: What are the experts of the same group to the same node?\n",
            "A: to reduce inter-node data traffic\n",
            "Q: What is the name of the algorithm that we open-source?\n",
            "A: EP load balancing algorithm\n",
            "Q: What is the name of the repo that can be used to generate questions from?\n",
            "A: Algorithm\n",
            "Q: What is the hierarchical load balancing policy?\n",
            "A: to harness the group-limited expert routing\n",
            "Q: What do we generate questions from?\n",
            "A: replicated experts to individual GPUs\n",
            "Q: What can be generated from the hierarchical load balancing policy?\n",
            "A: smaller expert-parallel size\n",
            "Q: What does Global Load Balancing policy use?\n",
            "A: global load balancing\n",
            "Q: What is the policy for a larger expert-parallel size?\n",
            "A: decoding stage\n",
            "Q: What does the shared storage layer help you create?\n",
            "A: distributed applications\n",
            "Q: What is the name of the feature that 3FS provides?\n",
            "A: Performance and Usability\n",
            "Q: What are the most common questions that can be generated from the file interface?\n",
            "A: well known and used everywhere.\n",
            "Q: What is the need to learn a new storage API?\n",
            "A: There is no\n",
            "Q: What did the cluster generate?\n",
            "A: 180 storage nodes\n",
            "Q: What did we use to generate questions from?\n",
            "A: GraySort benchmark\n",
            "Q: What is the first step of the two-phase approach?\n",
            "A: partitioning data via shuffle\n",
            "Q: What did the test cluster generate?\n",
            "A: 25 storage nodes\n",
            "Q: What is the name of the technique used to optimize the LLM inference process?\n",
            "A: KVCache</li>  KVCache\n",
            "Q: What does the encoding avoid?\n",
            "A: redundant computations\n",
            "Q: What is the read throughput of all KVCache clients?\n",
            "A: 40 GiB/s\n",
            "Q: What are the IOPS of removing ops from garbage collection?\n",
            "A: The bottom figure\n",
            "Q: What is the source of the article?\n",
            "A: https://medium.com\n",
            "Q: What is the latest model of Deepseek?\n",
            "A: Deepseek r1\n",
            "Q: What is the most downloaded free app on the U.S. App Store?\n",
            "A: ChatGPT\n",
            "Q: What is the significance of Deepseek as a disruptor in the industry?\n",
            "A: its approach\n",
            "Q: What is the name of the software that Deepseek used to train its model?\n",
            "A: Nvidia H800 GPUs\n",
            "Q: What are the most common questions that DeepSeekV3 can generate?\n",
            "A: The capital expenditure for owning the hardware. 2\n",
            "Q: What are the costs associated with prior research?\n",
            "A: Costs\n",
            "Q: How did Deepseek make training more efficient?\n",
            "A: 45 times\n",
            "Q: What does the sparse activation of the model generate?\n",
            "A: compute requirements\n",
            "Q: What did they implement?\n",
            "A: an FP8 mixed precision training framework\n",
            "Q: What does the FP16/FP32 format use to generate questions?\n",
            "A: memory footprint\n",
            "Q: What did the MoE architecture use to determine load balancing?\n",
            "A: Load Balancing Strategy\n",
            "Q: What did HAI-LLM help with?\n",
            "A: efficient pipeline parallelism\n",
            "Q: What reduces pipeline bubbles and overlapping computation and communication?\n",
            "A: Efficient cross-node all-to-all communication kernels\n",
            "Q: What is the name of the Deepseek v3 model?\n",
            "A: flagship model v3\n",
            "Q: Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5\n",
            "A: Deepseek excels at reasoning and math\n",
            "Q: How was the Llama 403b trained?\n",
            "A: 11x\n",
            "Q: How much money does the claim of $5.5 million make?\n",
            "A: marketing trick\n",
            "Q: What is the metric used to measure GPU hours?\n",
            "A: Ideal\n",
            "Q: What is the smallest amount of FLOPs required?\n",
            "A: 8.33×10⁸ seconds⇒≈0\n",
            "Q: What is the reference model for Llama 3.1?\n",
            "A: 405B parameters, 15 T tokens\n",
            "Q: What is the ratio of FLOPs needed for DeepSeekV3 versus Ll\n",
            "A: 3.64×10²⁵\n",
            "Q: What is the estimated GPU hours for DeepSeekV3?\n",
            "A: 2.79M\n",
            "Q: What did DeepSeekV3 generate?\n",
            "A: a cluster of 2,048 H800 GPUs\n",
            "Q: What is the total GPU hours?\n",
            "A: 5K GPU hours\n",
            "Q: How many GPU hours does it take to train DeepSeekV3?\n",
            "A: 180K H800\n",
            "Q: What does DeepSeekV3 require to train?\n",
            "A: 2.788M GPU hours\n",
            "Q: What are the costs of training the H800 GPU?\n",
            "A: $5.576M. 5\n",
            "Q: How many GPU hours did DeepSeek estimate?\n",
            "A: 2.79\n",
            "Q: What are the questions that you can ask about the DeepSeek-V3 model?\n",
            "A: performance and economical training\n",
            "Q: What are the questions that will be asked in this paper?\n",
            "A: various features that were invented and applied\n",
            "Q: What is the name of the model that DeepSeek created?\n",
            "A: LLMs\n",
            "Q: What will be added to the story?\n",
            "A: my own interpretation of the DeekSeek model\n",
            "Q: What are the main questions of the DeekSeek-V3 model?\n",
            "A: core architecture\n",
            "Q: What were the parts of the V3 model built upon?\n",
            "A: V2 paper\n",
            "Q: What did the Transformer block have in common with the Llama?\n",
            "A: its attention and Feed-Forward Network were more sophisticated\n",
            "Q: What are the two main components of DeepSeekMoE?\n",
            "A: Multi-Head Latent Attention(MLA)\n",
            "Q: What are the most common questions that can be generated from the MLA database?\n",
            "A: speed and memory\n",
            "Q: What is the name of the technique that is used to analyze data?\n",
            "A: Principal component analysis\n",
            "Q: What are the questions that can be generated from the Latent Diffusion model?\n",
            "A: compress and decompress the input data\n",
            "Q: What can DeepSeek model generate questions from?\n",
            "A: a compressed vector\n",
            "Q: What should be generated from the weight matrix?\n",
            "A: compression\n",
            "Q: What did the V2 paper show that decoupled RoPE was used to?\n",
            "A: Applying RoPE to the compressed vector\n",
            "Q: What is the RoPE-applied query and key used for?\n",
            "A: concatenated\n",
            "Q: What is the name of the network that is split into experts?\n",
            "A: Feed-Forward Network\n",
            "Q: What is the name of the AI that generates questions from?\n",
            "A: DeekSeekMoE\n",
            "Q: What can each expert do instead of dealing with entire range of tokens alone?\n",
            "A: coping\n",
            "Q: What are the experts selected to be activated?\n",
            "A: tokens\n",
            "Q: What algorithm can be used to select experts?\n",
            "A: vector\n",
            "Q: What is the domain of expert and input token?\n",
            "A: similar\n",
            "Q: What is input vector to FFN?\n",
            "A: uₜ\n",
            "Q: What does the dot product utT ei quantify?\n",
            "A: the similarity\n",
            "Q: What is the name of the algorithm that selects Kr experts with high score?\n",
            "A: Topk\n",
            "Q: What is the output of experts?\n",
            "A: the final output\n",
            "Q: What is the name of the method that DeepSeek improved?\n",
            "A: Multi-Token Prediction\n",
            "Q: What did DeepSeek decide to use instead of parallel MTP?\n",
            "A: sequential MTP\n",
            "Q: What do MTP modules send out?\n",
            "A: output of prediction\n",
            "Q: What can a single Transformer block generate multiple tokens?\n",
            "A: multi-token prediction\n",
            "Q: What can the model learn and prepare for?\n",
            "A: additional tokens\n",
            "Q: What did DeepSeek generate one additional token?\n",
            "A: computational cost\n",
            "Q: What are the MTP modules discarded?\n",
            "A: one token per prediction\n",
            "Q: What did people think would be more useful for training LLM?\n",
            "A: high-performance GPUs\n",
            "Q: What is the DeepSeek model trained on?\n",
            "A: 2048 H800 GPUs\n",
            "Q: What are the most common questions that you can ask?\n",
            "A: new data is copied from other GPU\n",
            "Q: What is the name of the method that DeepSeek invented to reduce bubble?\n",
            "A: bubble\n",
            "Q: What are the questions that can be generated from the model?\n",
            "A: data flows through the model in forward and backward processes\n",
            "Q: What is the backward process?\n",
            "A: data moves from the output layer to the input later\n",
            "Q: What did researchers find about the backward process?\n",
            "A: the backward process can be split into two processes\n",
            "Q: What does the backward for input do?\n",
            "A: computation of the gradient of the loss with respect to the input data\n",
            "Q: What are the backward for input and backward for weight?\n",
            "A: completed ahead\n",
            "Q: What are the questions you can generate from?\n",
            "A: the backward for input is used for the calculation of backward for weight\n",
            "Q: What are the most common questions that GPUs may have?\n",
            "A: communications\n",
            "Q: What is the first step in the DualPipe?\n",
            "A: The batch 0\n",
            "Q: What is the device 7 used to generate questions from?\n",
            "A: batch 0\n",
            "Q: What do we use to generate questions?\n",
            "A: continuously copy them together on other devices\n",
            "Q: What are the most common questions that can be asked about weaker H800 GPUs?\n",
            "A: they couldn’t improve the speed\n",
            "Q: What are the most common questions that can be asked by LLM?\n",
            "A: Mixed precision training\n",
            "Q: What are the most important parts of a mixed precision training?\n",
            "A: model\n",
            "Q: What are the researchers trying to get out of DeepSeek-V3 model?\n",
            "A: reduce precision\n",
            "Q: What did DeepSeek preserve high precision for matrix addition and storing data?\n",
            "A: relatively lightweight computation\n",
            "Q: What are the numerical values clipped to a certain representable range?\n",
            "A: If the numerical values are quantized in lower precision\n",
            "Q: What can be generated from the above?\n",
            "A: values\n",
            "Q: What is the name of the feature that allows you to generate questions from?\n",
            "A: Fine-Grained Quantization\n",
            "Q: What is the method used to generate questions from?\n",
            "A: the values are grouped, and each group has its own scaling factor\n",
            "Q: What can be generated from the quantization of errors?\n",
            "A: small errors\n",
            "Q: What are the intermediate values copied in high precision?\n",
            "A: if the number of values reaches the interval\n",
            "Q: What are the values that are grouped in high precision?\n",
            "A: some values\n",
            "Q: What are the two techniques to prevent quantization error?\n",
            "A: visualized in following figure\n",
            "Q: What are the two types of reward models used?\n",
            "A: rule-based reward model(RM) and model-based reward model\n",
            "Q: What is the rule-based RM used to generate questions from?\n",
            "A: specific rules\n",
            "Q: What does the model-based RM determine?\n",
            "A: whether the answer matches the ground-truth answer\n",
            "Q: What did DeepSeek-V3 model use to generate questions from?\n",
            "A: Group Relative POlicy Optimization\n",
            "Q: What is the GRPO algorithm used to maximize?\n",
            "A: the following objective\n",
            "Q: What is output of the model?\n",
            "A: o\n",
            "Q: What is the policy model?\n",
            "A: outputs a probability distribution over tokens\n",
            "Q: What should we generate questions from?\n",
            "A: output o is right answer, we should reinforce the probability of that model\n",
            "Q: What should be minimized if the output o is correct?\n",
            "A: π(o|q)\n",
            "Q: What do we want to generate questions from?\n",
            "A: fine-tuned model\n",
            "Q: What does GRPO algorithm generate questions from?\n",
            "A: KL divergence and epsilon parameter\n",
            "Q: What is the KL divergence term?\n",
            "A: GRPO objective\n",
            "Q: What does the GRPO algorithm generate?\n",
            "A: model performance and reasoning capability\n",
            "Q: What are the main questions that you can ask about DeepSeek-V3?\n",
            "A: its performance exceeds the OpenAI model\n",
            "Q: What can researchers use to create their own models?\n",
            "A: DeekSeek models\n",
            "Q: What are the deep-seek researchers looking for?\n",
            "A: more advanced idea to improve the model performance and efficient training process\n",
            "Q: What are the most common questions that can be asked from an AI?\n",
            "A: data and model\n",
            "Q: What are the most important questions to ask?\n",
            "A: the performance of a good AI model\n",
            "Q: What are the questions that we're asking?\n",
            "A: pushing our limits in AGI exploration\n",
            "Q: What will open-source 5 repos starting this week?\n",
            "A: Feb 24, 2025\n",
            "Q: What are the building blocks of our online service?\n",
            "A: documented, deployed and battle-tested in production\n",
            "Q: What is the purpose of the daily unlocks?\n",
            "A: every line shared becomes collective momentum that accelerates the journey\n",
            "Q: What are the questions that you can ask from the FlashMLA repo?\n",
            "A: Performance: 3000 GB/s memory-bound\n",
            "Q: What is the name of the repository that contains the code for DeepEP?\n",
            "A: GitHub Repo\n",
            "Q: What is the name of the GitHub repo?\n",
            "A: DeepGEMM\n",
            "Q: What is the name of the expert-parallel load balancer for V3/R1?\n",
            "A: GitHub Repo ✅ EPLB\n",
            "Q: What is the name of the GitHub repo?\n",
            "A: Repo\n",
            "Q: What is the main feature of DeepSeek-V3/R1?\n",
            "A: Optimized throughput and latency\n",
            "Q: What are the main questions that can be generated from the 3FS system?\n",
            "A: cluster manager, metadata service, storage service and client\n",
            "Q: What does cluster manager generate questions from?\n",
            "A: Metadata and storage services\n",
            "Q: What are the primary questions for cluster managers?\n",
            "A: one of them is elected as the primary\n",
            "Q: What do you need to know about the cluster configuration?\n",
            "A: reliable distributed coordination service\n",
            "Q: What are the metadata services used to generate questions from?\n",
            "A: file system semantics\n",
            "Q: Clients can connect to any storage service.\n",
            "A: Clients can connect to any metadata service\n",
            "Q: What is the CRAQ write-all-read-any approach?\n",
            "A: helps to unleash the throughput\n",
            "Q: What is the most common question that can be asked from a FUSE client?\n",
            "A: Object store\n",
            "Q: What are the most common questions that can be asked by an application?\n",
            "A: file system semantics and a unified namespace\n",
            "Q: What can be generated from an object store?\n",
            "A: hierarchical directory structures\n",
            "Q: What does the program not support natively?\n",
            "A: operations like atomically moving files/directories\n",
            "Q: What are the most common questions that are generated by the application?\n",
            "A: creating a temporary directory, writing files to it\n",
            "Q: What are the most common questions that can be asked from a user?\n",
            "A: directories\n",
            "Q: What do applications use to create lightweight snapshots of dynamically updated datasets?\n",
            "A: symbolic and hard links\n",
            "Q: What is the file interface well known and used everywhere?\n",
            "A: Familiar interface\n",
            "Q: What are the most common questions that you can ask about file-based data loaders?\n",
            "A: 3FS FUSE client or native client\n",
            "Q: What does FUSE do to simplify file system client development?\n",
            "A: redirecting I/O operations to user-space processes\n",
            "Q: What does it create the illusion that applications are accessing the remote file system as if it were\n",
            "A: it were a local file system\n",
            "Q: What is the name of the file system that can be used to generate questions?\n",
            "A: daemon\n",
            "Q: What does FUSE use to generate questions from?\n",
            "A: multi-threaded shared queue\n",
            "Q: What does the user-space file system daemon process requests from?\n",
            "A: queue\n",
            "Q: What does FUSE use to generate questions?\n",
            "A: concurrency\n",
            "Q: What does profiling reveal about the kernel-space spin lock?\n",
            "A: consumes a significant amount of CPU time\n",
            "Q: What can data analytics generate?\n",
            "A: large block writes\n",
            "Q: What does FUSE not support concurrent writes to the same file?\n",
            "A: Linux 5.x\n",
            "Q: What are the most common types of questions that can be generated?\n",
            "A: Read operations exhibit more complex patterns\n",
            "Q: What are the most common types of dataset samples?\n",
            "A: not 4K-aligned in files\n",
            "Q: What are the most common questions that data loaders can ask?\n",
            "A: batches of samples\n",
            "Q: What does the Asynchronous zero-copy API generate?\n",
            "A: file system client\n",
            "Q: What are the most common questions that can be asked about bugs?\n",
            "A: machines may crash and leave no log message for debugging\n",
            "Q: What are the most common questions that can be asked by the FUSE client?\n",
            "A: machine restart\n",
            "Q: What does the client generate questions from?\n",
            "A: asynchronous zero-copy I/O operations\n",
            "Q: What do applications call to obtain a file descriptor?\n",
            "A: <code>open()</code>\n",
            "Q: What is the name of the asynchronous zero-copy API?\n",
            "A: io_uring\n",
            "Q: What are the key data structures in the API?\n",
            "A: <ul> <li> <em>Iov\n",
            "Q: What is the name of the ring buffer used for communication between user process and native client?\n",
            "A: Ior\n",
            "Q: What is the use of Ior similar to?\n",
            "A: Linux\n",
            "Q: What are the requests generated from?\n",
            "A: batches\n",
            "Q: What are the most common threads used to fetch I/O requests?\n",
            "A: multiple rings\n",
            "Q: What are the requests that are batched and sent to storage services?\n",
            "A: small read requests\n",
            "Q: What does 3FS generate?\n",
            "A: file data\n",
            "Q: What is the name of the file that is stored on multiple storage services?\n",
            "A: Each chunk\n",
            "Q: What is the name of the seed generated to shuffle the selected chains?\n",
            "A: random\n",
            "Q: What does the allocation strategy ensure?\n",
            "A: balanced data distribution\n",
            "Q: What can the client generate?\n",
            "A: chunk IDs and chains\n",
            "Q: What does 3FS use FoundationDB as its distributed storage system for metadata?\n",
            "A: transactional key-value store\n",
            "Q: What does 3FS store in FoundationDB?\n",
            "A: all metadata as key-value pairs\n",
            "Q: What can clients use to fail over to other available services?\n",
            "A: timeouts\n",
            "Q: What do inodes store?\n",
            "A: attribute information\n",
            "Q: What are the inode keys used for?\n",
            "A: spread inodes over multiple FoundationDB nodes\n",
            "Q: What are the basic attributes of an inode type?\n",
            "A: ownership, permissions, access/modification/change times\n",
            "Q: What are some additional attributes for file inodes?\n",
            "A: file length, chunk size\n",
            "Q: What are the basic attributes for directory inodes?\n",
            "A: the parent directory’s inode id, default layout configurations\n",
            "Q: What is required to detect loops when moving directories?\n",
            "A: The parent’s inode id\n",
            "Q: What do we need to check for when moving a directory?\n",
            "A: all ancestors\n",
            "Q: What are the names of the directory entry keys?\n",
            "A: DENT\" prefix, the parent inode ID, and the entry name\n",
            "Q: Generate questions from: All entries within a directory naturally form a contiguous key range\n",
            "A: range queries\n",
            "Q: What are the meta operations used for?\n",
            "A: leverage FoundationDB’s transactions\n",
            "Q: What does FoundationDB generate questions from?\n",
            "A: conflict detection sets\n",
            "Q: What does the design of the meta services allow for?\n",
            "A: process requests in parallel while maintaining file system metadata consistency\n",
            "Q: What are the names of the files that are used to generate questions from?\n",
            "A: all file descriptors\n",
            "Q: What would be a big burden on meta service and FoundationDB?\n",
            "A: Storing all file descriptors\n",
            "Q: What does 3FS not track file descriptors opened in read-only mode?\n",
            "A: training jobs\n",
            "Q: What does 3FS generate questions from?\n",
            "A: concurrent writes\n",
            "Q: What does meta service generate when a file with active write sessions is deleted?\n",
            "A: delays the deletion\n",
            "Q: What does the 3FS meta service generate?\n",
            "A: file length\n",
            "Q: What do clients report to meta service maximum write position of each file opened in write mode?\n",
            "A: Clients periodically\n",
            "Q: What is the new file length adopted from?\n",
            "A: inode\n",
            "Q: What is the name of the method used to generate questions from?\n",
            "A: concurrent writes from multiple clients\n",
            "Q: What does the meta service get from the storage service?\n",
            "A: the precise file length\n",
            "Q: What is the name of the operation that causes non-negligible overhead?\n",
            "A: file data is striped across multiple chains\n",
            "Q: What does the meta service generate questions from?\n",
            "A: inode IDs and the rendezvous hash algorithm\n",
            "Q: What is the number of potentially used chains stored in file inode?\n",
            "A: length\n",
            "Q: What is the first value of the file chunk?\n",
            "A: 16\n",
            "Q: What is the goal of a chunk storage system?\n",
            "A: to achieve the highest bandwidth possible\n",
            "Q: What should be generated from the read/write throughput of 3FS?\n",
            "A: SSDs and bisection network bandwidth between clients and storage services\n",
            "Q: What is the name of the process that generates questions from?\n",
            "A: chain replication\n",
            "Q: What are the read requests sent to?\n",
            "A: any of the storage target\n",
            "Q: What is the chain table constructed from?\n",
            "A: If each chunk has 3 replicas\n",
            "Q: What is the version number of each chain?\n",
            "A: Target 1\n",
            "Q: What is the name of the database that can be used to generate questions from?\n",
            "A: chain tables\n",
            "Q: What can be generated from two chain tables?\n",
            "A: batch/offline jobs and another for online services\n",
            "Q: What can be generated from a chain table?\n",
            "A: metadata service pick a table for each file and stripe file chunks\n",
            "Q: What is the 'balanced traffic during recovery'?\n",
            "A: Suppose read traffic\n",
            "Q: What would be the bottleneck of the entire system?\n",
            "A: B\n",
            "Q: What are the most common questions that can be asked from an SSD?\n",
            "A: failed SSD and syncing data\n",
            "Q: What is the chain table used to generate?\n",
            "A: A is paired with every other SSDs\n",
            "Q: What is the name of the code that is used to generate questions from?\n",
            "A: F3\n",
            "Q: What is the optimal solution obtained by using integer programming solver?\n",
            "A: Data replication\n",
            "Q: What is the most important part of a read throughput?\n",
            "A: read bandwidth\n",
            "Q: What does a storage service do when it receives a write request?\n",
            "A: reject\n",
            "Q: What is the name of the service that generates questions from?\n",
            "A: RDMA Read\n",
            "Q: What is the name of the command that generates questions from?\n",
            "A: Concurrent\n",
            "Q: What does the service read the committed version of the chunk into memory?\n",
            "A: <li>\n",
            "Q: What are the version numbers of pending versions?\n",
            "A: <code>v</code>\n",
            "Q: What is the tail of the service?\n",
            "A: the committed version is atomically replaced by the pending version\n",
            "Q: What is the current chain version stored as a field in the chunk metadata?\n",
            "A: When the committed version\n",
            "Q: What is the name of the question that can be generated from a storage service?\n",
            "A: acknowledgment message\n",
            "Q: What is the chain of targets?\n",
            "A: <code>A, B, C</code>\n",
            "Q: What does a cluster manager mark as offline when it detects a failure?\n",
            "A: <code>B</code>\n",
            "Q: What does code>A/code> forward the write request to?\n",
            "A: the new successor\n",
            "Q: What can be generated from the request?\n",
            "A: latest chain table\n",
            "Q: What does CRAQ not issue version query to the tail target?\n",
            "A: implementation\n",
            "Q: What does the service generate questions from?\n",
            "A: a special status code\n",
            "Q: What does the cluster manager generate questions from?\n",
            "A: heartbeats\n",
            "Q: What are the metadata services stateless?\n",
            "A: The metadata services are stateless.\n",
            "Q: What are the meta services that cluster manager provides?\n",
            "A: online\n",
            "Q: What does the cluster manager generate?\n",
            "A: storage services\n",
            "Q: What is the public state used to serve read requests?\n",
            "A: chain tables\n",
            "Q: What is the local state of a cluster?\n",
            "A: Local state is only known by storage services and cluster manager\n",
            "Q: What does a storage service do when a storage service is down?\n",
            "A: storage targets managed by the service are marked offline\n",
            "Q: What can be generated from a storage target?\n",
            "A: change from one public state to another in response to the latest local state\n",
            "Q: What does the cluster manager generate?\n",
            "A: updates the public states of targets\n",
            "Q: What is the name of the question that is generated?\n",
            "A: The chain version\n",
            "Q: What does lastsrv do?\n",
            "A: exits immediately\n",
            "Q: What does the storage service set the target's local state to up-to-date in subsequent\n",
            "A: heartbeat messages\n",
            "Q: What is the name of the service that exits when a storage service exits?\n",
            "A: Data recovery\n",
            "Q: What are the most common questions that can be generated from a crash or restart during an upgrade?\n",
            "A: all related storage targets\n",
            "Q: What is the name of the service that is used to generate questions?\n",
            "A: recovery process\n",
            "Q: What does a service send heartbeats from when all its storage targets have been marked offline?\n",
            "A: the latest chain tables\n",
            "Q: What is the first step in the recovery process?\n",
            "A: write request\n",
            "Q: What is the local committed version of the service?\n",
            "A: the tail\n",
            "Q: What is the state of the predecessor copied to the returning service?\n",
            "A: The full\n",
            "Q: What does the predecessor send to the returning service?\n",
            "A: dump-chunkmeta request\n",
            "Q: What does the service iterate the local chunk metadata store to collect?\n",
            "A: the ids\n",
            "Q: What does the service know about the storage target?\n",
            "A: up-to-date\n",
            "Q: What does a storage service do when it finds a previously offline successor is online?\n",
            "A: starts to forward normal write requests to the successor\n",
            "Q: What is the name of the service that sends a dump-chunkmeta request to\n",
            "A: The service sends a dump-chunkmeta request to the successor\n",
            "Q: What does the DB generate?\n",
            "A: chunk metadata\n",
            "Q: What are the chunks that are selected are transferred to the successor?\n",
            "A: full-chunk-replace write requests\n",
            "Q: What is the chain version?\n",
            "A: committed version number and chunk content are read and transferred to successor\n",
            "Q: What are the rules used to decide which chunks should be transferred?\n",
            "A: <ul> <li>\n",
            "Q: What should be transferred if the chain version of local chunk replica is greater than that of remote chunk\n",
            "A: If the chain version of local chunk replica\n",
            "Q: What should be transferred if the chain versions of local/remote chunk replicas are the\n",
            "A: local committed version number\n",
            "Q: What are the chunks and the metadata stored in?\n",
            "A: chunk engine\n",
            "Q: What is the name of the chunk engine?\n",
            "A: SSD\n",
            "Q: What does the chunk engine generate?\n",
            "A: an in-memory cache of chunk metadata\n",
            "Q: What is the chunk engine interface used for?\n",
            "A: thread-safe access\n",
            "Q: What does get the chunk metadata and reference-counted handle from?\n",
            "A: hashmap cache\n",
            "Q: What does COW semantics generate?\n",
            "A: copy-on-write\n",
            "Q: How is the chunk metadata updated?\n",
            "A: write batches\n",
            "Q: What will the allocator assign to the blocks whose sizes match the actual chunk size?\n",
            "A: physical blocks whose sizes\n",
            "Q: What is the name of the resource pool?\n",
            "A: physical block size, with each pool containing 256 physical files\n",
            "Q: What will be generated for subsequent allocations?\n",
            "A: The actual storage space\n",
            "Q: When no available physical blocks remain, fallocate() will be used to create 256 new physical\n",
            "A: <code>fallocate\n",
            "Q: What does the allocator read from the chunk?\n",
            "A: a new physical block\n",
            "Q: What is the process used to create a new copy of metadata?\n",
            "A: appends\n",
            "Q: RocksDB updates the chunk metadata and statuses of new and old physical blocks.\n",
            "A: atomically updated in RocksDB\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the pipelines\n",
        "question_generator = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
        "qa_extractor = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "\n",
        "# Process each context chunk to generate questions and extract answers\n",
        "qa_pairs_per_chunk= []  # List to hold QA pairs\n",
        "for context in chunks:\n",
        "    # Step 1: Generate questions\n",
        "    question = question_generator(f\"Generate questions from: {context}\")[0][\"generated_text\"] \n",
        "    # Step 2: Extract answers for each question\n",
        "    answer = qa_extractor(question=question, context=context)\n",
        "    qa_pairs_per_chunk.append({\"context\":context,\"question\": question, \"answer\": answer[\"answer\"]})\n",
        "\n",
        "\n",
        "for item in qa_pairs_per_chunk:                                    # Display the QA pairs for each chunk\n",
        "    print(f\"Q: {item['question']}\")\n",
        "    print(f\"A: {item['answer']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD90on_49z9S",
        "outputId": "c0f3db93-6fea-4b45-ecf7-eab2b7bc7650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "575\n"
          ]
        }
      ],
      "source": [
        "print(len(qa_pairs_per_chunk[1]))\n",
        "print(len(chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "575\n"
          ]
        }
      ],
      "source": [
        "print(len(qa_pairs_per_chunk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'context': 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning DeepSeek-AI research@deepseek.com Abstract We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.', 'question': 'What are the first-generation reasoning models?', 'answer': 'DeepSeek-R1-Zero and DeepSeek-R1'}, {'context': 'DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super- vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.', 'question': 'What is the model trained via large-scale reinforcement learning?', 'answer': 'DeepSeek-R1-Zero'}, {'context': 'Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing.', 'question': 'What does DeepSeek-R1-Zero naturally emerge with?', 'answer': 'powerful and intriguing reasoning behaviors'}, {'context': 'To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek- R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks.', 'question': 'What does DeepSeek-R1 do?', 'answer': 'incorporates multi-stage training and cold-start data before RL'}, {'context': 'To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.', 'question': 'What are the names of the models that we open-source?', 'answer': 'DeepSeek-R1-Zero, DeepSeek-R1'}, {'context': 'AIME 2024 (Pass@1) Codeforces (Percentile) GPQA Diamond (Pass@1) MATH-500 (Pass@1) MMLU (Pass@1) SWE-bench Verified (Resolved) 0 20 40 60 80 100 Accuracy / Percentile (%) 79.8 96.3 71.5 97.3 90.8 49.2 79.2 96.6 75.7 96.4 91.8 48.9 72.6 90.6 62.1 94.3 87.4 36.8 63.6 93.4 60.0 90.0 85.2 41.6 39.2 58.7 59.1 90.2 88.5 42.0 DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3 Figure 1 | Benchmark performance of DeepSeek-R1.', 'question': 'What is the percentage of accuracy of DeepSeek-R1?', 'answer': '42.0'}, {'context': 'arXiv:2501.12948v1  [cs.CL]  22 Jan 2025  Contents 1 Introduction 3 1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . .', 'question': 'What is the name of the cs.CL database?', 'answer': 'arXiv:2501.12948v1'}, {'context': '. 4 2 Approach 5 2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . . 5 2.2.1 Reinforcement Learning Algorithm . . . . . . . .', 'question': 'What is the base model of DeepSeek?', 'answer': 'Reinforcement Learning on the Base Model'}, {'context': '. . . . . . . . . . . . . . 5 2.2.2 Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2.3 Training Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', 'question': 'What is the name of the training template?', 'answer': 'Template'}, {'context': '6 2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero 6 2.3 DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . . 9 2.3.1 Cold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', 'question': 'What are the main topics of DeepSeek-R1-Zero?', 'answer': 'Self-evolution Process'}, {'context': '. . 9 2.3.2 Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . . 10 2.3.3 Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . . 10 2.3.4 Reinforcement Learning for all Scenarios . . . . . . . . . . . . . . . . .', 'question': 'What is the name of the program that is used to teach Reinforcement Learning?', 'answer': 'Rejection Sampling and Supervised Fine-Tuning'}, {'context': '. . 11 2.4 Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . . 11 3 Experiment 11 3.1 DeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.2 Distilled Model Evaluation . . . . . . . . .', 'question': 'What is the name of the experiment?', 'answer': 'DeepSeek-R1 Evaluation'}, {'context': '. . . . . . . . . . . . . . . . . . . . . . 14 4 Discussion 14 4.1 Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 14 4.2 Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', 'question': 'What is the difference between Distillation and Reinforcement Learning?', 'answer': '14 4 Discussion 14 4.1'}, {'context': '15 5 Conclusion, Limitations, and Future Work 16 A Contributions and Acknowledgments 20 2  1.', 'question': 'What are the main points of the work?', 'answer': 'A Contributions and Acknowledgments 20 2  1'}, {'context': 'Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI).', 'question': 'What are the most common questions that can be asked from LLMs?', 'answer': 'Large Language Models'}, {'context': 'Recently, post-training has emerged as an important component of the full training pipeline.', 'question': 'What are the most common questions that are asked after training?', 'answer': 'post-training'}, {'context': 'It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training.', 'question': 'What does the software generate?', 'answer': 'accuracy'}, {'context': 'In the context of reasoning capabilities, OpenAI’s o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of- Thought reasoning process.', 'question': 'What did OpenAI generate?', 'answer': 'inference-time scaling'}, {'context': 'This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community.', 'question': 'What is the most common question that researchers can ask?', 'answer': 'test-time scaling'}, {'context': 'Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024).', 'question': 'What are some of the approaches that have been explored?', 'answer': 'process-based reward models'}, {'context': 'However, none of these methods has achieved general reasoning performance comparable to OpenAI’s o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL).', 'question': 'What is the first step toward improving language model reasoning capabilities?', 'answer': 'pure reinforcement learning (RL)'}, {'context': 'Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process.', 'question': 'What are the questions that we want to ask?', 'answer': 'their self-evolution'}, {'context': 'Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.', 'question': 'What is the base model used for?', 'answer': 'DeepSeek-V3-Base'}, {'context': 'During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks.', 'question': 'What did DeepSeek-R1-Zero generate?', 'answer': 'reasoning behaviors'}, {'context': 'For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.', 'question': 'What is the pass@1 score on AIME 2024?', 'answer': '15.6% to 71.0%'}, {'context': 'However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing.', 'question': 'What is the main feature of DeepSeek-R1-Zero?', 'answer': 'poor readability, and language mixing'}, {'context': 'To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline.', 'question': 'What is the name of the DeepSeek-R1?', 'answer': 'DeepSeek-R1'}, {'context': 'Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1- Zero.', 'question': 'What do we collect to refine DeepSeek-V3-Base model?', 'answer': 'thousands of cold-start data'}, {'context': 'Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.', 'question': 'What do we generate questions from?', 'answer': 'DeepSeek-V3'}, {'context': 'After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios.', 'question': 'What are the new data generated from?', 'answer': 'all scenarios'}, {'context': 'After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. We further explore distillation from DeepSeek-R1 to smaller dense models.', 'question': 'What did we generate from DeepSeek-R1?', 'answer': 'distillation'}, {'context': 'Using Qwen2.5- 32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru- cial for improving reasoning capabilities.', 'question': 'What is the base model for DeepSeek-R1?', 'answer': 'Qwen2.5- 32B'}, {'context': 'We open-source the distilled Qwen and Llama (Dubey et al., 2024) series.', 'question': 'What are the Qwen and Llama series of books based on?', 'answer': 'distilled'}, {'context': 'Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. 3  1.1.', 'question': 'What did the distilled 14B model outperform the open-source QwQ-32', 'answer': 'open-source QwQ-32B-Preview'}, {'context': 'Contributions Post-Training: Large-Scale Reinforcement Learning on the Base Model • We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step.', 'question': 'What are the questions that you can ask from the post-training?', 'answer': 'supervised fine-tuning'}, {'context': 'This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero.', 'question': 'What is the model able to generate questions from?', 'answer': 'chain-of-thought (CoT)'}, {'context': 'DeepSeek- R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community.', 'question': 'DeepSeek- R1-Zero demonstrates capabilities such as self-verification,', 'answer': 'DeepSeek- R1-Zero'}, {'context': 'Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.', 'question': 'What is the first open research to validate that reasoning capabilities of LLMs can be incen', 'answer': 'RL'}, {'context': '• We introduce our pipeline to develop DeepSeek-R1.', 'question': '• We introduce our pipeline to develop DeepSeek-R1:', 'answer': 'We introduce our pipeline to develop DeepSeek-R1'}, {'context': 'The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human pref- erences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities.', 'question': 'What does the pipeline generate?', 'answer': 'two RL stages'}, {'context': 'We believe the pipeline will benefit the industry by creating better models.', 'question': 'What do you think the pipeline will benefit the industry by creating better models?', 'answer': 'the pipeline will benefit the industry by creating better models'}, {'context': 'Distillation: Smaller Models Can Be Powerful Too • We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models.', 'question': 'What do we demonstrate that larger models can be distilled into smaller models?', 'answer': 'reasoning patterns'}, {'context': 'The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.', 'question': 'What will DeepSeek-R1 help the research community to distill better smaller models?', 'answer': 'open source'}, {'context': '• Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks.', 'question': 'What did DeepSeek-R1 generate?', 'answer': 'reasoning data'}, {'context': 'DeepSeek- R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi- tionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench.', 'question': 'What does DeepSeek-R1-Distill-Qwen-7B score on', 'answer': '55.5%'}, {'context': 'These results significantly outperform previous open- source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. 1.2.', 'question': 'What did we open-source to the community?', 'answer': '1.2'}, {'context': 'Summary of Evaluation Results • Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217.', 'question': 'What are the questions that DeepSeek-R1 generate?', 'answer': 'OpenAI-o1-1217'}, {'context': 'On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-o1-1217 and significantly outperforming other models.', 'question': 'What questions does MATH-500 generate?', 'answer': 'significantly outperforming other models'}, {'context': '(2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition.', 'question': 'DeepSeek-R1 demonstrates expert level in code competition tasks.', 'answer': '(2) On coding-related tasks'}, {'context': 'For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks.', 'question': 'DeepSeek-R1 performs slightly better than DeepSeek-V3 for engineering tasks', 'answer': 'DeepSeek-V3'}, {'context': '• Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek- R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond.', 'question': 'What does DeepSeek- R1 achieve outstanding results on?', 'answer': 'benchmarks'}, {'context': 'While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks.', 'question': 'What is the name of the OpenAI-o1-1217?', 'answer': 'DeepSeek-R1'}, {'context': 'On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.', 'question': 'DeepSeek-R1 outperforms DeepSeek-V3 on the factual', 'answer': 'SimpleQA'}, {'context': '4  • Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more.', 'question': 'What is the most common question that DeepSeek-R1 can generate?', 'answer': 'general question answering'}, {'context': 'It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are- naHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.', 'question': 'What is the best way to generate questions from?', 'answer': 'non-exam-oriented queries'}, {'context': 'Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. 2. Approach 2.1.', 'question': 'What does DeepSeek-R1 do well on long-context benchmarks?', 'answer': 'outstanding performance'}, {'context': 'Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance.', 'question': 'What are the questions that you can generate from?', 'answer': 'supervised data'}, {'context': 'In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start.', 'question': 'What are the most common questions that can be asked?', 'answer': 'reasoning capabilities'}, {'context': 'Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data.', 'question': 'What can be further enhanced with the inclusion of a small amount of cold-start data?', 'answer': 'performance'}, {'context': 'In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples.', 'question': 'What does DeepSeek-R1-Zero do?', 'answer': 'applies RL directly to the base model without any SFT data'}, {'context': '3) Distill the reasoning capability from DeepSeek-R1 to small dense models. 2.2.', 'question': 'How do you generate questions from DeepSeek-R1?', 'answer': 'Distill the reasoning capability'}, {'context': 'DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev- idenced by our previous works (Shao et al., 2024; Wang et al., 2023).', 'question': 'What is the DeepSeek-R1-Zero?', 'answer': 'Reinforcement Learning'}, {'context': 'However, these works heavily depended on supervised data, which are time-intensive to gather.', 'question': 'What kind of data are supervised works able to generate?', 'answer': 'supervised data, which are time-intensive'}, {'context': 'In this section, we explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure reinforcement learning process.', 'question': 'What are the questions that we ask in this section?', 'answer': 'the potential of LLMs to develop reasoning capabilities without any supervised data'}, {'context': 'We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. 2.2.1.', 'question': 'What are the first questions that the community can ask?', 'answer': 'RL algorithm'}, {'context': 'Reinforcement Learning Algorithm Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead.', 'question': 'What is the GRPO?', 'answer': 'Group Relative Policy Optimization'}, {'context': 'Specifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, · · · , 𝑜𝐺} from the old policy 𝜋𝜃𝑜𝑙𝑑and then optimizes the policy model 𝜋𝜃by maximizing the following objective: J𝐺𝑅𝑃𝑂(𝜃) = E[𝑞∼𝑃(𝑄), {𝑜𝑖}𝐺 𝑖=1 ∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑞)] 1 𝐺 𝐺 ∑︁ 𝑖=1 \\x12 min \\x12 𝜋𝜃(𝑜𝑖|𝑞) 𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) 𝐴𝑖, clip \\x12 𝜋𝜃(𝑜𝑖|𝑞) 𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) , 1 −𝜀, 1 + 𝜀 \\x13 𝐴𝑖 \\x13 −𝛽D𝐾𝐿 \\x00𝜋𝜃||𝜋𝑟𝑒𝑓 \\x01\\x13 , (1) D𝐾𝐿 \\x00𝜋𝜃||𝜋𝑟𝑒𝑓 \\x01 = 𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞) 𝜋𝜃(𝑜𝑖|𝑞) −log 𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞) 𝜋𝜃(𝑜𝑖|𝑞) −1, (2) where 𝜀and 𝛽are hyper-parameters, and 𝐴𝑖is the advantage, computed using a group of rewards {𝑟1, 𝑟2, .', 'question': 'What does GRPO generate for each question q?', 'answer': 'a group of outputs'}, {'context': '. . , 𝑟𝐺} corresponding to the outputs within each group: 𝐴𝑖= 𝑟𝑖−m𝑒𝑎𝑛({𝑟1, 𝑟2, · · · , 𝑟𝐺}) s𝑡𝑑({𝑟1, 𝑟2, · · · , 𝑟𝐺}) . (3) 5  A conversation between User and Assistant. The user asks a question, and the Assistant solves it.', 'question': 'What is the output of each group?', 'answer': '𝐴𝑖= 𝑟𝑖−m𝑒𝑎𝑛'}, {'context': 'The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.', 'question': 'What does the assistant generate?', 'answer': 'the answer'}, {'context': 'The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: prompt. Assistant: Table 1 | Template for DeepSeek-R1-Zero.', 'question': 'What are the reasoning process and answer enclosed within?', 'answer': '</think> and <answer> </answer> tags'}, {'context': 'prompt will be replaced with the specific reasoning question during training. 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL.', 'question': 'What will be replaced with a specific reasoning question during training?', 'answer': 'prompt'}, {'context': 'To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: • Accuracy rewards: The accuracy reward model evaluates whether the response is correct.', 'question': 'What are the rewards for DeepSeek-R1-Zero?', 'answer': 'Accuracy rewards'}, {'context': 'For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness.', 'question': 'What is the model required to provide the final answer in a specified format?', 'answer': 'within a box'}, {'context': 'Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.', 'question': 'What can a compiler generate questions from?', 'answer': 'predefined test cases'}, {'context': '• Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’ tags.', 'question': 'What does the format reward model enforce?', 'answer': 'to put its thinking process'}, {'context': 'We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.', 'question': 'What do we not use in developing DeepSeek-R1-Zero?', 'answer': 'the outcome or process neural reward model'}, {'context': '2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions.', 'question': 'What are the questions that you can generate from?', 'answer': 'specified instructions'}, {'context': 'As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer.', 'question': 'What does DeepSeek-R1-Zero generate questions from?', 'answer': 'reasoning process, followed by the final answer'}, {'context': 'We intentionally limit our constraints to this structural format, avoiding any content-specific biases—such as mandating reflective reasoning or promoting particular problem-solving strate- gies—to ensure that we can accurately observe the model’s natural progression during the RL process.', 'question': 'What are the constraints of the model?', 'answer': 'mandating reflective reasoning or promoting particular problem-solving strate- gies'}, {'context': '2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek- R1-Zero on the AIME 2024 benchmark throughout the RL training process.', 'question': 'What are the questions that you can generate from the wiki?', 'answer': 'Performance, Self-evolution Process'}, {'context': 'As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances.', 'question': 'What is the DeepSeek-R1-Zero training program?', 'answer': 'a steady and consistent enhancement in performance'}, {'context': 'Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-o1-0912.', 'question': 'What is the average pass@1 score on AIME 2024?', 'answer': '15.6%'}, {'context': 'This significant improvement highlights the efficacy of our RL algorithm in optimizing the model’s performance over time.', 'question': 'What is the most important thing to do with the RL algorithm?', 'answer': 'optimizing the model’s performance over time'}, {'context': 'Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI’s o1-0912 models across a variety of reasoning-related benchmarks.', 'question': 'What are the questions generated from DeepSeek-R1-Zero?', 'answer': 'reasoning-related benchmarks'}, {'context': 'The findings reveal that RL empowers 6  Model AIME 2024 MATH-500 GPQA LiveCode CodeForces Diamond Bench pass@1 cons@64 pass@1 pass@1 pass@1 rating OpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820 OpenAI-o1-0912 74.4 83.3 94.8 77.3 63.4 1843 DeepSeek-R1-Zero 71.0 86.7 95.9 73.3 50.0 1444 Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks.', 'question': 'What did the findings reveal about RL empower?', 'answer': '6  Model AIME'}, {'context': 'Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation.', 'question': 'How accurate is DeepSeek-R1-Zo?', 'answer': 'AIME'}, {'context': 'DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the model’s ability to learn and generalize effectively through RL alone.', 'question': 'What did DeepSeek-R1-Zero achieve?', 'answer': 'robust reasoning capabilities'}, {'context': 'Additionally, the performance of DeepSeek- R1-Zero can be further augmented through the application of majority voting.', 'question': 'What can be generated from the DeepSeek- R1-Zero?', 'answer': 'majority voting'}, {'context': 'For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912.', 'question': 'What is the most common question that DeepSeek-R1-Zero gets asked?', 'answer': '86.7%'}, {'context': 'The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks.', 'question': 'What is the name of the game that DeepSeek-R1-Zero can generate', 'answer': 'DeepSeek-R1-Zero'}, {'context': 'Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously.', 'question': 'What is the self-evolution process of DeepSeek-R1-Zero?', 'answer': 'RL can drive a model to improve its reasoning capabilities autonomously'}, {'context': 'By initiating RL directly from the base model, we can closely monitor the model’s progression without the influence of the supervised fine-tuning stage.', 'question': 'What can we use to initiate RL directly from the base model?', 'answer': 'supervised fine-tuning stage'}, {'context': 'This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks.', 'question': 'What does the model generate?', 'answer': 'complex reasoning tasks'}, {'context': 'As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve- 7  Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process.', 'question': 'What is the thinking time of DeepSeek-R1-Zero?', 'answer': 'Figure 3'}, {'context': 'DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. ment throughout the training process. This improvement is not the result of external adjustments but rather an intrinsic development within the model.', 'question': 'What does DeepSeek-R1-Zero naturally learn to solve reasoning tasks with more', 'answer': 'thinking time'}, {'context': 'DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time compu- tation.', 'question': 'What does DeepSeek-R1-Zero generate questions from?', 'answer': 'test-time compu- tation'}, {'context': 'This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth.', 'question': 'What can the model generate?', 'answer': 'reasoning tokens'}, {'context': 'One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases.', 'question': 'What are the most remarkable aspects of self-evolution?', 'answer': 'the emergence of sophisticated behaviors'}, {'context': 'Behaviors such as reflection—where the model revisits and reevaluates its previous steps—and the exploration of alternative approaches to problem-solving arise spontaneously.', 'question': 'What do you generate questions from?', 'answer': 'reflection'}, {'context': 'These behaviors are not explicitly programmed but instead emerge as a result of the model’s interaction with the reinforcement learning environment.', 'question': 'What do the models interact with the reinforcement learning environment?', 'answer': 'behaviors'}, {'context': 'This spontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy.', 'question': 'What does the spontaneous development of DeepSeek-R1-Zero allow for?', 'answer': 'to tackle more challenging tasks with greater efficiency and accuracy'}, {'context': 'Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as illustrated in Table 3, occurs in an intermediate version of the model.', 'question': 'What is the Aha Moment of DeepSeek-R1-Zero?', 'answer': 'the occurrence of an “aha moment”'}, {'context': 'During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach.', 'question': 'What does DeepSeek-R1-Zero learn to allocate more thinking time to ', 'answer': 'by reevaluating its initial approach'}, {'context': 'This behavior is not only a testament to the model’s growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.', 'question': \"What is the model's reasoning ability?\", 'answer': 'growing'}, {'context': 'This moment is not only an “aha moment” for the model but also for the researchers observing its behavior.', 'question': 'What are the researchers looking at in the moment?', 'answer': 'its behavior'}, {'context': 'It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies.', 'question': 'What is the most powerful aspect of reinforcement learning?', 'answer': 'beauty'}, {'context': 'The “aha moment” serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future.', 'question': 'What is the aha moment?', 'answer': 'a powerful reminder of the potential of RL to unlock new levels of intelligence'}, {'context': '8  Question: If 𝑎> 1, then the sum of the real solutions of √︁ 𝑎−√ 𝑎+ 𝑥= 𝑥is equal to Response: <think> To solve the equation √︁ 𝑎−√ 𝑎+ 𝑥= 𝑥, let’s start by squaring both · · · \\x10√︁ 𝑎−√ 𝑎+ 𝑥 \\x112 = 𝑥2 =⇒𝑎−√ 𝑎+ 𝑥= 𝑥2.', 'question': 'What are the real solutions of a a+ x= x equal to', 'answer': 'Response'}, {'context': 'Rearrange to isolate the inner square root term: (𝑎−𝑥2)2 = 𝑎+ 𝑥=⇒𝑎2 −2𝑎𝑥2 + (𝑥2)2 = 𝑎+ 𝑥=⇒𝑥4 −2𝑎𝑥2 −𝑥+ (𝑎2 −𝑎) = 0 . . . Wait, wait. Wait. That’s an aha moment I can flag here.', 'question': 'What is the inner square root term?', 'answer': '2'}, {'context': 'Let’s reevaluate this step-by-step to identify if the correct sum can be · · · We started with the equation: √︁ 𝑎−√ 𝑎+ 𝑥= 𝑥 First, let’s square both sides: 𝑎−√ 𝑎+ 𝑥= 𝑥2 =⇒√ 𝑎+ 𝑥= 𝑎−𝑥2 Next, I could square both sides again, treating the equation: · · · . .', 'question': 'What is the correct sum?', 'answer': '· · ·'}, {'context': '. Table 3 | An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning.', 'question': \"What is the model's answer to the aha moment?\", 'answer': 'allowing us to witness the power and beauty of reinforcement learning'}, {'context': 'Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues.', 'question': 'What is the drawback of DeepSeek-R1-Zero?', 'answer': 'strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors'}, {'context': 'For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing.', 'question': 'What is the name of the application that struggles with language mixing?', 'answer': 'DeepSeek-R1-Zero'}, {'context': 'To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. 2.3.', 'question': 'What are the questions that DeepSeek-R1 uses?', 'answer': 'human-friendly cold-start data'}, {'context': 'DeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start?', 'question': 'What is the name of the DeepSeek-R1?', 'answer': 'Reinforcement Learning with Cold Start'}, {'context': '2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1.', 'question': 'What are the main questions we want to answer?', 'answer': 'DeepSeek-R1'}, {'context': 'The pipeline consists of four stages, outlined as follows. 2.3.1.', 'question': 'What is the first step of the pipeline?', 'answer': '2.3.1'}, {'context': 'Cold Start Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor.', 'question': 'What are the cold start questions?', 'answer': 'to prevent the early unstable cold start phase of RL training'}, {'context': 'To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1- Zero outputs in a readable format, and refining the results through post-processing by human annotators.', 'question': 'How do we collect data from DeepSeek?', 'answer': 'in a readable format'}, {'context': 'In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL.', 'question': 'What is the starting point for RL?', 'answer': 'DeepSeek-V3-Base'}, {'context': 'Compared to DeepSeek-R1-Zero, the advantages of cold start data 9  include: • Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading.', 'question': 'What are the advantages of cold start data 9?', 'answer': 'its content is often not suitable for reading'}, {'context': 'Responses may mix multiple languages or lack markdown formatting to highlight answers for users.', 'question': 'What can users use to generate questions?', 'answer': 'Responses may mix multiple languages or lack markdown formatting'}, {'context': 'In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly.', 'question': 'What is the name of the data generated by DeepSeek-R1?', 'answer': 'cold-start data'}, {'context': 'Here, we define the output format as |special_token|<reasoning_process>|special_token|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results.', 'question': 'What is the output format of the query?', 'answer': '|special_token'}, {'context': '• Potential: By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. 2.3.2.', 'question': 'What is the potential of DeepSeek-R1-Zero?', 'answer': 'better performance'}, {'context': 'Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero.', 'question': 'What is the purpose of DeepSeek-V3-Base?', 'answer': 'apply the same large-scale reinforcement learning training process'}, {'context': 'This phase focuses on enhancing the model’s reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions.', 'question': 'What is the first phase of the model?', 'answer': 'reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning'}, {'context': 'During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages.', 'question': 'What do we use to train CoT?', 'answer': 'language mixing'}, {'context': 'To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT.', 'question': 'What are the language consistency rewards?', 'answer': 'the proportion of target language words in the CoT'}, {'context': 'Although ablation experiments show that such alignment results in a slight degradation in the model’s performance, this reward aligns with human preferences, making it more readable.', 'question': \"What is the reward for a human's ablation?\", 'answer': 'preferences'}, {'context': 'Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. 2.3.3.', 'question': 'What do we generate questions from?', 'answer': 'accuracy of reasoning tasks and the reward for language consistency'}, {'context': 'Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round.', 'question': 'What do we use to collect SFT data for the subsequent round?', 'answer': 'checkpoint'}, {'context': 'Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model’s capabilities in writing, role-playing, and other general-purpose tasks.', 'question': 'What does the model incorporate data from other domains?', 'answer': 'initial cold-start data'}, {'context': 'Specifically, we generate the data and fine-tune the model as described below. Reasoning data We curate reasoning prompts and generate reasoning trajectories by perform- ing rejection sampling from the checkpoint from the above RL training.', 'question': 'What do we generate questions from?', 'answer': 'the data'}, {'context': 'In the previous stage, we only included data that could be evaluated using rule-based rewards.', 'question': 'What did we use to generate questions?', 'answer': 'rule-based rewards'}, {'context': 'However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.', 'question': 'What is the dataset used to generate questions from?', 'answer': 'DeepSeek-V3'}, {'context': 'Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones.', 'question': 'What are the most common questions in the model?', 'answer': 'the correct ones'}, {'context': 'In total, we collect about 600k reasoning related training samples.', 'question': 'How many training samples are collected?', 'answer': '600k'}, {'context': '10  Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3.', 'question': 'What is the SFT dataset used for?', 'answer': 'DeepSeek-V3'}, {'context': 'For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as “hello” we do not provide a CoT in response.', 'question': 'What does DeepSeek-V3 generate questions from?', 'answer': 'a potential chain-of-thought'}, {'context': 'In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. 2.3.4.', 'question': 'What did we collect?', 'answer': 'a total of approximately 200k training samples'}, {'context': 'Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model’s helpfulness and harmlessness while simultane- ously refining its reasoning capabilities.', 'question': 'What is the primary reinforcement learning stage?', 'answer': 'improving the model’s helpfulness and harmlessness'}, {'context': 'Specifically, we train the model using a combination of reward signals and diverse prompt distributions.', 'question': 'What do we train the model using?', 'answer': 'a combination of reward signals and diverse prompt distributions'}, {'context': 'For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains.', 'question': 'What do we use to generate questions?', 'answer': 'rule-based rewards'}, {'context': 'For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train- ing prompts.', 'question': 'What do we use to generate questions from?', 'answer': 'DeepSeek-V3 pipeline'}, {'context': 'For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process.', 'question': 'What do we use to determine the usefulness of the final summary?', 'answer': 'assessment'}, {'context': 'For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.', 'question': \"What is the model's harmlessness?\", 'answer': 'to identify and mitigate any potential risks, biases, or harmful content'}, {'context': 'Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. 2.4.', 'question': 'What do you want to learn about the model?', 'answer': 'reasoning'}, {'context': 'Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in §2.3.3.', 'question': 'What did DeepSeek-R1 enable?', 'answer': 'reasoning capabilities'}, {'context': 'Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models.', 'question': 'What did the distillation method help to generate?', 'answer': 'reasoning abilities'}, {'context': 'The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5- 14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1.', 'question': 'What are the base models we use?', 'answer': 'Qwen2.5-Math-1.5B'}, {'context': 'For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance.', 'question': 'What do we use for distilled models?', 'answer': 'SFT'}, {'context': 'Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. 3.', 'question': 'What are the primary goals of the distillation technique?', 'answer': 'to demonstrate the effectiveness'}, {'context': 'Experiment Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 11  2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08 – 2025-01), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math- ematics Examination 2024 (AIME 2024) (MAA, 2024).', 'question': 'What are the benchmarks for the MMLU?', 'answer': 'Experiment Benchmarks'}, {'context': 'In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges.', 'question': 'What do we use as judges?', 'answer': 'LLMs'}, {'context': 'Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons.', 'question': 'What are the original configurations of AlpacaEval 2.0 and Arena-Hard?', 'answer': 'Li et al., 2024'}, {'context': 'Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.', 'question': 'What are the most common questions that are used in this section?', 'answer': 'AIME 2024, MATH-500, GPQA Diamond'}, {'context': 'Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple- evals framework.', 'question': 'What are the benchmarks used to evaluate?', 'answer': 'GPQA Diamond, and SimpleQA'}, {'context': 'For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting.', 'question': 'What are the first questions that are generated by MMLU-Redux?', 'answer': 'the original prompts are few-shot'}, {'context': 'The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators.', 'question': 'What is the CoT in few-shot that may hurt the performance of DeepSeek-R', 'answer': 'DeepSeek-R1'}, {'context': 'For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash).', 'question': 'What is the HumanEval-Mul dataset?', 'answer': 'eight mainstream programming languages'}, {'context': 'Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025.', 'question': 'What are the most common questions that are generated from the model?', 'answer': 'Model performance'}, {'context': 'The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated.', 'question': 'What is the Codeforces dataset used for?', 'answer': 'evaluated using problems from 10 Div.2 contests'}, {'context': 'SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark.', 'question': 'What are the questions generated from?', 'answer': 'agentless framework'}, {'context': 'Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.', 'question': 'What are the most common questions that are generated from the DeepSeek-V3?', 'answer': 'Claude-Sonnet-3.5-1022'}, {'context': 'Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor- mance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a).', 'question': 'What are the questions that we generate from the OpenAI-o1-1217 API?', 'answer': 'perfor- mance based on official reports'}, {'context': 'Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints.', 'question': 'How long is the maximum generation length?', 'answer': '32,768 tokens'}, {'context': 'Therefore, we default to pass@𝑘evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature.', 'question': 'What is the default value for pass@kevaluation?', 'answer': '𝑘evaluation'}, {'context': 'Specifically, we use a sampling temperature of 0.6 and a top-𝑝value of 0.95 to generate 𝑘 responses (typically between 4 and 64, depending on the test set size) for each question.', 'question': 'What temperature is used to generate k responses for each question?', 'answer': 'sampling temperature of 0.6 and a top-𝑝value of 0.95'}, {'context': 'Pass@1 is then calculated as pass@1 = 1 𝑘 𝑘 ∑︁ 𝑖=1 𝑝𝑖, where 𝑝𝑖denotes the correctness of the 𝑖-th response. This method provides more reliable performance estimates.', 'question': 'What is the pass@1 calculated as?', 'answer': 'pass@1 = 1'}, {'context': 'For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. 1https://aider.chat 2https://codeforces.com 3https://www.cms.org.cn/Home/comp/comp/cid/12.html 12  3.1.', 'question': 'What are the results of the consensus vote for AIME 2024?', 'answer': '64 samples'}, {'context': 'DeepSeek-R1 Evaluation Benchmark (Metric) Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek Sonnet-1022 0513 V3 o1-mini o1-1217 R1 Architecture - - MoE - - MoE # Activated Params - - 37B - - 37B # Total Params - - 671B - - 671B English MMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8 MMLU-Redux (EM) 88.9 88.0 89.1 86.7 - 92.9 MMLU-Pro (EM) 78.0 72.6 75.9 80.3 - 84.0 DROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2 IF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 - 83.3 GPQA Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5 SimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1 FRAMES (Acc.)', 'question': 'What is the name of the test that is used to determine the performance of DeepSeek-R', 'answer': 'Evaluation'}, {'context': '72.5 80.5 73.3 76.9 - 82.5 AlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 - 87.6 ArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3 Code LiveCodeBench (Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9 Codeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3 Codeforces (Rating) 717 759 1134 1820 2061 2029 SWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2 Aider-Polyglot (Acc.)', 'question': 'What is the name of the game that generates questions from?', 'answer': 'Aider-Polyglot'}, {'context': '45.3 16.0 49.6 32.9 61.7 53.3 Math AIME 2024 (Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8 MATH-500 (Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3 CNMO 2024 (Pass@1) 13.1 10.8 43.2 67.6 - 78.8 Chinese CLUEWSC (EM) 85.4 87.9 90.9 89.9 - 92.8 C-Eval (EM) 76.7 76.0 86.5 68.9 - 91.8 C-SimpleQA (Correct) 55.4 58.7 68.0 40.3 - 63.7 Table 4 | Comparison between DeepSeek-R1 and other representative models.', 'question': 'What is the name of the test that is generated from the questions in the test?', 'answer': 'Math AIME 2024'}, {'context': 'For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3.', 'question': 'What does DeepSeek-R1 demonstrate superior performance against?', 'answer': 'DeepSeek-V3'}, {'context': 'This im- provement is primarily attributed to enhanced accuracy in STEM-related questions, where signif- icant gains are achieved through large-scale reinforcement learning.', 'question': 'What is the most important question that can be generated?', 'answer': 'STEM'}, {'context': 'Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks.', 'question': 'DeepSeek-R1 excels on FRAMES, a long-context-', 'answer': 'DeepSeek-R1 excels on FRAMES'}, {'context': 'On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses GPT-4o on this benchmark.', 'question': 'DeepSeek-R1 outperforms DeepSeek-V3 on the factual', 'answer': 'SimpleQA'}, {'context': 'However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%.', 'question': 'What is the Chinese SimpleQA benchmark?', 'answer': 'DeepSeek-V3'}, {'context': 'DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a model’s ability to follow format instructions.', 'question': 'What does DeepSeek-R1 generate questions from?', 'answer': 'IF-Eval'}, {'context': 'These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training.', 'question': 'What can be linked to the inclusion of instruction-following data during the final stages of supervised', 'answer': 'improvements'}, {'context': 'Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering.', 'question': 'DeepSeek-R1 is able to write tasks and open-domain question answering.', 'answer': 'AlpacaEval2.0 and ArenaHard'}, {'context': 'Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains.', 'question': 'What is the name of the application that is able to generate questions from?', 'answer': 'DeepSeek-V3'}, {'context': 'Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0.', 'question': 'What is the average summary length on ArenaHard?', 'answer': '689 tokens'}, {'context': 'This indicates that 13  DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks.', 'question': 'What does 13 DeepSeek-R1 avoid introducing length bias during GPT-based evaluation', 'answer': 'further solidifying its robustness across multiple tasks'}, {'context': 'On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin.', 'question': 'DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217 on', 'answer': 'On math tasks'}, {'context': 'A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks.', 'question': 'What do the benchmarks LiveCodeBench and Codeforces use?', 'answer': 'reasoning-focused models'}, {'context': 'On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified.', 'question': 'What does OpenAI-o1-1217 outperform DeepSeek-R1 on Aid', 'answer': 'Aider'}, {'context': 'We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. 3.2.', 'question': 'What are the most common questions that DeepSeek-R1 can ask?', 'answer': 'RL training data'}, {'context': 'Distilled Model Evaluation Model AIME 2024 MATH-500 GPQA LiveCode CodeForces Diamond Bench pass@1 cons@64 pass@1 pass@1 pass@1 rating GPT-4o-0513 9.3 13.4 74.6 49.9 32.9 759 Claude-3.5-Sonnet-1022 16.0 26.7 78.3 65.0 38.9 717 OpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820 QwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 1316 DeepSeek-R1-Distill-Qwen-1.5B 28.9 52.7 83.9 33.8 16.9 954 DeepSeek-R1-Distill-Qwen-7B 55.5 83.3 92.8 49.1 37.6 1189 DeepSeek-R1-Distill-Qwen-14B 69.7 80.0 93.9 59.1 53.1 1481 DeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 1691 DeepSeek-R1-Distill-Llama-8B 50.4 80.0 89.1 49.0 39.6 1205 DeepSeek-R1-Distill-Llama-70B 70.0 86.7 94.5 65.2 57.5 1633 Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks.', 'question': 'What is the name of the model that is used to evaluate a distilled model?', 'answer': 'Comparison of DeepSeek-R1'}, {'context': 'As shown in Table 5, simply distilling DeepSeek-R1’s outputs enables the efficient DeepSeek- R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non- reasoning models like GPT-4o-0513 across the board.', 'question': 'What does the DeepSeek-R1-7B outperform other models?', 'answer': 'GPT-4o-0513'}, {'context': 'DeepSeek-R1-14B surpasses QwQ-32B- Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla- tion.', 'question': 'What does DeepSeek-R1-14B surpass on all evaluation metrics?', 'answer': 'QwQ-32B- Preview'}, {'context': 'Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. 4. Discussion 4.1.', 'question': 'What did we find about applying RL to the distilled models?', 'answer': 'significant further gains'}, {'context': 'Distillation v.s. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results.', 'question': 'What is the name of the model that can be used to help us learn?', 'answer': 'DeepSeek-R1'}, {'context': 'However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?', 'question': 'What is the first question that can be asked about the model?', 'answer': 'can the model achieve comparable performance'}, {'context': 'To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B.', 'question': 'What is the deep-seek-r1-zero-Qwen-32B', 'answer': 'DeepSeek-R1-Zero-Qwen-32B'}, {'context': 'The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale 14  Model AIME 2024 MATH-500 GPQA Diamond LiveCodeBench pass@1 cons@64 pass@1 pass@1 pass@1 QwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 DeepSeek-R1-Zero-Qwen-32B 47.0 60.0 91.6 55.0 40.2 DeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 Table 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks.', 'question': 'What is the 32B base model?', 'answer': '14  Model AIME 2024'}, {'context': 'RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1- Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks.', 'question': 'What is the name of the test that results in the best performance?', 'answer': 'RL training'}, {'context': 'Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation.', 'question': 'What are the results of distilling more powerful models into smaller ones?', 'answer': 'excellent results'}, {'context': 'Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger- scale reinforcement learning. 4.2.', 'question': 'What are the most important questions to ask?', 'answer': 'distillation strategies are both economical and effective'}, {'context': 'Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way.', 'question': 'What did DeepSeek-R1 fail to generate?', 'answer': 'Unsuccessful Attempts'}, {'context': 'We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models.', 'question': 'What are the failures of the approaches discussed in this article?', 'answer': 'incapable of developing effective reasoning models'}, {'context': 'Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023).', 'question': 'What is the PRM?', 'answer': 'Process Reward Model'}, {'context': 'However, in practice, PRM has three main limitations that may hinder its ultimate suc- cess. First, it is challenging to explicitly define a fine-grain step in general reasoning.', 'question': 'What is the first limitation of PRM?', 'answer': 'it is challenging to explicitly define a fine-grain step in general reasoning'}, {'context': 'Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not con- ducive to scaling up.', 'question': 'What are the first two steps of an annotation?', 'answer': 'current intermediate step is correct'}, {'context': 'Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline.', 'question': 'What is the main reason for the need for additional training resources?', 'answer': 'reward hacking'}, {'context': 'In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments.', 'question': 'What is the name of the model that PRM can help you with?', 'answer': 'guided search'}, {'context': 'Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil- ver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability.', 'question': 'What did we use to enhance test-time compute scalability?', 'answer': 'Monte Carlo Tree Search (MCTS)'}, {'context': 'This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically.', 'question': 'What is the first step in generating questions from a model?', 'answer': 'breaking answers into smaller parts'}, {'context': 'To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model.', 'question': 'How do we find answers to questions?', 'answer': 'MCTS'}, {'context': 'Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training.', 'question': 'What do we use to train both the actor model and the value model?', 'answer': 'question-answer pairs'}, {'context': 'First, unlike chess, where the search space is relatively well-defined, token generation presents an 15  exponentially larger search space.', 'question': 'What is the search space in token generation?', 'answer': '15  exponentially larger search space'}, {'context': 'To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly influences the quality of generation since it guides each step of the search process.', 'question': 'What does the value model directly influence the quality of generation?', 'answer': 'it guides each step of the search process'}, {'context': 'Training a fine-grained value model is inherently difficult, which makes it challenging for the model to iteratively improve.', 'question': 'What are the most common questions you can ask from a model?', 'answer': 'fine-grained value model'}, {'context': 'While AlphaGo’s core success relied on training a value model to progressively enhance its performance, this principle proves difficult to replicate in our setup due to the complexities of token generation.', 'question': 'What is the name of the tokens that AlphaGo uses?', 'answer': 'value model'}, {'context': 'In conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a significant challenge. 5.', 'question': 'What can be generated from MCTS?', 'answer': 'pre-trained value model'}, {'context': 'Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning.', 'question': 'What are the conclusions of this work?', 'answer': 'we share our journey in enhancing model reasoning abilities through reinforcement learning'}, {'context': 'DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning.', 'question': 'What is the deep-seek-r1-zero approach?', 'answer': 'pure RL approach without relying on cold-start data'}, {'context': 'Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models.', 'question': 'DeepSeek-R1 can generate questions from what?', 'answer': 'small dense models'}, {'context': 'We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models.', 'question': 'What is the teacher model used for?', 'answer': 'to generate 800K training samples'}, {'context': 'The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH.', 'question': 'What is the name of the test?', 'answer': 'DeepSeek-R1-Distill-Qwen'}, {'context': 'Other dense models also achieve impressive results, significantly outperforming other instruction- tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1.', 'question': 'What are the main questions for DeepSeek-R1?', 'answer': 'research across the following directions'}, {'context': '• General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output.', 'question': 'What is the general capability of DeepSeek-R1?', 'answer': 'General Capability'}, {'context': 'Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields.', 'question': 'What are the first questions we want to ask?', 'answer': 'how long CoT can be leveraged to enhance tasks in these fields'}, {'context': '• Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages.', 'question': 'What languages are supported in DeepSeek-R1?', 'answer': 'Chinese and English'}, {'context': 'For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates.', 'question': 'What language might DeepSeek-R1 use?', 'answer': 'English'}, {'context': '• Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance.', 'question': 'What does DeepSeek-R1 use to generate questions?', 'answer': 'Few-shot prompting'}, {'context': 'Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.', 'question': 'What are the most common questions that users can ask?', 'answer': 'output format using a zero-shot setting for optimal results'}, {'context': '• Software Engineering Tasks: Due to the long evaluation times, which impact the effi- ciency of the RL process, large-scale RL has not been applied extensively in software engineering tasks.', 'question': 'What is the most common type of RL used in software engineering tasks?', 'answer': 'large-scale'}, {'context': 'As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks.', 'question': 'What does DeepSeek-R1 not show a huge improvement over DeepSeek-V', 'answer': 'software engineering benchmarks'}, {'context': 'Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency. 16  References AI@Meta. Llama 3.1 model card, 2024.', 'question': 'What will be generated from the AI@Meta. Llama 3.1 model card?', 'answer': '16  References'}, {'context': 'URL https://github.com/meta-llama/llama-m odels/blob/main/models/llama3_1/MODEL_CARD.md. Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3 -5-sonnet.', 'question': 'What is the name of the Claude 3.5 sonnet?', 'answer': 'claude'}, {'context': 'M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A.', 'question': 'What are some of the questions that you can ask?', 'answer': 'H. P. de Oliveira Pinto'}, {'context': 'Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba.', 'question': 'What is the name of the author of the book?', 'answer': 'D. Cummings'}, {'context': 'Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al.', 'question': 'What is the name of the paper that was published in 2021?', 'answer': 'CoRR'}, {'context': 'The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.', 'question': 'What is the name of the preprint of the book?', 'answer': 'arXiv'}, {'context': 'X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024. URL https: //arxiv.org/abs/2309.17179. L. Gao, J. Schulman, and J. Hilton.', 'question': 'What is the name of the tree-search that can help with large language model decoding and training', 'answer': 'Alphazero'}, {'context': 'Scaling laws for reward model overoptimization, 2022. URL https://arxiv.org/abs/2210.10760. A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X.', 'question': 'What are the questions that you can ask from the axiv database?', 'answer': 'Scaling laws for reward model overoptimization'}, {'context': 'Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or g/10.48550/arXiv.2406.04127. Google. Our next-generation model: Gemini 1.5, 2024.', 'question': 'What is the name of the next-generation Gemini model?', 'answer': 'Gemini 1.5, 2024'}, {'context': 'URL https://blog.google/techno logy/ai/google-gemini-next-generation-model-february-2024. Y. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi- nese simpleqa: A chinese factuality evaluation for large language models.', 'question': 'What is the name of the next generation model?', 'answer': 'february-2024'}, {'context': 'arXiv preprint arXiv:2411.07140, 2024. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.', 'question': 'What is the name of the preprint for the book?', 'answer': 'arXiv'}, {'context': 'Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.', 'question': 'What is the name of the preprint of C-Eval?', 'answer': 'arXiv:2305.08322, 2023'}, {'context': 'N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024.', 'question': 'What is the name of the project that was created by N. Jain?', 'answer': 'Livecodebench'}, {'context': 'URL https://doi.org/10.48550/arXiv.2403.07974. 17  S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation.', 'question': 'What is the URL of the augmented generation?', 'answer': 'unified evaluation'}, {'context': 'CoRR, abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485 50/arXiv.2409.12941. A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs, et al.', 'question': 'What is the abs/2409.12941?', 'answer': 'CoRR, abs/2409.12941, 2024'}, {'context': 'Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin.', 'question': 'What is the name of the preprint of the book?', 'answer': 'arXiv'}, {'context': 'CMMLU: Measur- ing massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, 2023. T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica.', 'question': 'What is the name of the preprint of CMMLU?', 'answer': 'arXiv'}, {'context': 'From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe.', 'question': 'What is the name of the preprint of arXiv?', 'answer': '2406.11939, 2024'}, {'context': 'Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023. B. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL https://github.com/WildEval/ZeroEval. MAA. American invitational mathematics examination - aime.', 'question': 'What is the name of the preprint for the book?', 'answer': 'arXiv:2305.20050, 2023'}, {'context': 'In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math -competitions/american-invitational-mathematics-examination-aime. OpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/. OpenAI.', 'question': 'What is the name of the American Invitational Mathematics Examination?', 'answer': 'AIME 2024'}, {'context': 'Learning to reason with llms, 2024b. URL https://openai.com/index/learnin g-to-reason-with-llms/. OpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing -simpleqa/. OpenAI.', 'question': 'What is the name of the book that opens a new question from OpenAI?', 'answer': 'SimpleQA'}, {'context': 'Introducing SWE-bench verified we’re releasing a human-validated subset of swe- bench that more, 2024d. URL https://openai.com/index/introducing-swe-bench -verified/. Qwen. Qwq: Reflect deeply on the boundaries of the unknown, 2024a.', 'question': 'What are the questions that you can ask from the Introducing SWE-bench?', 'answer': 'human-validated subset'}, {'context': 'URL https://qwenlm .github.io/blog/qwq-32b-preview/. Qwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b log/qwen2.5. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman.', 'question': 'What is the name of the party of foundation models?', 'answer': 'A party'}, {'context': 'GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.', 'question': 'What is the name of the benchmark that is google proof?', 'answer': 'q&a'}, {'context': 'arXiv preprint arXiv:2402.03300, 2024. D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis.', 'question': 'What is the name of the preprint of arXiv?', 'answer': 'arXiv:2402.03300, 2024'}, {'context': 'Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017a. URL http://arxiv.org/abs/1712.01815.', 'question': 'What is the abs/1712.01815?', 'answer': 'URL'}, {'context': '18  D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis.', 'question': 'What is the name of the author of the book?', 'answer': 'D. Hassabis'}, {'context': 'Mastering the game of go without human knowledge. Nat., 550(7676):354–359, 2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270. C. Snell, J. Lee, K. Xu, and A. Kumar.', 'question': 'What is the name of the study that was published in 2017b?', 'answer': 'Nat., 550'}, {'context': 'Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033 14. T. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations.', 'question': 'What are the most common questions that can be asked about scaling llm test-time?', 'answer': 'without human demonstrations'}, {'context': 'Nature, 2024. doi: 10.1038/s41586-023-06747-5. J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.', 'question': 'What is the name of the book that was published in 2022?', 'answer': 'Nature'}, {'context': 'P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A label- free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, 2023. X. Wang, J. Wei, D. Schuurmans, Q.', 'question': 'What is the name of the preprint of Math-sheerd?', 'answer': 'arXiv:2312.08935, 2023'}, {'context': 'Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X.', 'question': 'What is the name of the preprint of arXiv?', 'answer': '2203.11171, 2022'}, {'context': 'He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024. URL https://doi.org/10.48550/arXiv.2406.01574.', 'question': 'What is the name of the newest version of Mmlu-pro?', 'answer': 'W. Chen'}, {'context': 'C. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024. H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q.', 'question': 'What is the name of the book that is based on the agentless method?', 'answer': 'arXiv preprint'}, {'context': 'Du, W. Gao, Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL https://arxiv.org/abs/2408.08152.', 'question': 'What is the name of the version of Deepseek-prover?', 'answer': 'v1.5'}, {'context': 'J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. 19  Appendix A.', 'question': 'What is the name of the preprint of arXiv?', 'answer': 'Appendix A'}, {'context': 'Contributions and Acknowledgments Core Contributors Daya Guo Dejian Yang Haowei Zhang Junxiao Song Ruoyu Zhang Runxin Xu Qihao Zhu Shirong Ma Peiyi Wang Xiao Bi Xiaokang Zhang Xingkai Yu Yu Wu Z.F.', 'question': 'What are the core contributors?', 'answer': 'Contributions and Acknowledgments'}, {'context': 'Wu Zhibin Gou Zhihong Shao Zhuoshu Li Ziyi Gao Contributors Aixin Liu Bing Xue Bingxuan Wang Bochao Wu Bei Feng Chengda Lu Chenggang Zhao Chengqi Deng Chong Ruan Damai Dai Deli Chen Dongjie Ji Erhang Li Fangyun Lin Fucong Dai Fuli Luo* Guangbo Hao Guanting Chen Guowei Li H. Zhang Hanwei Xu Honghui Ding Huazuo Gao Hui Qu Hui Li Jianzhong Guo Jiashi Li Jingchang Chen Jingyang Yuan Jinhao Tu Junjie Qiu Junlong Li J.L.', 'question': 'Who contributed to Wu Zhibin Gou?', 'answer': 'Gao Contributors'}, {'context': 'Cai Jiaqi Ni Jian Liang Jin Chen Kai Dong Kai Hu* Kaichao You Kaige Gao Kang Guan Kexin Huang Kuai Yu Lean Wang Lecong Zhang Liang Zhao Litong Wang Liyue Zhang Lei Xu Leyi Xia Mingchuan Zhang Minghua Zhang Minghui Tang Mingxu Zhou Meng Li Miaojun Wang Mingming Li Ning Tian Panpan Huang Peng Zhang Qiancheng Wang Qinyu Chen Qiushi Du Ruiqi Ge* Ruisong Zhang Ruizhe Pan Runji Wang R.J. Chen R.L.', 'question': 'What is the name of the person who asked the question?', 'answer': 'Cai Jiaqi Ni Jian'}, {'context': 'Jin 20  Ruyi Chen Shanghao Lu Shangyan Zhou Shanhuang Chen Shengfeng Ye Shiyu Wang Shuiping Yu Shunfeng Zhou Shuting Pan S.S. Li Shuang Zhou Shaoqing Wu Shengfeng Ye Tao Yun Tian Pei Tianyu Sun T. Wang Wangding Zeng Wen Liu Wenfeng Liang Wenjun Gao Wenqin Yu* Wentao Zhang W.L.', 'question': 'Who is the Jin 20 Ruyi Chen Shanghao Lu Shangyan Zhou', 'answer': 'Jin 20  Ruyi Chen Shanghao Lu Shangyan'}, {'context': 'Xiao Wei An Xiaodong Liu Xiaohan Wang Xiaokang Chen Xiaotao Nie Xin Cheng Xin Liu Xin Xie Xingchao Liu Xinyu Yang Xinyuan Li Xuecheng Su Xuheng Lin X.Q.', 'question': 'What are the names of the questions that you can ask?', 'answer': 'Su Xuheng Lin X.Q'}, {'context': 'Li Xiangyue Jin Xiaojin Shen Xiaosha Chen Xiaowen Sun Xiaoxiang Wang Xinnan Song Xinyi Zhou Xianzu Wang Xinxia Shan Y.K. Li Y.Q. Wang Y.X.', 'question': 'What are the names of the people who are in the Xinnan Song Xin', 'answer': 'Jin Xiaojin Shen Xiaosha Chen Xiaowen Sun Xiaoxiang'}, {'context': 'Wei Yang Zhang Yanhong Xu Yao Li Yao Zhao Yaofeng Sun Yaohui Wang Yi Yu Yichao Zhang Yifan Shi Yiliang Xiong Ying He Yishi Piao Yisong Wang Yixuan Tan Yiyang Ma* Yiyuan Liu Yongqiang Guo Yuan Ou Yuduan Wang Yue Gong Yuheng Zou Yujia He Yunfan Xiong Yuxiang Luo Yuxiang You Yuxuan Liu Yuyang Zhou Y.X.', 'question': 'What are the names of the people who are in the Yin Yang family?', 'answer': 'Wei Yang Zhang Yanhong Xu Yao Li Yao Zhao Yaofeng Sun Yaohui'}, {'context': 'Zhu Yanping Huang Yaohui Li Yi Zheng Yuchen Zhu Yunxian Ma Ying Tang Yukun Zha Yuting Yan Z.Z.', 'question': 'What are the names of the people who can help you?', 'answer': 'Ma Ying Tang Yukun Zha Yuting Yan Z.Z'}, {'context': 'Ren Zehui Ren Zhangli Sha Zhe Fu Zhean Xu Zhenda Xie Zhengyan Zhang Zhewen Hao Zhicheng Ma Zhigang Yan Zhiyu Wu Zihui Gu 21  Zijia Zhu Zijun Liu* Zilin Li Ziwei Xie Ziyang Song Zizheng Pan Zhen Huang Zhipeng Xu Zhongyu Zhang Zhen Zhang Within each role, authors are listed alphabetically by the first name.', 'question': 'Who is the author of the role?', 'answer': 'the first name'}, {'context': 'Names marked with * denote individuals who have departed from our team. 22', 'question': 'What are the names of the individuals who have departed from our team?', 'answer': 'Names marked with *'}, {'context': 'DualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles.', 'question': 'What is the name of the algorithm that created the DeepSeek-V3 Technical Report?', 'answer': 'DualPipe'}, {'context': 'For detailed information on computation-communication overlap, please refer to the profile data.', 'question': 'What are the questions that you can generate from the profile data?', 'answer': 'computation-communication overlap'}, {'context': 'Pipeline Bubbles and Memory Usage Comparison | Method    | Bubble                  | Parameter | Activation | |:---------:|:-----------------------:|:---------:|:----------:| | 1F1B      | (PP-1)(𝐹+𝐵)            | 1×        | PP         | | ZB1P      | (PP-1)(𝐹+𝐵-2𝑊)         | 1×        | PP         | | DualPipe  | (PP/2-1)(𝐹&amp;𝐵+𝐵-3𝑊)     | 2×        | PP+1       | 𝐹 denotes the execution time of a forward chunk, 𝐵 denotes the execution time of a full backward chunk, 𝑊 denotes the execution time of a \"backward for weights\" chunk, and 𝐹&amp;𝐵 denotes the execution time of two mutually overlapped forward and backward chunks.', 'question': 'What are the names of the bubbles and memory usage comparison?', 'answer': 'Pipeline Bubbles and Memory Usage Comparison'}, {'context': '<h3>About</h3> A bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training <code>DualPipe was created and developed by Jiashi Li and Chengqi Deng and Wenfeng Liang.</code>  Here, we publicly share profiling data from our training and inference framework to help the community better understand the communication-computation overlap strategies and low-level implementation details.', 'question': 'What are the main questions that you can ask about DualPipe?', 'answer': 'communication-computation overlap strategies and low-level implementation details'}, {'context': 'The profiling data was captured using the PyTorch Profiler. After downloading, you can visualize it directly by navigating to chrome://tracing in the Chrome browser (or edge://tracing in the Edge browser).', 'question': 'What did you generate questions from?', 'answer': 'PyTorch Profiler'}, {'context': 'Notice that we simulate an absolutely balanced MoE routing strategy for profiling. <h2>Training</h2> The training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe.', 'question': 'What does the training profile data show?', 'answer': 'our overlapping strategy'}, {'context': 'Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.', 'question': 'What are the 4 layers of MoE?', 'answer': 'Mixture of Experts'}, {'context': '<h2>Inference</h2> <h3>Prefilling</h3> For prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU.', 'question': 'What does DeepSeek generate questions from?', 'answer': 'online deployment'}, {'context': 'In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.', 'question': 'What are the questions that are generated from?', 'answer': 'micro-batches'}, {'context': '<h3>Decoding</h3> For decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU.', 'question': 'What does the profile employ for decoding?', 'answer': 'EP128, TP1'}, {'context': 'Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication.', 'question': 'Decoding leverages two micro-batches for computation and all-to-all communication.', 'answer': 'decoding'}, {'context': 'However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished.', 'question': 'What does the system wait for after the computation has finished?', 'answer': 'all-to-all communication'}, {'context': 'For more information about the all-to-all implementation, please refer to DeepEP. When using expert parallelism (EP), different experts are assigned to different GPUs.', 'question': 'What are the experts assigned to different GPUs?', 'answer': 'expert parallelism'}, {'context': 'Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced.', 'question': 'What are the most common questions that you can ask?', 'answer': 'different experts may vary depending on the current workload'}, {'context': 'As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs.', 'question': 'What is the name of the paper that describes the strategy that is used to duplicate heavy-loaded experts', 'answer': 'DeepSeek-V3'}, {'context': 'Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.', 'question': 'What are the experts of the same group to the same node?', 'answer': 'to reduce inter-node data traffic'}, {'context': 'To facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads.', 'question': 'What is the name of the algorithm that we open-source?', 'answer': 'EP load balancing algorithm'}, {'context': \"Note that the exact method to predict the loads of experts is out of this repo's scope. A common method is to use moving average of historical statistics. <h2>The Algorithm</h2> The load balancing algorithm comes with two policies used for different cases.\", 'question': 'What is the name of the repo that can be used to generate questions from?', 'answer': 'Algorithm'}, {'context': '<h2>Hierarchical Load Balancing</h2> When the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing.', 'question': 'What is the hierarchical load balancing policy?', 'answer': 'to harness the group-limited expert routing'}, {'context': 'We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced.', 'question': 'What do we generate questions from?', 'answer': 'replicated experts to individual GPUs'}, {'context': 'The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.', 'question': 'What can be generated from the hierarchical load balancing policy?', 'answer': 'smaller expert-parallel size'}, {'context': '<h3>Global Load Balancing</h3> In other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs.', 'question': 'What does Global Load Balancing policy use?', 'answer': 'global load balancing'}, {'context': 'This policy can be adopted in decoding stage with a larger expert-parallel size. The Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads.', 'question': 'What is the policy for a larger expert-parallel size?', 'answer': 'decoding stage'}, {'context': 'It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications.', 'question': 'What does the shared storage layer help you create?', 'answer': 'distributed applications'}, {'context': 'Key features and benefits of 3FS include: <ul> <li> Performance and Usability <ul> <li>Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.</li> <li>Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application code simple and easy to reason about.</li> <li>File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB).', 'question': 'What is the name of the feature that 3FS provides?', 'answer': 'Performance and Usability'}, {'context': 'The file interface is well known and used everywhere.', 'question': 'What are the most common questions that can be generated from the file interface?', 'answer': 'well known and used everywhere.'}, {'context': 'There is no need to learn a new storage API.</li> </ul> </li> <li> Diverse Workloads <ul> <li>Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.</li> <li>Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.</li> <li>Checkpointing Supports high-throughput parallel checkpointing for large-scale training.</li> <li>KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.</li> </ul> </li> </ul> <h2>Performance</h2>  <li>Peak throughput</li>  The following figure demonstrates the throughput of read stress test on a large 3FS cluster.', 'question': 'What is the need to learn a new storage API?', 'answer': 'There is no'}, {'context': 'This cluster consists of 180 storage nodes, each equipped with 2×200Gbps InfiniBand NICs and sixteen 14TiB NVMe SSDs. Approximately 500+ client nodes were used for the read stress test, with each client node configured with 1x200Gbps InfiniBand NIC.', 'question': 'What did the cluster generate?', 'answer': '180 storage nodes'}, {'context': 'The final aggregate read throughput reached approximately 6.6 TiB/s with background traffic from training jobs. <li>GraySort</li>  We evaluated smallpond using the GraySort benchmark, which measures sort performance on large-scale datasets.', 'question': 'What did we use to generate questions from?', 'answer': 'GraySort benchmark'}, {'context': 'Our implementation adopts a two-phase approach: (1) partitioning data via shuffle using the prefix bits of keys, and (2) in-partition sorting. Both phases read/write data from/to 3FS.', 'question': 'What is the first step of the two-phase approach?', 'answer': 'partitioning data via shuffle'}, {'context': 'The test cluster comprised 25 storage nodes (2 NUMA domains/node, 1 storage service/NUMA, 2×400Gbps NICs/node) and 50 compute nodes (2 NUMA domains, 192 physical cores, 2.2 TiB RAM, and 1×200 Gbps NIC/node).', 'question': 'What did the test cluster generate?', 'answer': '25 storage nodes'}, {'context': 'Sorting 110.5 TiB of data across 8,192 partitions completed in 30 minutes and 14 seconds, achieving an average throughput of 3.66 TiB/min. <li>KVCache</li>  KVCache is a technique used to optimize the LLM inference process.', 'question': 'What is the name of the technique used to optimize the LLM inference process?', 'answer': 'KVCache</li>  KVCache'}, {'context': 'It avoids redundant computations by caching the key and value vectors of previous tokens in the decoder layers.', 'question': 'What does the encoding avoid?', 'answer': 'redundant computations'}, {'context': 'The top figure demonstrates the read throughput of all KVCache clients, highlighting both peak and average values, with peak throughput reaching up to 40 GiB/s.', 'question': 'What is the read throughput of all KVCache clients?', 'answer': '40 GiB/s'}, {'context': 'The bottom figure presents the IOPS of removing ops from garbage collection (GC) during the same time period.', 'question': 'What are the IOPS of removing ops from garbage collection?', 'answer': 'The bottom figure'}, {'context': '<source name=\"https://medium.com/@visithkumarapperuma/deepseek-v3-a-game-changer-in-a-i-heres-why-it-matters-75591957ca07\"> author - Visith Kumarapperuma  Currently, the AI models from the Chinese startup Deepseek are causing quite a stir in the AI space.', 'question': 'What is the source of the article?', 'answer': 'https://medium.com'}, {'context': 'Their latest reasoning model, Deepseek r1, shows better or equal performance to competitors. But above all, they achieved it with a fraction of the training and inference cost.', 'question': 'What is the latest model of Deepseek?', 'answer': 'Deepseek r1'}, {'context': 'DeepSeek’s AI Assistant overtook ChatGPT to become the most downloaded free app on the U.S. App Store. This development has led to market concerns about A.I. investments to major U.S. tech companies. Impacting share prices of tech firms including Nvidia.', 'question': 'What is the most downloaded free app on the U.S. App Store?', 'answer': 'ChatGPT'}, {'context': '<h2>So what made Deepseek such a big impact to A.I. ?</h2> The significance of Deepseek as a disruptor in the industry lies in its approach. Unlike other companies that pushed for better hardware, Deepseek improved the algorithms.', 'question': 'What is the significance of Deepseek as a disruptor in the industry?', 'answer': 'its approach'}, {'context': 'Thus achieving better results at a software level. Note that the following details are for the Deepseek V3 model. • Deepseek said it trained a model using a data centre of some 2,000 of Nvidia H800 GPUs.', 'question': 'What is the name of the software that Deepseek used to train its model?', 'answer': 'Nvidia H800 GPUs'}, {'context': '• Time duration 2 months with the cost of the *final training run being ~$5.5 million This ~$5.5M reflects the “rental” cost for the GPU hours needed to train DeepSeek‑V3. It does not include: 1. The capital expenditure for owning the hardware. 2.', 'question': 'What are the most common questions that DeepSeekV3 can generate?', 'answer': 'The capital expenditure for owning the hardware. 2'}, {'context': 'Costs associated with prior research, ablation studies, or experiments on alternative architectures/algorithms/data.', 'question': 'What are the costs associated with prior research?', 'answer': 'Costs'}, {'context': '<h3>Deepseek made training more efficient (45 times more efficient)</h3> <ul> <li>Use 8-bit instead of 32-bit to save memory.</li> <li>Compress key value indices which eat up a lot of VRAM; they got 93% compression ratios.</li> <li>Do multi-token prediction instead of single-token prediction -&gt; doubled inference speeds</li> <li>The MOE model decomposes a big model into small models that can run on consumer-grade hardware.</li> </ul> <h2>Summary of how Deepseek v3 was so efficient at training the frontier model</h2>  <li>Model Architecture The model employs a Mixture-of-Experts (MoE) architecture, where only 37B parameters fire for each token out of the total 671B.', 'question': 'How did Deepseek make training more efficient?', 'answer': '45 times'}, {'context': 'This sparse activation significantly reduces compute requirements compared to dense models. The model uses Multi-head Latent Attention (MLA).', 'question': 'What does the sparse activation of the model generate?', 'answer': 'compute requirements'}, {'context': 'This compresses the Key-Value cache, reducing memory usage and enabling more efficient training.</li> <li>FP8 Mixed Precision Training: They implemented an FP8 mixed precision training framework.', 'question': 'What did they implement?', 'answer': 'an FP8 mixed precision training framework'}, {'context': 'Which reduces memory usage and accelerates training compared to higher precision formats. Reduced memory footprint by up to 50% compared to traditional FP16/FP32 formats.', 'question': 'What does the FP16/FP32 format use to generate questions?', 'answer': 'memory footprint'}, {'context': 'They use fine-grained quantisation strategies and increased accumulation precision to maintain accuracy.</li> <li>Load Balancing Strategy They pioneered an auxiliary loss-free strategy for load balancing in the MoE architecture.', 'question': 'What did the MoE architecture use to determine load balancing?', 'answer': 'Load Balancing Strategy'}, {'context': 'This improved performance without the drawbacks of traditional auxiliary loss methods.</li> <li>Training Framework They developed a custom training framework called HAI-LLM with several optimisations: DualPipe algorithm for efficient pipeline parallelism.', 'question': 'What did HAI-LLM help with?', 'answer': 'efficient pipeline parallelism'}, {'context': 'This reduces pipeline bubbles and overlapping computation and communication. Efficient cross-node all-to-all communication kernels to fully utilise network bandwidth.', 'question': 'What reduces pipeline bubbles and overlapping computation and communication?', 'answer': 'Efficient cross-node all-to-all communication kernels'}, {'context': 'Careful memory optimisations to avoid using costly tensor parallelism.</li>  <h2>Breakdown of the costs of the Deepseek v3 model</h2> Deepseek’s flagship model v3 showcases an architecture with a 671B parameter MOE (Mixture of Agents) with 37B active parameters per token - Their success stems from breakthrough engineering: using MoE architecture, implementing FP8 mixed precision training, and developing a custom HAI-LLM framework.', 'question': 'What is the name of the Deepseek v3 model?', 'answer': 'flagship model v3'}, {'context': '- Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5 Sonnet. - For writing and coding tasks, Claude 3.5 Sonnet maintains a slight lead.', 'question': 'Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5', 'answer': 'Deepseek excels at reasoning and math'}, {'context': '- Deepseek pre-trained this model on 14.8 trillion high-quality data, taking 2,788,000 GPU hours on the Nvidia h800s cluster, costing around only $6 million - the Llama 403b was trained on 11x of that, taking 30,840,000 GPU hours, also on 15 trillion tokens.', 'question': 'How was the Llama 403b trained?', 'answer': '11x'}, {'context': '<code>So how true is the claim of $5.5 million, or is it another marketing trick?</code>  <li>Underlying FLOP calculations Model Details:</li> <li>Active Parameters: 37B (using FP8 precision)</li> <li>FLOPs per token: Using the rule of thumb “6 FLOPs per parameter per token.” <code>37B×6 = 222B FLOPs per token</code></li> <li>Total Training Tokens: Approximately 14.8 trillion tokens</li> <li>Total FLOPs required: <code>222 B FLOPs/token×14.8 T tokens ≈ 3.3×10²⁴ FLOPs</code></li>  <h3>GPU FLOP Capacity (H800/H100):</h3> An H100 is roughly estimated to deliver about.', 'question': 'How much money does the claim of $5.5 million make?', 'answer': 'marketing trick'}, {'context': '3.958×10¹⁵ FLOPs (per second or per some standardised interval — here used as a comparative metric). Ideal (Perfect Efficiency) GPU hours.', 'question': 'What is the metric used to measure GPU hours?', 'answer': 'Ideal'}, {'context': '(Dividing total required FLOPs by per‑GPU capability gives) <code>3.3×10²⁴ / 3.958×10¹⁵ \\u200b≈ 8.33×10⁸ seconds⇒≈0.4 million GPU hour</code> Note: This “perfect efficiency” scenario is a lower bound. Real-world training is less efficient. 2.', 'question': 'What is the smallest amount of FLOPs required?', 'answer': '8.33×10⁸ seconds⇒≈0'}, {'context': 'Adjusting for Real‑World Inefficiencies (Comparison with Llama 3.1) Reference Model: Llama 3.1 (405B parameters, 15 T tokens) reportedly required 30.84 M GPU hours in practice.', 'question': 'What is the reference model for Llama 3.1?', 'answer': '405B parameters, 15 T tokens'}, {'context': 'Recalculating FLOPs for Llama 3.1: <code>Using the same math: 3.64×10²⁵ FLOPs required</code> Scaling Efficiency Using the ratio of FLOPs needed for DeepSeek‑V3 versus Llama 3.1. and assuming similar inefficiencies.', 'question': 'What is the ratio of FLOPs needed for DeepSeekV3 versus Ll', 'answer': '3.64×10²⁵'}, {'context': 'The estimate adjusts to roughly 2.79M GPU hours for DeepSeek‑V3 training. 3.', 'question': 'What is the estimated GPU hours for DeepSeekV3?', 'answer': '2.79M'}, {'context': 'DeepSeek‑V3 Reported Training Breakdown According to the DeepSeek‑V3 paper Pre‑training Stage: - Per Trillion Tokens: 180K H800 GPU hours - Overall Pre‑training: Total of 2,664K GPU hours - This stage was completed in less than two months using a cluster of 2,048 H800 GPUs.', 'question': 'What did DeepSeekV3 generate?', 'answer': 'a cluster of 2,048 H800 GPUs'}, {'context': 'Context Length Extension: - Additional 119K GPU hours Post‑training: - An extra 5K GPU hours Total GPU Hours: <code>2,664 K+119 K+5 K≈2.788M GPU hours</code> 4.', 'question': 'What is the total GPU hours?', 'answer': '5K GPU hours'}, {'context': 'Cost Estimation Assumed GPU Rental Price: $2 per GPU hour Total Rental Cost: <code>2.788M GPU hours×$2/hour≈$5.576 million</code> as stated in Deepseek paper During the pre‑training stage, training DeepSeek‑V3 on each trillion tokens requires only 180K H800 GPU hours… Consequently, our pre‑training stage is completed in less than two months and costs 2664K GPU hours.', 'question': 'How many GPU hours does it take to train DeepSeekV3?', 'answer': '180K H800'}, {'context': 'Combined with 119K GPU hours for the context length extension and 5K GPU hours for post‑training, DeepSeek‑V3 costs only 2.788M GPU hours for its full training.', 'question': 'What does DeepSeekV3 require to train?', 'answer': '2.788M GPU hours'}, {'context': 'Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. 5.', 'question': 'What are the costs of training the H800 GPU?', 'answer': '$5.576M. 5'}, {'context': 'Summary Theoretical (Perfect Efficiency) Estimate: ~0.4 M GPU hours (using idealised FLOP counts and assuming perfect hardware utilisation0 Adjusted (Real‑World) Estimate (via Llama 3.1 comparison): ~2.79 GPU hours DeepSeek‑V3 Reported Breakdown: Pre‑training: 2,664K GPU hours Context Extension: 119K GPU hours Post‑training: 5K GPU hours Total: ~2.788 M GPU hours <h3>Cost (at $2 per GPU hour): ~$5.576 million</h3>', 'question': 'How many GPU hours did DeepSeek estimate?', 'answer': '2.79'}, {'context': '<source name=\"https://medium.com/@jjjy213/deepseek-v3-explained-fdac83ba280c\"/> author - Ataka jeong  <li>Introduction How could the DeepSeek-V3 model achieve incredible performance and economical training as an open source model?', 'question': 'What are the questions that you can ask about the DeepSeek-V3 model?', 'answer': 'performance and economical training'}, {'context': 'In this paper review, we will explore the various features that were invented and applied to build the DeepSeek-V3 model.', 'question': 'What are the questions that will be asked in this paper?', 'answer': 'various features that were invented and applied'}, {'context': 'The way the paper presents the model may seem complicated to people who are unfamiliar with the new conceptualization invented by DeepSeek. However, its core principle still resembles that of the standard Transformer and well-known LLMs.', 'question': 'What is the name of the model that DeepSeek created?', 'answer': 'LLMs'}, {'context': 'It will be incredibly helpful to have general knowledge of previously released large language models like LLaMA. I will also add my own interpretation of the DeekSeek model in this story.', 'question': 'What will be added to the story?', 'answer': 'my own interpretation of the DeekSeek model'}, {'context': 'Let’s dive into the new features of model architecture step by step.</li> <li>Model Architecture First of all, we will investigate the core architecture of DeekSeek-V3 model. The DeekSeek-V3 model has inherited most parts of model from previous V2 model.', 'question': 'What are the main questions of the DeekSeek-V3 model?', 'answer': 'core architecture'}, {'context': 'These parts of model were elaborated more in V2 paper, but it is worth noting the principle how V3 model was built upon.', 'question': 'What were the parts of the V3 model built upon?', 'answer': 'V2 paper'}, {'context': 'While they used the structure of the ordinary transformer block like Llama, its attention and Feed-Forward Network were more sophisticated to boost the model performance. The overview of the Transformer block is as shown in the following diagram.', 'question': 'What did the Transformer block have in common with the Llama?', 'answer': 'its attention and Feed-Forward Network were more sophisticated'}, {'context': 'The two main components are Multi-Head Latent Attention(MLA) and DeepSeekMoE.</li> <li>2.1 Multi-Head Latent Attention(MLA) What is Multi-Head Latent Attention(MLA)? You might noticed that “Latent” is only additional word to conventional attention module.', 'question': 'What are the two main components of DeepSeekMoE?', 'answer': 'Multi-Head Latent Attention(MLA)'}, {'context': 'MLA improved the speed and memory usage in the attention block by compressing the input vector. From a data analysis perspective, the data can be compressed into a lower dimension while preserving the information it contains.', 'question': 'What are the most common questions that can be generated from the MLA database?', 'answer': 'speed and memory'}, {'context': 'One of the well-known techniques is Principal component analysis (PCA), which reduces the dimension of the data and maintains variance to retain its information.', 'question': 'What is the name of the technique that is used to analyze data?', 'answer': 'Principal component analysis'}, {'context': 'In latent diffusion model, the input data is compressed by variational autoencoder and reconstructed in initial dimension. The Multi-Head Latent Attention(MLA) applied this principle to compress and decompress the input data.', 'question': 'What are the questions that can be generated from the Latent Diffusion model?', 'answer': 'compress and decompress the input data'}, {'context': 'By storing a compressed vector for the KV cache, the DeepSeek model can improve both speed by reducing data copying and memory efficiency by using a smaller compressed vector.', 'question': 'What can DeepSeek model generate questions from?', 'answer': 'a compressed vector'}, {'context': 'The weight matrix for compression is additionally required, because human cannot compress it manually, but AI should learn it how the compression should be done.', 'question': 'What should be generated from the weight matrix?', 'answer': 'compression'}, {'context': 'Applying RoPE to the compressed vector is not mathematically compatible, which was shown in detail in V2 paper, they used decoupled RoPE. As illustrated in the figure above, RoPE is applied to query and key, but also the query and key without RoPE.', 'question': 'What did the V2 paper show that decoupled RoPE was used to?', 'answer': 'Applying RoPE to the compressed vector'}, {'context': 'The RoPE-applied query and key are then concatenated with their respective non-RoPE counterparts. Finally, the query and key are obtained as normal transformer block, then the computation of dot-product attention will not be different from original one.', 'question': 'What is the RoPE-applied query and key used for?', 'answer': 'concatenated'}, {'context': 'But we could reach this point with a more economical KV cache thanks to the lower dimension of data.</li> <li>2.2 DeekSeekMoE Secondly, you can note that Feed-Forward Network is unusual as it was split into a lot of experts, rather than one large FFN.', 'question': 'What is the name of the network that is split into experts?', 'answer': 'Feed-Forward Network'}, {'context': 'They called it as DeekSeekMoE. Like humans in a group, the AI also needs to specialized in certain domain to improve the performance. Thus, the mixture of experts come into play here.', 'question': 'What is the name of the AI that generates questions from?', 'answer': 'DeekSeekMoE'}, {'context': 'Each expert can specialize in certain domain, in this case, the group of tokens that they are familiar with, instead of coping with entire range of tokens alone.', 'question': 'What can each expert do instead of dealing with entire range of tokens alone?', 'answer': 'coping'}, {'context': 'Dependent on the input sequence(tokens), the certain experts are selected to be activated and they contribute to make output. Shared experts are generalist and are activated for all kind of tokens.', 'question': 'What are the experts selected to be activated?', 'answer': 'tokens'}, {'context': 'Then it might be interesting to know by what algorithm we can select the experts? We need to assign a vector to each expert which determines the range of tokens(domain) that experts can deal with well.', 'question': 'What algorithm can be used to select experts?', 'answer': 'vector'}, {'context': 'And we give score to each expert to check how similar the domain of expert and input token are. If the score is high, then we should select the expert and let them activated to make output. Well, it sounds quite simple. Let’s see the math behind it.', 'question': 'What is the domain of expert and input token?', 'answer': 'similar'}, {'context': 'eᵢ is a centroid vector. It is learned during training and represents the type of input tokens the expert specialized in. Each expert’s centroid vector encodes the knowledge domain it specializes in. uₜ is input vector to FFN.', 'question': 'What is input vector to FFN?', 'answer': 'uₜ'}, {'context': 'The dot product uₜᵀ eᵢ quantifies the similarity between the input vector uₜ\\u200b and the centroid (or domain) of expert eᵢ, effectively measuring the alignment of the input data with the expert’s specialized domain.', 'question': 'What does the dot product utT ei quantify?', 'answer': 'the similarity'}, {'context': 'So, the sᵢ = Sigmoid(uₜᵀ eᵢ) represents the score for each i-th expert, determining whether the expert should be selected. By gating value gᵢ, which select Kᵣ experts with high score by Topk algorithm.', 'question': 'What is the name of the algorithm that selects Kr experts with high score?', 'answer': 'Topk'}, {'context': 'We add all outputs of selected experts and shared experts, then we arrive to the final output. 2.3 Multi-Token Prediction In a standard transformer, the model generates a token each time, and this new token is fed back into decoder as input.', 'question': 'What is the output of experts?', 'answer': 'the final output'}, {'context': 'Since this way restricts the efficiency and the speed of convergence during training, many researchers have made effort to come up with a method to generate multiple tokens each time. DeepSeek improved the conventional way of Multi-Token Prediction(MTP).', 'question': 'What is the name of the method that DeepSeek improved?', 'answer': 'Multi-Token Prediction'}, {'context': 'Instead of previous parallel MTP, DeepSeek decided sequential MTP. They construct independent MTP modules, where the previous output of the Transformer block is concatenated into the subsequent MTP module, as illustrated in the following figure.', 'question': 'What did DeepSeek decide to use instead of parallel MTP?', 'answer': 'sequential MTP'}, {'context': 'As shown in the figure, the structure of MTP modules is akin to RNN model. But, unlike RNN, which preserve hidden states of nodes, the MTP modules send output of prediction to the subsequent module.', 'question': 'What do MTP modules send out?', 'answer': 'output of prediction'}, {'context': 'Even though a single Transformer block cannot generate multiple tokens, the entire system of MTP modules collectively enables multi-token prediction.', 'question': 'What can a single Transformer block generate multiple tokens?', 'answer': 'multi-token prediction'}, {'context': 'As it compares additional tokens per prediction, it provides more information for weight updates during training, leading to more efficient learning and faster convergence. The model can proactively learn and prepare for the additional tokens.', 'question': 'What can the model learn and prepare for?', 'answer': 'additional tokens'}, {'context': 'In actual training, DeepSeek opted to generate only one additional token, presumably due to the computational cost which is caused by using many MTP modules. It necessitate the compromise between the benefits of MTP and computational cost.', 'question': 'What did DeepSeek generate one additional token?', 'answer': 'computational cost'}, {'context': 'During inference, the MTP modules are discarded, generating only one token per prediction.</li> <li>Infrastructure 3.1 DualPipe Since the U.S. did not export great GPUs like the NVIDIA H100 to China, DeepSeek researchers had to devise innovative methods to accelerate model training using the weaker H800 GPUs.', 'question': 'What are the MTP modules discarded?', 'answer': 'one token per prediction'}, {'context': 'Since they succeeded, NVIDIA’s stock price briefly plunged, as people believed that high-performance GPUs would no longer be necessary for training LLM.', 'question': 'What did people think would be more useful for training LLM?', 'answer': 'high-performance GPUs'}, {'context': 'Because the DeepSeek model was trained on 2048 H800 GPUs, communication between GPUs accounts for large portion of training time. Therefore, enhanching networking between GPUs has to play crucial role to reduce training time.', 'question': 'What is the DeepSeek model trained on?', 'answer': '2048 H800 GPUs'}, {'context': 'When we use many GPUs simultaneously, the GPUs have to wait for a certain amount of time until new data is copied from other GPU.', 'question': 'What are the most common questions that you can ask?', 'answer': 'new data is copied from other GPU'}, {'context': 'This waiting time, which causes training inefficiencies, is known as a “bubble,” and we should minimize it as much as possible. DeepSeek invented a innovative method to reduce bubble.', 'question': 'What is the name of the method that DeepSeek invented to reduce bubble?', 'answer': 'bubble'}, {'context': 'During model training, data flows through the model in forward and backward processes. In forward process, data goes from the input layer to the output layer.', 'question': 'What are the questions that can be generated from the model?', 'answer': 'data flows through the model in forward and backward processes'}, {'context': 'On the other hand, during the backward process data moves from the output layer to the input later, updating weights based on the information to minimize the loss.', 'question': 'What is the backward process?', 'answer': 'data moves from the output layer to the input later'}, {'context': 'Prior to DeepSeek, researchers found that the backward process can be split into two processes, which are backward for input and backward for weight, in order to remove more bubbles.', 'question': 'What did researchers find about the backward process?', 'answer': 'the backward process can be split into two processes'}, {'context': 'The backward for input is computation of the gradient of the loss with respect to the input data, whereas the backward for weight calculates gradient of the loss with respect to the weight.', 'question': 'What does the backward for input do?', 'answer': 'computation of the gradient of the loss with respect to the input data'}, {'context': 'The backward for input must be completed ahead of the backward for weight, because it is necessary to compute the backward for weight.', 'question': 'What are the backward for input and backward for weight?', 'answer': 'completed ahead'}, {'context': 'Mathematically, the chain rule is applied to the calculation of backpropagation, where the backward for input is used for the calculation of backward for weight.', 'question': 'What are the questions you can generate from?', 'answer': 'the backward for input is used for the calculation of backward for weight'}, {'context': 'In such process, it is certain that an enormous number of communications between GPUs is required.', 'question': 'What are the most common questions that GPUs may have?', 'answer': 'communications'}, {'context': 'In order to reduce the number of communication, the DeepSeek’s DualPipe combines the forward process and the backward for input by initiating training data from two devices in the opposite directions as illustrated in following figure.</li>  The batch 0 is the initial data, which starts processing on the device 0 and continues on the subsequent devices.', 'question': 'What is the first step in the DualPipe?', 'answer': 'The batch 0'}, {'context': 'In a conventional training plan, the device 7 remains idle, waiting for the batch 0 to be copied onto it. However, DualPipe makes the device 7 start training with other batch data in the opposite direction.', 'question': 'What is the device 7 used to generate questions from?', 'answer': 'batch 0'}, {'context': 'This allows us to combine them as a chunk and continuously copy them together on other devices to reduce communication between GPUs.', 'question': 'What do we use to generate questions?', 'answer': 'continuously copy them together on other devices'}, {'context': 'With weaker H800 GPUs, they couldn’t improve the speed of the GPUs, but reduce the communication between GPUs to accelerate the training.', 'question': 'What are the most common questions that can be asked about weaker H800 GPUs?', 'answer': 'they couldn’t improve the speed'}, {'context': '3.2 Mixed precision training Mixed precision training is already prevalent technique of LLM to improve training and memory efficiency while maintaining the model accuracy.', 'question': 'What are the most common questions that can be asked by LLM?', 'answer': 'Mixed precision training'}, {'context': 'In mixed precision training, it is critical task to find out which parts of model are less significant for the model accuracy and reduce the precision that parts.', 'question': 'What are the most important parts of a mixed precision training?', 'answer': 'model'}, {'context': 'In DeepSeek-V3 model, the researchers have found that they should reduce precision in the parts of model where heavy computations are executed, such as matrix multiplication.', 'question': 'What are the researchers trying to get out of DeepSeek-V3 model?', 'answer': 'reduce precision'}, {'context': 'In contrast, they preserved high precision for matrix addition and storing data, which are relatively lightweight computation. The mixed precision training of DeepSeek is shown in the following figure.', 'question': 'What did DeepSeek preserve high precision for matrix addition and storing data?', 'answer': 'relatively lightweight computation'}, {'context': 'While reducing the precision by the method above, overflow and underflow arise as a impediment. If the numerical values are quantized in lower precision like FP8 format, the values are clipped to the certain representable range.', 'question': 'What are the numerical values clipped to a certain representable range?', 'answer': 'If the numerical values are quantized in lower precision'}, {'context': 'While computation in lower precision, the values can easily exceed the range during the computation. Scaling the values can mitigate the overflow and underflow by adjusting the values and lead to more proper representation in limited range.', 'question': 'What can be generated from the above?', 'answer': 'values'}, {'context': 'But, static scaling, which applies fixed scaling factor to all values, can still cause overflow and underflow for many values. To cope with this issue, DeepSeek implemented Fine-Grained Quantization.', 'question': 'What is the name of the feature that allows you to generate questions from?', 'answer': 'Fine-Grained Quantization'}, {'context': 'In this method, the values are grouped, and each group has its own scaling factor. This approach allows the each group of values to have a more suitable scaling factor, by which the overflow and underflow can be averted.', 'question': 'What is the method used to generate questions from?', 'answer': 'the values are grouped, and each group has its own scaling factor'}, {'context': 'Another issue of quantization is that the small errors can be accumulated and become more serious problem later.', 'question': 'What can be generated from the quantization of errors?', 'answer': 'small errors'}, {'context': 'In order to avoid that a lot of values with error are summed and their errors are accumulated, intermediate values are copied in high precision, if the number of values reaches the interval.', 'question': 'What are the intermediate values copied in high precision?', 'answer': 'if the number of values reaches the interval'}, {'context': 'It means that some values are grouped, and their values are stored in high precision. Then, the errors of values aren’t accumulated on a large scale, because the small group of values don’t contribute to large error.', 'question': 'What are the values that are grouped in high precision?', 'answer': 'some values'}, {'context': 'These two techniques to prevent quantization error are visualized in following figure. <li>Reinforcement Learning After supervised fine-tuning, DeepSeek additionally implemented reinforcement learning.', 'question': 'What are the two techniques to prevent quantization error?', 'answer': 'visualized in following figure'}, {'context': 'A reward model has to be built and trained for reinforcement learning, which gives feedback to the model and determine the direction of learning. The rule-based reward model(RM) and model-based reward model(RM) were employed.', 'question': 'What are the two types of reward models used?', 'answer': 'rule-based reward model(RM) and model-based reward model'}, {'context': 'The rule-based RM is applied to the questions with specific rules, such as math problems and LeetCode problems. In these domains, the specific rules are used to verify the correctness of the answers and the questions about logical reasoning are involved.', 'question': 'What is the rule-based RM used to generate questions from?', 'answer': 'specific rules'}, {'context': 'However, for many questions, the answer cannot be verified by a specific rule. In those cases where no rule is provided, the model-based RM determines, whether the answer matches the ground-truth answer.', 'question': 'What does the model-based RM determine?', 'answer': 'whether the answer matches the ground-truth answer'}, {'context': 'Another innovative idea of DeepSeek is including the chain-of-thought to the reward, whereas conventional models only included final reward based on the answer. DeepSeek-V3 model, as V2 model did, adopted Group Relative POlicy Optimization (GRPO).', 'question': 'What did DeepSeek-V3 model use to generate questions from?', 'answer': 'Group Relative POlicy Optimization'}, {'context': 'This GRPO algorithm maximizes the following objective by updating the policy model π.</li>  Maximize this objective by updating the weights of the model based on the reward. Advantage is defined as the normalized reward.', 'question': 'What is the GRPO algorithm used to maximize?', 'answer': 'the following objective'}, {'context': 'In LLM case, the policy model π is model itself, and θ is weights of the model. q is question and o is output of the model.', 'question': 'What is output of the model?', 'answer': 'o'}, {'context': 'We can interpret the policy model(LLM) outputs a probability distribution over tokens, where the policy π(o|q) is a probability of output o given the question q. Therefore, the policy model is LLM itself.', 'question': 'What is the policy model?', 'answer': 'outputs a probability distribution over tokens'}, {'context': 'If the output o is right answer, we should reinforce the probability of that model makes this output o. So we need to maximize π(o|q) by multiplying advantage(normalized reward).', 'question': 'What should we generate questions from?', 'answer': 'output o is right answer, we should reinforce the probability of that model'}, {'context': 'If the output o is correct, the advantage (reward) will be a positive value and the policy will be reinforced. Otherwise, it will be negative and π(o|q) should be minimized.', 'question': 'What should be minimized if the output o is correct?', 'answer': 'π(o|q)'}, {'context': 'Plus, we have a fine-tuned model as the initial base model and do not want it to go too far from this base model, which might cause model to forget basic language understanding and important knowledge that the model learned during pre-training and fine-tuning.', 'question': 'What do we want to generate questions from?', 'answer': 'fine-tuned model'}, {'context': 'To implement this safety concerns, GRPO algorithm used KL divergence and epsilon parameter. The KL divergence measures the difference between current policy model and reference policy model(initial base model).', 'question': 'What does GRPO algorithm generate questions from?', 'answer': 'KL divergence and epsilon parameter'}, {'context': 'So the KL divergence term should be minimized to maximized the GRPO objective. And we pick minimum between the original policy and the clipped policy in (1-ε, 1+ε), by which the model cannot deviate too much from 1.', 'question': 'What is the KL divergence term?', 'answer': 'GRPO objective'}, {'context': 'So, the current policy cannot differ a lot from the old policy, restricting the effect of reinforcement learning. This GRPO algorithm based on rule-based and model-based reward model enhances model performance and reasoning capability.', 'question': 'What does the GRPO algorithm generate?', 'answer': 'model performance and reasoning capability'}, {'context': '<li>Conclusion DeepSeek-V3 model offered great opportunity for efficient training with cheaper GPUs. It is unclear that its performance exceeds the OpenAI model, but DeepSeek is way more economical to train and open-source model.', 'question': 'What are the main questions that you can ask about DeepSeek-V3?', 'answer': 'its performance exceeds the OpenAI model'}, {'context': 'AI researchers can directly use DeekSeek models and they can also implement the innovative ideas and designs in their own model, because the new methods of DeepSeek and source code are opened.', 'question': 'What can researchers use to create their own models?', 'answer': 'DeekSeek models'}, {'context': 'Seemingly, the DeepSeek researchers have potential to come up with more advanced idea to improve the model performance and efficient training process.', 'question': 'What are the deep-seek researchers looking for?', 'answer': 'more advanced idea to improve the model performance and efficient training process'}, {'context': 'In AI development, a lower training cost almost always implies better model accuracy later on, as the data and model can easily be scaled up at a lower cost.', 'question': 'What are the most common questions that can be asked from an AI?', 'answer': 'data and model'}, {'context': 'I hope that the performance of a good AI model does not have to be undermined by the censorship and suppression of the Chinese government.</li>', 'question': 'What are the most important questions to ask?', 'answer': 'the performance of a good AI model'}, {'context': \"We're a tiny team @deepseek-ai pushing our limits in AGI exploration.\", 'question': \"What are the questions that we're asking?\", 'answer': 'pushing our limits in AGI exploration'}, {'context': \"Starting this week , Feb 24, 2025 we'll open-source 5 repos – one daily drop – not because we've made grand claims, but simply as developers sharing our small-but-sincere progress with full transparency.\", 'question': 'What will open-source 5 repos starting this week?', 'answer': 'Feb 24, 2025'}, {'context': 'These are humble building blocks of our online service: documented, deployed and battle-tested in production. No vaporware, just sincere code that moved our tiny yet ambitious dream forward. Why?', 'question': 'What are the building blocks of our online service?', 'answer': 'documented, deployed and battle-tested in production'}, {'context': \"Because every line shared becomes collective momentum that accelerates the journey. Daily unlocks begin soon. No ivory towers - just pure garage-energy and community-driven innovation 🔧 Stay tuned – let's geek out in the open together.\", 'question': 'What is the purpose of the daily unlocks?', 'answer': 'every line shared becomes collective momentum that accelerates the journey'}, {'context': '<h2>Day 1 - FlashMLA</h2> Efficient MLA Decoding Kernel for Hopper GPUs Optimized for variable-length sequences, battle-tested in production 🔗 FlashMLA GitHub Repo ✅ BF16 support ✅ Paged KV cache (block size 64) ⚡ Performance: 3000 GB/s memory-bound | BF16 580 TFLOPS compute-bound on H800 <h2>Day 2 - DeepEP</h2> Excited to introduce DeepEP - the first open-source EP communication library for MoE model training and inference.', 'question': 'What are the questions that you can ask from the FlashMLA repo?', 'answer': 'Performance: 3000 GB/s memory-bound'}, {'context': '🔗 DeepEP GitHub Repo ✅ Efficient and optimized all-to-all communication ✅ Both intranode and internode support with NVLink and RDMA ✅ High-throughput kernels for training and inference prefilling ✅ Low-latency kernels for inference decoding ✅ Native FP8 dispatch support ✅ Flexible GPU resource control for computation-communication overlapping <h2>Day 3 - DeepGEMM</h2> Introducing DeepGEMM - an FP8 GEMM library that supports both dense and MoE GEMMs, powering V3/R1 training and inference.', 'question': 'What is the name of the repository that contains the code for DeepEP?', 'answer': 'GitHub Repo'}, {'context': '🔗 DeepGEMM GitHub Repo ⚡ Up to 1350+ FP8 TFLOPS on Hopper GPUs ✅ No heavy dependency, as clean as a tutorial ✅ Fully Just-In-Time compiled ✅ Core logic at ~300 lines - yet outperforms expert-tuned kernels across most matrix sizes ✅ Supports dense layout and two MoE layouts <h2>Day 4 - Optimized Parallelism Strategies</h2> ✅ DualPipe - a bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training.', 'question': 'What is the name of the GitHub repo?', 'answer': 'DeepGEMM'}, {'context': '🔗 GitHub Repo ✅ EPLB - an expert-parallel load balancer for V3/R1. 🔗 GitHub Repo 📊 Analyze computation-communication overlap in V3/R1.', 'question': 'What is the name of the expert-parallel load balancer for V3/R1?', 'answer': 'GitHub Repo ✅ EPLB'}, {'context': '🔗 GitHub Repo <h2>Day 5 - 3FS, Thruster for All DeepSeek Data Access</h2> Fire-Flyer File System (3FS) - a parallel file system that utilizes the full bandwidth of modern SSDs and RDMA networks.', 'question': 'What is the name of the GitHub repo?', 'answer': 'Repo'}, {'context': '⚡ 6.6 TiB/s aggregate read throughput in a 180-node cluster ⚡ 3.66 TiB/min throughput on GraySort benchmark in a 25-node cluster ⚡ 40+ GiB/s peak throughput per client node for KVCache lookup 🧬 Disaggregated architecture with strong consistency semantics ✅ Training data preprocessing, dataset loading, checkpoint saving/reloading, embedding vector search &amp; KVCache lookups for inference in V3/R1 📥 3FS → https://github.com/deepseek-ai/3FS ⛲ Smallpond - data processing framework on 3FS → https://github.com/deepseek-ai/smallpond <h2>Day 6 - One More Thing: DeepSeek-V3/R1 Inference System Overview</h2> Optimized throughput and latency via: 🔧 Cross-node EP-powered batch scaling 🔄 Computation-communication overlap ⚖️ Load balancing Production data of V3/R1 online services: ⚡ 73.7k/14.8k input/output tokens per second per H800 node 🚀 Cost profit margin 545%', 'question': 'What is the main feature of DeepSeek-V3/R1?', 'answer': 'Optimized throughput and latency'}, {'context': '<h2>Design and implementation</h2> The 3FS system has four components: cluster manager, metadata service, storage service and client. All components are connected in an RDMA network (InfiniBand or RoCE).', 'question': 'What are the main questions that can be generated from the 3FS system?', 'answer': 'cluster manager, metadata service, storage service and client'}, {'context': 'Metadata and storage services send heartbeats to cluster manager. Cluster manager handles membership changes and distributes cluster configuration to other services and clients.', 'question': 'What does cluster manager generate questions from?', 'answer': 'Metadata and storage services'}, {'context': 'Multiple cluster managers are deployed and one of them is elected as the primary. Another manager is promoted as primary when the primary fails.', 'question': 'What are the primary questions for cluster managers?', 'answer': 'one of them is elected as the primary'}, {'context': 'Cluster configuration is typically stored in a reliable distributed coordination service, such as ZooKeeper or etcd. In our production environment, we use the same key-value store as file metadata to reduce dependencies. File metadata operations (e.g.', 'question': 'What do you need to know about the cluster configuration?', 'answer': 'reliable distributed coordination service'}, {'context': 'open or create files/directories) are sent to metadata services, which implement the file system semantics. Metadata services are stateless, since file metadata are stored in a transactional key-value store (e.g. FoundationDB).', 'question': 'What are the metadata services used to generate questions from?', 'answer': 'file system semantics'}, {'context': 'Clients can connect to any metadata service. Each storage service manages a few local SSDs and provides a chunk store interface. The storage service implements Chain Replication with Apportioned Queries (CRAQ) to ensure strong consistency.', 'question': 'Clients can connect to any storage service.', 'answer': 'Clients can connect to any metadata service'}, {'context': 'CRAQ’s write-all-read-any approach helps to unleash the throughput of SSDs and RDMA network. A 3FS file is split into equally sized chunks, which are replicated over multiple SSDs. Two clients are developed for applications: FUSE client and native client.', 'question': 'What is the CRAQ write-all-read-any approach?', 'answer': 'helps to unleash the throughput'}, {'context': 'Most applications use FUSE client, which has a low adoption barrier. Performance-critical applications are integrated with the native client. <h2>File system interfaces</h2> Object store is becoming a popular option for data analytics and machine learning.', 'question': 'What is the most common question that can be asked from a FUSE client?', 'answer': 'Object store'}, {'context': 'However, file system semantics and a unified namespace where files are organized in directories provide greater flexibility for applications.', 'question': 'What are the most common questions that can be asked by an application?', 'answer': 'file system semantics and a unified namespace'}, {'context': '<ul> <li> <em>Atomic directory manipulation</em> An object store can approximate hierarchical directory structures by using slashes (/) in object keys.', 'question': 'What can be generated from an object store?', 'answer': 'hierarchical directory structures'}, {'context': 'However, it doesn’t natively support operations like atomically moving files/directories, or recursively deleting entire directories.', 'question': 'What does the program not support natively?', 'answer': 'operations like atomically moving files/directories'}, {'context': 'Actually a common pattern in our internal applications involves creating a temporary directory, writing files to it, and then moving the directory to its final location.', 'question': 'What are the most common questions that are generated by the application?', 'answer': 'creating a temporary directory, writing files to it'}, {'context': 'When handling a large number of small files, the recursive delete for directories is crucial. Without it, applications have to traverse each directory and remove files one by one.', 'question': 'What are the most common questions that can be asked from a user?', 'answer': 'directories'}, {'context': '</li> <li> <em>Symbolic and hard links</em> Our applications utilize symbolic and hard links to create lightweight snapshots of dynamically updated datasets, where new data is appended as individual files.', 'question': 'What do applications use to create lightweight snapshots of dynamically updated datasets?', 'answer': 'symbolic and hard links'}, {'context': '</li> <li> <em>Familiar interface</em> The file interface is well known and used everywhere. There is no need to learn a new storage API. Many datasets are stored as CSV/Parquet files.', 'question': 'What is the file interface well known and used everywhere?', 'answer': 'Familiar interface'}, {'context': 'Adapting file-based data loaders to use the 3FS FUSE client or native client is straightforward.', 'question': 'What are the most common questions that you can ask about file-based data loaders?', 'answer': '3FS FUSE client or native client'}, {'context': '</li> </ul> <h3>Limitations of FUSE</h3> FUSE (Filesystem in Userspace) simplifies file system client development by redirecting I/O operations to user-space processes through the FUSE kernel module.', 'question': 'What does FUSE do to simplify file system client development?', 'answer': 'redirecting I/O operations to user-space processes'}, {'context': 'It creates the illusion that applications are accessing the remote file system as if it were a local file system.', 'question': 'What does it create the illusion that applications are accessing the remote file system as if it were', 'answer': 'it were a local file system'}, {'context': 'However, it has performance limitations: <ul> <li> <em>Memory copy overhead</em> The user-space file system daemon cannot access application memory. Data transfer between kernel and user spaces consumes memory bandwidth and increases end-to-end latency.', 'question': 'What is the name of the file system that can be used to generate questions?', 'answer': 'daemon'}, {'context': '</li> <li> <em>Primitive multi-threading support</em> When an application initiates I/O requests, FUSE places these requests into a multi-threaded shared queue, protected by a spin lock.', 'question': 'What does FUSE use to generate questions from?', 'answer': 'multi-threaded shared queue'}, {'context': 'The user-space file system daemon then retrieves and processes requests from this queue. Due to lock contention, FUSE’s I/O processing capability fails to scale with the number of threads.', 'question': 'What does the user-space file system daemon process requests from?', 'answer': 'queue'}, {'context': 'Our benchmark results indicate that FUSE only handles approximately 400K 4KiB reads per second. Further increasing concurrency does not improve performance as lock contention intensifies.', 'question': 'What does FUSE use to generate questions?', 'answer': 'concurrency'}, {'context': '<code>perf</code> profiling reveals that the kernel-space spin lock consumes a significant amount of CPU time. </li> </ul> Most applications, e.g.', 'question': 'What does profiling reveal about the kernel-space spin lock?', 'answer': 'consumes a significant amount of CPU time'}, {'context': 'data analytics, perform large block writes on 3FS or they can buffer data in memory and flush it to 3FS when write buffer is full.', 'question': 'What can data analytics generate?', 'answer': 'large block writes'}, {'context': 'However, FUSE on Linux 5.x does not support concurrent writes to the same file<a href=\"https://elixir.bootlin.com/linux/v5.4.284/source/fs/fuse/file.c#L1573\">^1</a>.', 'question': 'What does FUSE not support concurrent writes to the same file?', 'answer': 'Linux 5.x'}, {'context': 'Applications overcome this limitation by writing to multiple files concurrently, maximizing the total throughput. Read operations exhibit more complex patterns.', 'question': 'What are the most common types of questions that can be generated?', 'answer': 'Read operations exhibit more complex patterns'}, {'context': 'Some training jobs require random access to dataset samples, with read sizes varying from a few kilobytes to several megabytes per sample. And samples are typically not 4K-aligned in files.', 'question': 'What are the most common types of dataset samples?', 'answer': 'not 4K-aligned in files'}, {'context': 'Data loaders are specifically designed to fetch batches of samples. But they perform poorly when handling small random reads on FUSE-mounted 3FS. Bandwidth of SSDs and RDMA network are not fully utilized.', 'question': 'What are the most common questions that data loaders can ask?', 'answer': 'batches of samples'}, {'context': '<h3>Asynchronous zero-copy API</h3> Implementing the file system client as a VFS kernel module avoids performance issues mentioned above. But kernel module development is significantly more challenging than user-space system programming.', 'question': 'What does the Asynchronous zero-copy API generate?', 'answer': 'file system client'}, {'context': 'Bugs are difficult to diagnose and can lead to catastrophic failures in production environments. For example, machines may crash and leave no log message for debugging.', 'question': 'What are the most common questions that can be asked about bugs?', 'answer': 'machines may crash and leave no log message for debugging'}, {'context': 'When upgrading a kernel module, all processes using the file system must be stopped cleanly; otherwise, a machine restart is required. For these reasons, we have chosen to implement a native client within the FUSE daemon.', 'question': 'What are the most common questions that can be asked by the FUSE client?', 'answer': 'machine restart'}, {'context': 'This client offers an interface that supports asynchronous zero-copy I/O operations. File meta operations are still handled by FUSE daemon (e.g. open/close/stat files).', 'question': 'What does the client generate questions from?', 'answer': 'asynchronous zero-copy I/O operations'}, {'context': 'Applications call <code>open()</code> to obtain a file descriptor (fd) and register it via native API. They can then perform I/O operations on the file with native client.', 'question': 'What do applications call to obtain a file descriptor?', 'answer': '<code>open()</code>'}, {'context': 'This approach ensures consistency in metadata operations with the POSIX API, making it easier to migrate existing code. The asynchronous, zero-copy API is inspired by Linux <code>io_uring</code>.', 'question': 'What is the name of the asynchronous zero-copy API?', 'answer': 'io_uring'}, {'context': 'Below are the key data structures in the API: <ul> <li> <em>Iov</em> A large memory region for zero-copy read/write operations, shared between the user process and the native client. InfiniBand memory registration is managed by the client.', 'question': 'What are the key data structures in the API?', 'answer': '<ul> <li> <em>Iov'}, {'context': 'In native API, all read data will be read into Iov, and all write data should be written to Iov before calling the API. </li> <li> <em>Ior</em> A small shared ring buffer for communication between user process and native client.', 'question': 'What is the name of the ring buffer used for communication between user process and native client?', 'answer': 'Ior'}, {'context': 'The usage of Ior is similar to Linux <code>io_uring</code>, where the user process enqueues read/write requests, and the native client dequeues these requests for completion.', 'question': 'What is the use of Ior similar to?', 'answer': 'Linux'}, {'context': 'The requests are executed in batches, with their sizes controlled by the <code>io_depth</code> parameter. Multiple batches are processed in parallel, whether from different rings or the same ring.', 'question': 'What are the requests generated from?', 'answer': 'batches'}, {'context': 'However, multiple rings are still recommended for multi-threaded applications, as sharing a ring requires synchronization, which can impact performance. </li> </ul> Within the native client, multiple threads are spawned to fetch I/O requests from the Iors.', 'question': 'What are the most common threads used to fetch I/O requests?', 'answer': 'multiple rings'}, {'context': 'These requests are batched and dispatched to storage services, reducing RPC overhead caused by small read requests.', 'question': 'What are the requests that are batched and sent to storage services?', 'answer': 'small read requests'}, {'context': '<h2>File metadata store</h2> <h3>Location of file chunks</h3> 3FS divides file data into equally sized chunks and stripes them across multiple replication chains (replication chains and chain tables are defined in Section <a href=\"#data-placement\">Data placement</a>).', 'question': 'What does 3FS generate?', 'answer': 'file data'}, {'context': 'Users can specify the chain table, chunk size, and stripe size for files on a per-directory basis. Each chunk is independently stored on multiple storage services, with its chunk ID generated by concatenating the file’s inode id and chunk index.', 'question': 'What is the name of the file that is stored on multiple storage services?', 'answer': 'Each chunk'}, {'context': 'When creating a new file, the metadata service employs a round-robin strategy to select consecutive replication chains from the designated chain table, based on the stripe size. Next, a random seed is generated to shuffle the selected chains.', 'question': 'What is the name of the seed generated to shuffle the selected chains?', 'answer': 'random'}, {'context': 'This allocation strategy ensures balanced data distribution across chains and SSDs. When an application opens a file, the client contacts the meta service to obtain the file’s data layout information.', 'question': 'What does the allocation strategy ensure?', 'answer': 'balanced data distribution'}, {'context': 'Then the client can independently compute chunk IDs and chains for data operations, minimizing the involvement of the meta service in the critical path.', 'question': 'What can the client generate?', 'answer': 'chunk IDs and chains'}, {'context': '<h3>File metadata on transactional key-value store</h3> 3FS uses FoundationDB as its distributed storage system for metadata. FoundationDB provides a key-value store interface and supports transactions with Serializable Snapshot Isolation (SSI).', 'question': 'What does 3FS use FoundationDB as its distributed storage system for metadata?', 'answer': 'transactional key-value store'}, {'context': '3FS stores all metadata as key-value pairs in FoundationDB. Meta services follow a stateless architecture, greatly enhancing maintainability by allowing administrators to seamlessly upgrade or restart services without disruption.', 'question': 'What does 3FS store in FoundationDB?', 'answer': 'all metadata as key-value pairs'}, {'context': 'When clients experience request failures or timeouts, they can automatically fail over to other available services. The file system metadata primarily consists of two core structures: inodes and directory entries.', 'question': 'What can clients use to fail over to other available services?', 'answer': 'timeouts'}, {'context': 'Inodes store attribute information for files, directories, and symbolic links, each identified by a globally unique 64-bit identifier that increments monotonically.', 'question': 'What do inodes store?', 'answer': 'attribute information'}, {'context': 'Inode keys are constructed by concatenating the \"INOD\" prefix with the inode id, which is encoded in little-endian byte order to spread inodes over multiple FoundationDB nodes.', 'question': 'What are the inode keys used for?', 'answer': 'spread inodes over multiple FoundationDB nodes'}, {'context': 'The inode values vary by its type: <ul> <li> All inode types contain basic attributes: ownership, permissions, access/modification/change times.', 'question': 'What are the basic attributes of an inode type?', 'answer': 'ownership, permissions, access/modification/change times'}, {'context': '</li> <li> Additional attributes for file inodes: file length, chunk size, selected range in chain table, shuffle seed.', 'question': 'What are some additional attributes for file inodes?', 'answer': 'file length, chunk size'}, {'context': '</li> <li> Additional attributes for directory inodes: the parent directory’s inode id, default layout configurations for subdirectories/files (chain table, chunk size, stripe size).', 'question': 'What are the basic attributes for directory inodes?', 'answer': 'the parent directory’s inode id, default layout configurations'}, {'context': 'The parent’s inode id is required to detect loops when moving directories.', 'question': 'What is required to detect loops when moving directories?', 'answer': 'The parent’s inode id'}, {'context': 'When moving <code>dir_a/dir_b</code> to <code>dir_c/</code>, we need to ensure that <code>dir_c</code> is not a descendant of <code>dir_b</code>, which can be achieved by checking all ancestors of <code>dir_c</code> upward.', 'question': 'What do we need to check for when moving a directory?', 'answer': 'all ancestors'}, {'context': '</li> <li> Additional attributes for symbolic link inodes: target path string. </li> </ul> Directory entry keys are composed of a \"DENT\" prefix, the parent inode ID, and the entry name. Directory entry values store the target inode id and inode type.', 'question': 'What are the names of the directory entry keys?', 'answer': 'DENT\" prefix, the parent inode ID, and the entry name'}, {'context': 'All entries within a directory naturally form a contiguous key range, allowing efficient directory listing via range queries.', 'question': 'Generate questions from: All entries within a directory naturally form a contiguous key range', 'answer': 'range queries'}, {'context': 'The meta operations leverage FoundationDB’s transactions: <ul> <li> Read-only transactions used for metadata queries: fstat, lookup, listdir etc. </li> <li> Read-write transactions used for metadata updates: create, link, unlink, rename etc.', 'question': 'What are the meta operations used for?', 'answer': 'leverage FoundationDB’s transactions'}, {'context': '</li> </ul> For write transactions, FoundationDB tracks the read/write key sets to form conflict detection sets. When concurrent transaction conflicts are detected, the meta service automatically retries the transaction.', 'question': 'What does FoundationDB generate questions from?', 'answer': 'conflict detection sets'}, {'context': 'This design enables multiple meta services to process requests in parallel while maintaining file system metadata consistency.', 'question': 'What does the design of the meta services allow for?', 'answer': 'process requests in parallel while maintaining file system metadata consistency'}, {'context': '<h3>Dynamic file attributes</h3> On most local file systems, deleting an opened file is deferred until all associated file descriptors are closed. Consequently, it is necessary to track all file descriptors of the file.', 'question': 'What are the names of the files that are used to generate questions from?', 'answer': 'all file descriptors'}, {'context': 'Training jobs open a large number of files during startup. Storing all file descriptors would impose heavy load on meta service and FoundationDB.', 'question': 'What would be a big burden on meta service and FoundationDB?', 'answer': 'Storing all file descriptors'}, {'context': 'Since training jobs do not depend on this feature, 3FS does not track file descriptors opened in read-only mode.', 'question': 'What does 3FS not track file descriptors opened in read-only mode?', 'answer': 'training jobs'}, {'context': '3FS maintains a file session for each file descriptor (fd) opened in write mode since deleting write opened files may lead to unreclaimable garbage chunks from concurrent writes.', 'question': 'What does 3FS generate questions from?', 'answer': 'concurrent writes'}, {'context': 'When a file with active write sessions is deleted, meta service delays the deletion until all its fds are closed.', 'question': 'What does meta service generate when a file with active write sessions is deleted?', 'answer': 'delays the deletion'}, {'context': 'To prevent lingering sessions from offline clients, the 3FS meta service periodically checks client liveness and cleans up sessions of offline clients. The file length is stored in the inode.', 'question': 'What does the 3FS meta service generate?', 'answer': 'file length'}, {'context': 'For files being actively updated, the length stored in inode may diverge from the actual length. Clients periodically (5 seconds by default) report to meta service maximum write position of each file opened in write mode.', 'question': 'What do clients report to meta service maximum write position of each file opened in write mode?', 'answer': 'Clients periodically'}, {'context': 'If this position exceeds the length in inode and there is no concurrent truncate operation, this position is adopted as the new file length.', 'question': 'What is the new file length adopted from?', 'answer': 'inode'}, {'context': 'Due to the possibility of concurrent writes from multiple clients, the method described above ensures only eventual consistency for file lengths.', 'question': 'What is the name of the method used to generate questions from?', 'answer': 'concurrent writes from multiple clients'}, {'context': 'When processing close/fsync operations, the meta service obtains the precise file length by querying the ID and length of the last chunk from the storage service.', 'question': 'What does the meta service get from the storage service?', 'answer': 'the precise file length'}, {'context': 'Since file data is striped across multiple chains, this operation incurs non-negligible overhead. Concurrent updates to the same file’s length by multiple meta services may cause transaction conflicts and lead to repeated file length computation.', 'question': 'What is the name of the operation that causes non-negligible overhead?', 'answer': 'file data is striped across multiple chains'}, {'context': 'To mitigate this, meta service distributes file length update tasks across multiple meta services using inode IDs and the rendezvous hash algorithm. Our production environments use a large stripe size: 200.', 'question': 'What does the meta service generate questions from?', 'answer': 'inode IDs and the rendezvous hash algorithm'}, {'context': 'For small files, the number of chains containing file chunks is well below this number. The number of potentially used chains is stored in file inode and used as a hint when updating the length.', 'question': 'What is the number of potentially used chains stored in file inode?', 'answer': 'length'}, {'context': 'It starts with an initial value of 16 and is doubled each time additional file chunks are written to more chains. This allows us to avoid querying all 200 chains when updating lengths of small files.', 'question': 'What is the first value of the file chunk?', 'answer': '16'}, {'context': 'This optimization can also be extended to the deletion of small files. <h2>Chunk storage system</h2> The design goal of chunk storage system is to achieve the highest bandwidth possible even when there are storage medium failures.', 'question': 'What is the goal of a chunk storage system?', 'answer': 'to achieve the highest bandwidth possible'}, {'context': 'The read/write throughput of 3FS should scale linearly with the number of SSDs and bisection network bandwidth between clients and storage services. Applications access storage services in a locality-oblivious manner.', 'question': 'What should be generated from the read/write throughput of 3FS?', 'answer': 'SSDs and bisection network bandwidth between clients and storage services'}, {'context': '<h3>Data placement</h3> Each file chunk is replicated over a chain of storage targets using chain replication with apportioned queries (CRAQ). In CRAQ write requests are sent to the head target and propagated along a chain.', 'question': 'What is the name of the process that generates questions from?', 'answer': 'chain replication'}, {'context': 'Read requests can be sent to any of the storage target. Usually the read traffic is evenly distributed among all targets in a chain for better load balance. Multiple storage targets are created on each SSD and the targets join different chains.', 'question': 'What are the read requests sent to?', 'answer': 'any of the storage target'}, {'context': 'Suppose there are 6 nodes: A, B, C, D, E, F. Each node has 1 SSD. Create 5 storage targets on each SSD: 1, 2, ... 5. Then there are 30 targets in total: A1, A2, A3, ..., F5. If each chunk has 3 replicas, a chain table is constructed as follows.', 'question': 'What is the chain table constructed from?', 'answer': 'If each chunk has 3 replicas'}, {'context': '| Chain | Version | Target 1 (head) | Target 2 | Target 3 (tail) | | :---: | :-----: | :-------------: | :------: | :-------------: | |   1   |    1    |      <code>A1</code>       |   <code>B1</code>   |      <code>C1</code>       | |   2   |    1    |      <code>D1</code>       |   <code>E1</code>   |      <code>F1</code>       | |   3   |    1    |      <code>A2</code>       |   <code>B2</code>   |      <code>C2</code>       | |   4   |    1    |      <code>D2</code>       |   <code>E2</code>   |      <code>F2</code>       | |   5   |    1    |      <code>A3</code>       |   <code>B3</code>   |      <code>C3</code>       | |   6   |    1    |      <code>D3</code>       |   <code>E3</code>   |      <code>F3</code>       | |   7   |    1    |      <code>A4</code>       |   <code>B4</code>   |      <code>C4</code>       | |   8   |    1    |      <code>D4</code>       |   <code>E4</code>   |      <code>F4</code>       | |   9   |    1    |      <code>A5</code>       |   <code>B5</code>   |      <code>C5</code>       | |  10   |    1    |      <code>D5</code>       |   <code>E5</code>   |      <code>F5</code>       | Each chain has a version number.', 'question': 'What is the version number of each chain?', 'answer': 'Target 1'}, {'context': 'The version number is incremented if the chain is changed (e.g. a storage target is offline). Only the primary cluster manager makes changes to chain tables. A few chain tables can be constructed to support different data placement requirements.', 'question': 'What is the name of the database that can be used to generate questions from?', 'answer': 'chain tables'}, {'context': 'For example, two chain tables can be created, one for batch/offline jobs and another for online services. The two tables consist of storage targets on mutually exclusive nodes and SSDs. Logically, the state of each chain changes independently.', 'question': 'What can be generated from two chain tables?', 'answer': 'batch/offline jobs and another for online services'}, {'context': 'Each chain can be included in multiple chain tables. The concept of chain table is created to let metadata service pick a table for each file and stripe file chunks across chains in the table.', 'question': 'What can be generated from a chain table?', 'answer': 'metadata service pick a table for each file and stripe file chunks'}, {'context': '<h3>Balanced traffic during recovery</h3> Suppose read traffic is evenly distributed among all storage targets in the above chain table.', 'question': \"What is the 'balanced traffic during recovery'?\", 'answer': 'Suppose read traffic'}, {'context': 'When A fails its read requests would be redirected to B and C. Under heavy load the read bandwidth of B, C is immediately saturated and B, C become the bottleneck of the entire system.', 'question': 'What would be the bottleneck of the entire system?', 'answer': 'B'}, {'context': 'Replacing a failed SSD and syncing data to the new SSD can take several hours. The read throughput is impaired during this period. To reduce the performance impact, we can have more SSDs share the redirected traffic.', 'question': 'What are the most common questions that can be asked from an SSD?', 'answer': 'failed SSD and syncing data'}, {'context': 'In the following chain table, A is paired with every other SSDs. When A fails, each of the other SSDs receives 1/5 of A’s read traffic.', 'question': 'What is the chain table used to generate?', 'answer': 'A is paired with every other SSDs'}, {'context': '| Chain | Version | Target 1 (head) | Target 2 | Target 3 (tail) | | :---: | :-----: | :-------------: | :------: | :-------------: | |   1   |    1    |      <code>B1</code>       |   <code>E1</code>   |      <code>F1</code>       | |   2   |    1    |      <code>A1</code>       |   <code>B2</code>   |      <code>D1</code>       | |   3   |    1    |      <code>A2</code>       |   <code>D2</code>   |      <code>F2</code>       | |   4   |    1    |      <code>C1</code>       |   <code>D3</code>   |      <code>E2</code>       | |   5   |    1    |      <code>A3</code>       |   <code>C2</code>   |      <code>F3</code>       | |   6   |    1    |      <code>A4</code>       |   <code>B3</code>   |      <code>E3</code>       | |   7   |    1    |      <code>B4</code>       |   <code>C3</code>   |      <code>F4</code>       | |   8   |    1    |      <code>B5</code>       |   <code>C4</code>   |      <code>E4</code>       | |   9   |    1    |      <code>A5</code>       |   <code>C5</code>   |      <code>D4</code>       | |  10   |    1    |      <code>D5</code>       |   <code>E5</code>   |      <code>F5</code>       | To achieve maximum read throughput during recovery, the load balance problem can be formulated as a balanced incomplete block design.', 'question': 'What is the name of the code that is used to generate questions from?', 'answer': 'F3'}, {'context': 'The optimal solution is obtained by using integer programming solver. <h3>Data replication</h3> CRAQ is a write-all-read-any replication protocol optimized for read-heavy workloads.', 'question': 'What is the optimal solution obtained by using integer programming solver?', 'answer': 'Data replication'}, {'context': 'Utilizing read bandwidth of all replicas is critical to achieve highest read throughput in an all-flash storage system.', 'question': 'What is the most important part of a read throughput?', 'answer': 'read bandwidth'}, {'context': 'When a write request is received by a storage service, it goes through the following steps:  <li> The service checks if the chain version in write request matches with the latest known version; reject the request if it’s not.', 'question': 'What does a storage service do when it receives a write request?', 'answer': 'reject'}, {'context': 'The write request could be sent by a client or a predecessor in the chain. </li> <li> The service issues RDMA Read operations to pull write data. If the client/predecessor fails, the RDMA Read operations may time out and the write is aborted.', 'question': 'What is the name of the service that generates questions from?', 'answer': 'RDMA Read'}, {'context': '</li> <li> Once the write data is fetched into local memory buffer, a lock for the chunk to be updated is acquired from a lock manager. Concurrent writes to the same chunk are blocked. All writes are serialized at the head target.', 'question': 'What is the name of the command that generates questions from?', 'answer': 'Concurrent'}, {'context': '</li> <li> The service reads the committed version of the chunk into memory, applies the update, and stores the updated chunk as a pending version. A storage target may store two versions of a chunk: a committed version and a pending version.', 'question': 'What does the service read the committed version of the chunk into memory?', 'answer': '<li>'}, {'context': 'Each version has a monotonically-increasing version number. The version numbers of committed version and pending versions are <code>v</code> and <code>u</code> respectively, and satisfy <code>u = v + 1</code>.', 'question': 'What are the version numbers of pending versions?', 'answer': '<code>v</code>'}, {'context': '</li> <li> If the service is the tail, the committed version is atomically replaced by the pending version and an acknowledgment message is sent to the predecessor. Otherwise, the write request is forwarded to the successor.', 'question': 'What is the tail of the service?', 'answer': 'the committed version is atomically replaced by the pending version'}, {'context': 'When the committed version is updated, the current chain version is stored as a field in the chunk metadata.', 'question': 'What is the current chain version stored as a field in the chunk metadata?', 'answer': 'When the committed version'}, {'context': '</li> <li> When an acknowledgment message arrives at a storage service, the service replaces the committed version with the pending version and continues to propagate the message to its predecessor. The local chunk lock is then released.', 'question': 'What is the name of the question that can be generated from a storage service?', 'answer': 'acknowledgment message'}, {'context': '</li>  Suppose there are 3 targets in the chain: <code>A, B, C</code>. A write request has just entered step 5 at <code>A</code>. <code>A</code> forwards the request to successor <code>B</code>.', 'question': 'What is the chain of targets?', 'answer': '<code>A, B, C</code>'}, {'context': 'Then <code>B</code> instantly fails and the forwarded write request is lost. When cluster manager detects <code>B</code>’s failure, it marks <code>B</code> as offline and moves it to the end of chain and broadcasts the updated chain table.', 'question': 'What does a cluster manager mark as offline when it detects a failure?', 'answer': '<code>B</code>'}, {'context': 'Once <code>A</code> receives the latest chain table, it forwards the write request to the new successor <code>C</code>. <code>C</code> may not receive the latest chain table yet and rejects the request.', 'question': 'What does code>A/code> forward the write request to?', 'answer': 'the new successor'}, {'context': 'But <code>A</code> can keep forwarding the request to <code>C</code>. Eventually <code>C</code> gets the latest chain table and accepts the request.', 'question': 'What can be generated from the request?', 'answer': 'latest chain table'}, {'context': 'When a read request arrives at a storage service:  <li> When the service only has a committed version of the chunk, this version is returned to the client. </li> <li> Unlike CRAQ, our implementation does not issue version query to the tail target.', 'question': 'What does CRAQ not issue version query to the tail target?', 'answer': 'implementation'}, {'context': 'When there are both committed and pending versions, the service replies a special status code to notify the client. The client may wait for a short interval and retry. Or the client can issue a relaxed read request to get the pending version.', 'question': 'What does the service generate questions from?', 'answer': 'a special status code'}, {'context': '</li>  <h3>Failure detection</h3> The cluster manager relies on heartbeats to detect fail-stop failures. Cluster manager declares a service failed if it does not receive heartbeats from it for a configurable interval (e.g. T seconds).', 'question': 'What does the cluster manager generate questions from?', 'answer': 'heartbeats'}, {'context': 'A service stops processing requests and exits if it cannot communicate with cluster manager for T/2 seconds. The heartbeat can be seen as a request to *renew a lease* granted by the manager. The metadata services are stateless.', 'question': 'What are the metadata services stateless?', 'answer': 'The metadata services are stateless.'}, {'context': 'The list of online meta services provided by cluster manager is a simple service discovery mechanism that helps clients create connections to metadata services. If one meta service is down, the clients may switch to any other metadata service.', 'question': 'What are the meta services that cluster manager provides?', 'answer': 'online'}, {'context': 'Cluster manager plays a more critical role in membership changes of storage services. It maintains a global view of chain tables and storage targets’ states. Each storage target has a public state and a local state.', 'question': 'What does the cluster manager generate?', 'answer': 'storage services'}, {'context': 'Public state indicates if it’s ready to serve read requests and if write requests would be propagated to it. Public states are stored with chain tables and distributed to services and clients.', 'question': 'What is the public state used to serve read requests?', 'answer': 'chain tables'}, {'context': '| Public State | Read | Write | Notes                                           | | :----------- | :--: | :---: | :---------------------------------------------- | | serving      |  Y   |   Y   | service alive and serving client requests       | | syncing      |  N   |   Y   | service alive and data recovery is in progress  | | waiting      |  N   |   N   | service alive and data recovery not started yet | | lastsrv      |  N   |   N   | service down and it was the last serving target | | offline      |  N   |   N   | service down or storage medium failure          | Local state is only known by storage services and cluster manager, and it’s stored in the memory of cluster manager.', 'question': 'What is the local state of a cluster?', 'answer': 'Local state is only known by storage services and cluster manager'}, {'context': 'If a storage target has medium failure, the related service sets the target’s local state to offline in heartbeat. If a storage service is down, storage targets managed by the service are marked offline.', 'question': 'What does a storage service do when a storage service is down?', 'answer': 'storage targets managed by the service are marked offline'}, {'context': '| Local State | Notes                                                | | :---------- | :--------------------------------------------------- | | up-to-date  | service alive and serving client requests            | | online      | service alive and target in syncing or waiting state | | offline     | service down or storage medium failure               | A storage target can change from one public state to another in response to the latest local state.', 'question': 'What can be generated from a storage target?', 'answer': 'change from one public state to another in response to the latest local state'}, {'context': 'The local state plays the role of a triggering event. The cluster manager periodically scans every chain and updates the public states of targets on the chain according to a state-transition table.', 'question': 'What does the cluster manager generate?', 'answer': 'updates the public states of targets'}, {'context': '<ul> <li> The chain version is incremented if the chain is updated. </li> <li> If a storage target is marked offline, it’s moved to the end of chain.', 'question': 'What is the name of the question that is generated?', 'answer': 'The chain version'}, {'context': '</li> <li> If a storage service finds public state of any local storage target is lastsrv or offline, it exits immediately. The service may be isolated from the cluster manager by network partition error.', 'question': 'What does lastsrv do?', 'answer': 'exits immediately'}, {'context': '</li> <li> Once the data recovery of a storage target in syncing state is completed, the storage service set the target’s local state to up-to-date in subsequent heartbeat messages sent to cluster manager.', 'question': \"What does the storage service set the target's local state to up-to-date in subsequent\", 'answer': 'heartbeat messages'}, {'context': '</li> </ul> | Local State | Current Public State | Predecessor’s Public State | Next Public State | | :---------- | :------------------- | :------------------------- | :---------------- | | up-to-date  | serving              | (any)                      | serving           | |             | syncing              | (any)                      | serving           | |             | waiting              | (any)                      | waiting           | |             | lastsrv              | (any)                      | serving           | |             | offline              | (any)                      | waiting           | | online      | serving              | (any)                      | serving           | |             | syncing              | serving                    | syncing           | |             |                      | not serving                | waiting           | |             | waiting              | serving                    | syncing           | |             |                      | not serving                | waiting           | |             | lastsrv              | (any)                      | serving           | |             | offline              | (any)                      | waiting           | | offline     | serving              | has no predecessor         | lastsrv           | |             |                      | has predecessor            | offline           | |             | syncing              | (any)                      | offline           | |             | waiting              | (any)                      | offline           | |             | lastsrv              | (any)                      | lastsrv           | |             | offline              | (any)                      | offline           | <h3>Data recovery</h3> When a storage service exits (e.g.', 'question': 'What is the name of the service that exits when a storage service exits?', 'answer': 'Data recovery'}, {'context': 'process crashes or restarts during upgrade), or a storage medium failure occurs, all related storage targets will be marked as offline and moved to the end of chains by cluster manager.', 'question': 'What are the most common questions that can be generated from a crash or restart during an upgrade?', 'answer': 'all related storage targets'}, {'context': 'Once the service restarts, each target on the service enters into the recovery process independently. The entire recovery process overlaps with normal activity and minimizes any interruption.', 'question': 'What is the name of the service that is used to generate questions?', 'answer': 'recovery process'}, {'context': 'When a previously offline storage service starts:  <li> The service periodically pulls latest chain tables from cluster manager. But it does not send heartbeats until all its storage targets have been marked offline in the latest chain tables.', 'question': 'What does a service send heartbeats from when all its storage targets have been marked offline?', 'answer': 'the latest chain tables'}, {'context': 'This ensures all its targets would go through the data recovery process. </li> <li> When a write request arrives during recovery, the request is always a full-chunk-replace write.', 'question': 'What is the first step in the recovery process?', 'answer': 'write request'}, {'context': 'The local committed version is updated and any existing pending version is abandoned. Since current service is the tail, an acknowledgment message is sent to the predecessor.', 'question': 'What is the local committed version of the service?', 'answer': 'the tail'}, {'context': 'The full state of the predecessor is copied to the returning service through a continuous stream of full-chunk-replace writes.', 'question': 'What is the state of the predecessor copied to the returning service?', 'answer': 'The full'}, {'context': '</li> <li> Before the data recovery of a storage target starts, the predecessor sends a dump-chunkmeta request to the returning service.', 'question': 'What does the predecessor send to the returning service?', 'answer': 'dump-chunkmeta request'}, {'context': 'Then the service iterates the local chunk metadata store to collect the ids, chain versions and committed/pending version numbers of all chunks on the target, and replies the collected metadata to the predecessor.', 'question': 'What does the service iterate the local chunk metadata store to collect?', 'answer': 'the ids'}, {'context': '</li> <li> When a sync-done message arrives, the service knows that the storage target is up-to-date. It sets local state of the target to up-to-date in heartbeat messages sent to cluster manager.', 'question': 'What does the service know about the storage target?', 'answer': 'up-to-date'}, {'context': '</li>  When a storage service finds a previously offline successor is online:  <li> The service starts to forward normal write requests to the successor.', 'question': 'What does a storage service do when it finds a previously offline successor is online?', 'answer': 'starts to forward normal write requests to the successor'}, {'context': 'Clients may only update a portion of the chunk, but the forwarded write requests should contain the whole chunk, i.e. a full-chunk-replace write. </li> <li> The service sends a dump-chunkmeta request to the successor.', 'question': 'What is the name of the service that sends a dump-chunkmeta request to', 'answer': 'The service sends a dump-chunkmeta request to the successor'}, {'context': 'Once the metadata of all chunks on the successor target are received, it collects the chunk metadata on its local target. Then it compares the two copies of chunk metadata to decide which chunks should be transferred.', 'question': 'What does the DB generate?', 'answer': 'chunk metadata'}, {'context': '</li> <li> The selected chunks are transferred to the successor by issuing full-chunk-replace write requests. </li> <li> The chunk lock is first acquired for each chunk.', 'question': 'What are the chunks that are selected are transferred to the successor?', 'answer': 'full-chunk-replace write requests'}, {'context': '</li> <li> The chain version, committed version number and chunk content are read and transferred to successor by sending a full-chunk-replace request. </li> <li> The chunk lock is released. </li>  4.', 'question': 'What is the chain version?', 'answer': 'committed version number and chunk content are read and transferred to successor'}, {'context': 'When all required chunks have been transferred, a sync-done message is sent to the successor. The rules used to decide which chunks should be transferred are: <ul> <li> If a chunk only exists on the local target, it should be transferred.', 'question': 'What are the rules used to decide which chunks should be transferred?', 'answer': '<ul> <li>'}, {'context': '</li> <li> If a chunk only exists on the remote target, it should be removed. </li> <li> If the chain version of local chunk replica is greater than that of the remote chunk replica, it should be transferred.', 'question': 'What should be transferred if the chain version of local chunk replica is greater than that of remote chunk', 'answer': 'If the chain version of local chunk replica'}, {'context': '</li> <li> If the chain versions of local/remote chunk replicas are the same but local committed version number does not equal to the remote pending version number, it should be transferred.', 'question': 'What should be transferred if the chain versions of local/remote chunk replicas are the', 'answer': 'local committed version number'}, {'context': '</li> <li> Otherwise, two chunk replicas are either the same or being updated by in-progress write requests. </li> </ul> <h3>Chunks and the metadata</h3> File chunks are stored in the chunk engine.', 'question': 'What are the chunks and the metadata stored in?', 'answer': 'chunk engine'}, {'context': 'On each SSD, the persistent storage of the chunk engine consists of a fixed number of data files for storing chunk data, and a RocksDB instance for maintaining chunk metadata and other system information.', 'question': 'What is the name of the chunk engine?', 'answer': 'SSD'}, {'context': 'Additionally, the chunk engine maintains an in-memory cache of chunk metadata to enhance query performance. A chunk allocator is implemented for fast allocation of new chunks.', 'question': 'What does the chunk engine generate?', 'answer': 'an in-memory cache of chunk metadata'}, {'context': 'The chunk engine interface provides thread-safe access through the following operations:  <li> <em>open/close</em> Initializes the engine by loading metadata from RocksDB and reconstructing chunk allocator states.', 'question': 'What is the chunk engine interface used for?', 'answer': 'thread-safe access'}, {'context': '</li> <li> <em>get</em> Retrieves chunk metadata and reference-counted handle through a hashmap cache, enabling concurrent access with O(1) average complexity.', 'question': 'What does get the chunk metadata and reference-counted handle from?', 'answer': 'hashmap cache'}, {'context': '</li> <li> <em>update</em> Implements copy-on-write (COW) semantics by allocating new chunks before modifying data. Old chunks remain readable until all handles are released.', 'question': 'What does COW semantics generate?', 'answer': 'copy-on-write'}, {'context': '</li> <li> <em>commit</em> Commit the updated chunk metadata to RocksDB via write batches to ensure atomic updates; synchronously refresh the chunk metadata cache. </li>  The chunk data will ultimately be stored on physical blocks.', 'question': 'How is the chunk metadata updated?', 'answer': 'write batches'}, {'context': 'Physical block sizes range from 64KiB to 64MiB in increments of powers of two, totaling 11 distinct sizes. The allocator will assign physical blocks whose sizes most closely match the actual chunk size.', 'question': 'What will the allocator assign to the blocks whose sizes match the actual chunk size?', 'answer': 'physical blocks whose sizes'}, {'context': 'A resource pool is constructed for each physical block size, with each pool containing 256 physical files. The usage status of physical blocks is maintained in memory using bitmaps. When a physical block is reclaimed, its bitmap flag is set to 0.', 'question': 'What is the name of the resource pool?', 'answer': 'physical block size, with each pool containing 256 physical files'}, {'context': 'The actual storage space of the block remains preserved and will be prioritized for subsequent allocations.', 'question': 'What will be generated for subsequent allocations?', 'answer': 'The actual storage space'}, {'context': 'When no available physical blocks remain, <code>fallocate()</code> will be used to allocate a contiguous large space in physical files, creating 256 new physical blocks - this approach helps reduce disk fragmentation.', 'question': 'When no available physical blocks remain, fallocate() will be used to create 256 new physical', 'answer': '<code>fallocate'}, {'context': 'When performing write operations on a chunk, the allocator first assigns a new physical block. The system then reads existing chunk data into a buffer, applies the update, and writes the updated buffer to the newly allocated block.', 'question': 'What does the allocator read from the chunk?', 'answer': 'a new physical block'}, {'context': \"An optimized process is implemented for appends, where data is directly added in-place at the end of the existing block. A new copy of metadata is constructed from the new block's location and existing chunk metadata.\", 'question': 'What is the process used to create a new copy of metadata?', 'answer': 'appends'}, {'context': 'Subsequently, both the new chunk metadata and statuses of new and old physical blocks are atomically updated in RocksDB.', 'question': 'RocksDB updates the chunk metadata and statuses of new and old physical blocks.', 'answer': 'atomically updated in RocksDB'}]\n"
          ]
        }
      ],
      "source": [
        "print(qa_pairs_per_chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning DeepSeek-AI research@deepseek.com Abstract We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.', 'DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super- vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.', 'Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing.', 'To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek- R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks.', 'To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.', 'AIME 2024 (Pass@1) Codeforces (Percentile) GPQA Diamond (Pass@1) MATH-500 (Pass@1) MMLU (Pass@1) SWE-bench Verified (Resolved) 0 20 40 60 80 100 Accuracy / Percentile (%) 79.8 96.3 71.5 97.3 90.8 49.2 79.2 96.6 75.7 96.4 91.8 48.9 72.6 90.6 62.1 94.3 87.4 36.8 63.6 93.4 60.0 90.0 85.2 41.6 39.2 58.7 59.1 90.2 88.5 42.0 DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3 Figure 1 | Benchmark performance of DeepSeek-R1.', 'arXiv:2501.12948v1  [cs.CL]  22 Jan 2025  Contents 1 Introduction 3 1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '. 4 2 Approach 5 2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . . 5 2.2.1 Reinforcement Learning Algorithm . . . . . . . .', '. . . . . . . . . . . . . . 5 2.2.2 Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2.3 Training Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '6 2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero 6 2.3 DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . . 9 2.3.1 Cold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '. . 9 2.3.2 Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . . 10 2.3.3 Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . . 10 2.3.4 Reinforcement Learning for all Scenarios . . . . . . . . . . . . . . . . .', '. . 11 2.4 Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . . 11 3 Experiment 11 3.1 DeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.2 Distilled Model Evaluation . . . . . . . . .', '. . . . . . . . . . . . . . . . . . . . . . 14 4 Discussion 14 4.1 Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 14 4.2 Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '15 5 Conclusion, Limitations, and Future Work 16 A Contributions and Acknowledgments 20 2  1.', 'Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI).', 'Recently, post-training has emerged as an important component of the full training pipeline.', 'It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training.', 'In the context of reasoning capabilities, OpenAI’s o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of- Thought reasoning process.', 'This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community.', 'Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024).', 'However, none of these methods has achieved general reasoning performance comparable to OpenAI’s o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL).', 'Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process.', 'Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.', 'During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks.', 'For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.', 'However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing.', 'To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline.', 'Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1- Zero.', 'Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.', 'After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios.', 'After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. We further explore distillation from DeepSeek-R1 to smaller dense models.', 'Using Qwen2.5- 32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru- cial for improving reasoning capabilities.', 'We open-source the distilled Qwen and Llama (Dubey et al., 2024) series.', 'Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. 3  1.1.', 'Contributions Post-Training: Large-Scale Reinforcement Learning on the Base Model • We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step.', 'This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero.', 'DeepSeek- R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community.', 'Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.', '• We introduce our pipeline to develop DeepSeek-R1.', 'The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human pref- erences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities.', 'We believe the pipeline will benefit the industry by creating better models.', 'Distillation: Smaller Models Can Be Powerful Too • We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models.', 'The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.', '• Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks.', 'DeepSeek- R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi- tionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench.', 'These results significantly outperform previous open- source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. 1.2.', 'Summary of Evaluation Results • Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217.', 'On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-o1-1217 and significantly outperforming other models.', '(2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition.', 'For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks.', '• Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek- R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond.', 'While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks.', 'On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.', '4  • Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more.', 'It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are- naHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.', 'Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. 2. Approach 2.1.', 'Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance.', 'In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start.', 'Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data.', 'In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples.', '3) Distill the reasoning capability from DeepSeek-R1 to small dense models. 2.2.', 'DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev- idenced by our previous works (Shao et al., 2024; Wang et al., 2023).', 'However, these works heavily depended on supervised data, which are time-intensive to gather.', 'In this section, we explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure reinforcement learning process.', 'We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. 2.2.1.', 'Reinforcement Learning Algorithm Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead.', 'Specifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, · · · , 𝑜𝐺} from the old policy 𝜋𝜃𝑜𝑙𝑑and then optimizes the policy model 𝜋𝜃by maximizing the following objective: J𝐺𝑅𝑃𝑂(𝜃) = E[𝑞∼𝑃(𝑄), {𝑜𝑖}𝐺 𝑖=1 ∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑞)] 1 𝐺 𝐺 ∑︁ 𝑖=1 \\x12 min \\x12 𝜋𝜃(𝑜𝑖|𝑞) 𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) 𝐴𝑖, clip \\x12 𝜋𝜃(𝑜𝑖|𝑞) 𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) , 1 −𝜀, 1 + 𝜀 \\x13 𝐴𝑖 \\x13 −𝛽D𝐾𝐿 \\x00𝜋𝜃||𝜋𝑟𝑒𝑓 \\x01\\x13 , (1) D𝐾𝐿 \\x00𝜋𝜃||𝜋𝑟𝑒𝑓 \\x01 = 𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞) 𝜋𝜃(𝑜𝑖|𝑞) −log 𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞) 𝜋𝜃(𝑜𝑖|𝑞) −1, (2) where 𝜀and 𝛽are hyper-parameters, and 𝐴𝑖is the advantage, computed using a group of rewards {𝑟1, 𝑟2, .', '. . , 𝑟𝐺} corresponding to the outputs within each group: 𝐴𝑖= 𝑟𝑖−m𝑒𝑎𝑛({𝑟1, 𝑟2, · · · , 𝑟𝐺}) s𝑡𝑑({𝑟1, 𝑟2, · · · , 𝑟𝐺}) . (3) 5  A conversation between User and Assistant. The user asks a question, and the Assistant solves it.', 'The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.', 'The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: prompt. Assistant: Table 1 | Template for DeepSeek-R1-Zero.', 'prompt will be replaced with the specific reasoning question during training. 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL.', 'To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: • Accuracy rewards: The accuracy reward model evaluates whether the response is correct.', 'For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness.', 'Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.', '• Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’ tags.', 'We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.', '2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions.', 'As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer.', 'We intentionally limit our constraints to this structural format, avoiding any content-specific biases—such as mandating reflective reasoning or promoting particular problem-solving strate- gies—to ensure that we can accurately observe the model’s natural progression during the RL process.', '2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek- R1-Zero on the AIME 2024 benchmark throughout the RL training process.', 'As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances.', 'Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-o1-0912.', 'This significant improvement highlights the efficacy of our RL algorithm in optimizing the model’s performance over time.', 'Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI’s o1-0912 models across a variety of reasoning-related benchmarks.', 'The findings reveal that RL empowers 6  Model AIME 2024 MATH-500 GPQA LiveCode CodeForces Diamond Bench pass@1 cons@64 pass@1 pass@1 pass@1 rating OpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820 OpenAI-o1-0912 74.4 83.3 94.8 77.3 63.4 1843 DeepSeek-R1-Zero 71.0 86.7 95.9 73.3 50.0 1444 Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks.', 'Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation.', 'DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the model’s ability to learn and generalize effectively through RL alone.', 'Additionally, the performance of DeepSeek- R1-Zero can be further augmented through the application of majority voting.', 'For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912.', 'The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks.', 'Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously.', 'By initiating RL directly from the base model, we can closely monitor the model’s progression without the influence of the supervised fine-tuning stage.', 'This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks.', 'As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve- 7  Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process.', 'DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. ment throughout the training process. This improvement is not the result of external adjustments but rather an intrinsic development within the model.', 'DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time compu- tation.', 'This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth.', 'One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases.', 'Behaviors such as reflection—where the model revisits and reevaluates its previous steps—and the exploration of alternative approaches to problem-solving arise spontaneously.', 'These behaviors are not explicitly programmed but instead emerge as a result of the model’s interaction with the reinforcement learning environment.', 'This spontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy.', 'Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as illustrated in Table 3, occurs in an intermediate version of the model.', 'During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach.', 'This behavior is not only a testament to the model’s growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.', 'This moment is not only an “aha moment” for the model but also for the researchers observing its behavior.', 'It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies.', 'The “aha moment” serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future.', '8  Question: If 𝑎> 1, then the sum of the real solutions of √︁ 𝑎−√ 𝑎+ 𝑥= 𝑥is equal to Response: <think> To solve the equation √︁ 𝑎−√ 𝑎+ 𝑥= 𝑥, let’s start by squaring both · · · \\x10√︁ 𝑎−√ 𝑎+ 𝑥 \\x112 = 𝑥2 =⇒𝑎−√ 𝑎+ 𝑥= 𝑥2.', 'Rearrange to isolate the inner square root term: (𝑎−𝑥2)2 = 𝑎+ 𝑥=⇒𝑎2 −2𝑎𝑥2 + (𝑥2)2 = 𝑎+ 𝑥=⇒𝑥4 −2𝑎𝑥2 −𝑥+ (𝑎2 −𝑎) = 0 . . . Wait, wait. Wait. That’s an aha moment I can flag here.', 'Let’s reevaluate this step-by-step to identify if the correct sum can be · · · We started with the equation: √︁ 𝑎−√ 𝑎+ 𝑥= 𝑥 First, let’s square both sides: 𝑎−√ 𝑎+ 𝑥= 𝑥2 =⇒√ 𝑎+ 𝑥= 𝑎−𝑥2 Next, I could square both sides again, treating the equation: · · · . .', '. Table 3 | An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning.', 'Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues.', 'For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing.', 'To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. 2.3.', 'DeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start?', '2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1.', 'The pipeline consists of four stages, outlined as follows. 2.3.1.', 'Cold Start Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor.', 'To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1- Zero outputs in a readable format, and refining the results through post-processing by human annotators.', 'In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL.', 'Compared to DeepSeek-R1-Zero, the advantages of cold start data 9  include: • Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading.', 'Responses may mix multiple languages or lack markdown formatting to highlight answers for users.', 'In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly.', 'Here, we define the output format as |special_token|<reasoning_process>|special_token|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results.', '• Potential: By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. 2.3.2.', 'Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero.', 'This phase focuses on enhancing the model’s reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions.', 'During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages.', 'To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT.', 'Although ablation experiments show that such alignment results in a slight degradation in the model’s performance, this reward aligns with human preferences, making it more readable.', 'Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. 2.3.3.', 'Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round.', 'Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model’s capabilities in writing, role-playing, and other general-purpose tasks.', 'Specifically, we generate the data and fine-tune the model as described below. Reasoning data We curate reasoning prompts and generate reasoning trajectories by perform- ing rejection sampling from the checkpoint from the above RL training.', 'In the previous stage, we only included data that could be evaluated using rule-based rewards.', 'However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.', 'Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones.', 'In total, we collect about 600k reasoning related training samples.', '10  Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3.', 'For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as “hello” we do not provide a CoT in response.', 'In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. 2.3.4.', 'Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model’s helpfulness and harmlessness while simultane- ously refining its reasoning capabilities.', 'Specifically, we train the model using a combination of reward signals and diverse prompt distributions.', 'For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains.', 'For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train- ing prompts.', 'For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process.', 'For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.', 'Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. 2.4.', 'Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in §2.3.3.', 'Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models.', 'The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5- 14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1.', 'For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance.', 'Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. 3.', 'Experiment Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 11  2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08 – 2025-01), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math- ematics Examination 2024 (AIME 2024) (MAA, 2024).', 'In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges.', 'Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons.', 'Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.', 'Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple- evals framework.', 'For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting.', 'The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators.', 'For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash).', 'Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025.', 'The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated.', 'SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark.', 'Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.', 'Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor- mance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a).', 'Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints.', 'Therefore, we default to pass@𝑘evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature.', 'Specifically, we use a sampling temperature of 0.6 and a top-𝑝value of 0.95 to generate 𝑘 responses (typically between 4 and 64, depending on the test set size) for each question.', 'Pass@1 is then calculated as pass@1 = 1 𝑘 𝑘 ∑︁ 𝑖=1 𝑝𝑖, where 𝑝𝑖denotes the correctness of the 𝑖-th response. This method provides more reliable performance estimates.', 'For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. 1https://aider.chat 2https://codeforces.com 3https://www.cms.org.cn/Home/comp/comp/cid/12.html 12  3.1.', 'DeepSeek-R1 Evaluation Benchmark (Metric) Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek Sonnet-1022 0513 V3 o1-mini o1-1217 R1 Architecture - - MoE - - MoE # Activated Params - - 37B - - 37B # Total Params - - 671B - - 671B English MMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8 MMLU-Redux (EM) 88.9 88.0 89.1 86.7 - 92.9 MMLU-Pro (EM) 78.0 72.6 75.9 80.3 - 84.0 DROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2 IF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 - 83.3 GPQA Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5 SimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1 FRAMES (Acc.)', '72.5 80.5 73.3 76.9 - 82.5 AlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 - 87.6 ArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3 Code LiveCodeBench (Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9 Codeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3 Codeforces (Rating) 717 759 1134 1820 2061 2029 SWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2 Aider-Polyglot (Acc.)', '45.3 16.0 49.6 32.9 61.7 53.3 Math AIME 2024 (Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8 MATH-500 (Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3 CNMO 2024 (Pass@1) 13.1 10.8 43.2 67.6 - 78.8 Chinese CLUEWSC (EM) 85.4 87.9 90.9 89.9 - 92.8 C-Eval (EM) 76.7 76.0 86.5 68.9 - 91.8 C-SimpleQA (Correct) 55.4 58.7 68.0 40.3 - 63.7 Table 4 | Comparison between DeepSeek-R1 and other representative models.', 'For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3.', 'This im- provement is primarily attributed to enhanced accuracy in STEM-related questions, where signif- icant gains are achieved through large-scale reinforcement learning.', 'Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks.', 'On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses GPT-4o on this benchmark.', 'However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%.', 'DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a model’s ability to follow format instructions.', 'These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training.', 'Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering.', 'Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains.', 'Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0.', 'This indicates that 13  DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks.', 'On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin.', 'A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks.', 'On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified.', 'We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. 3.2.', 'Distilled Model Evaluation Model AIME 2024 MATH-500 GPQA LiveCode CodeForces Diamond Bench pass@1 cons@64 pass@1 pass@1 pass@1 rating GPT-4o-0513 9.3 13.4 74.6 49.9 32.9 759 Claude-3.5-Sonnet-1022 16.0 26.7 78.3 65.0 38.9 717 OpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820 QwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 1316 DeepSeek-R1-Distill-Qwen-1.5B 28.9 52.7 83.9 33.8 16.9 954 DeepSeek-R1-Distill-Qwen-7B 55.5 83.3 92.8 49.1 37.6 1189 DeepSeek-R1-Distill-Qwen-14B 69.7 80.0 93.9 59.1 53.1 1481 DeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 1691 DeepSeek-R1-Distill-Llama-8B 50.4 80.0 89.1 49.0 39.6 1205 DeepSeek-R1-Distill-Llama-70B 70.0 86.7 94.5 65.2 57.5 1633 Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks.', 'As shown in Table 5, simply distilling DeepSeek-R1’s outputs enables the efficient DeepSeek- R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non- reasoning models like GPT-4o-0513 across the board.', 'DeepSeek-R1-14B surpasses QwQ-32B- Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla- tion.', 'Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. 4. Discussion 4.1.', 'Distillation v.s. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results.', 'However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?', 'To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B.', 'The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale 14  Model AIME 2024 MATH-500 GPQA Diamond LiveCodeBench pass@1 cons@64 pass@1 pass@1 pass@1 QwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 DeepSeek-R1-Zero-Qwen-32B 47.0 60.0 91.6 55.0 40.2 DeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 Table 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks.', 'RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1- Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks.', 'Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation.', 'Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger- scale reinforcement learning. 4.2.', 'Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way.', 'We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models.', 'Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023).', 'However, in practice, PRM has three main limitations that may hinder its ultimate suc- cess. First, it is challenging to explicitly define a fine-grain step in general reasoning.', 'Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not con- ducive to scaling up.', 'Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline.', 'In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments.', 'Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil- ver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability.', 'This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically.', 'To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model.', 'Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training.', 'First, unlike chess, where the search space is relatively well-defined, token generation presents an 15  exponentially larger search space.', 'To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly influences the quality of generation since it guides each step of the search process.', 'Training a fine-grained value model is inherently difficult, which makes it challenging for the model to iteratively improve.', 'While AlphaGo’s core success relied on training a value model to progressively enhance its performance, this principle proves difficult to replicate in our setup due to the complexities of token generation.', 'In conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a significant challenge. 5.', 'Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning.', 'DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning.', 'Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models.', 'We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models.', 'The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH.', 'Other dense models also achieve impressive results, significantly outperforming other instruction- tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1.', '• General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output.', 'Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields.', '• Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages.', 'For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates.', '• Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance.', 'Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.', '• Software Engineering Tasks: Due to the long evaluation times, which impact the effi- ciency of the RL process, large-scale RL has not been applied extensively in software engineering tasks.', 'As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks.', 'Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency. 16  References AI@Meta. Llama 3.1 model card, 2024.', 'URL https://github.com/meta-llama/llama-m odels/blob/main/models/llama3_1/MODEL_CARD.md. Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3 -5-sonnet.', 'M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A.', 'Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba.', 'Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al.', 'The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.', 'X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024. URL https: //arxiv.org/abs/2309.17179. L. Gao, J. Schulman, and J. Hilton.', 'Scaling laws for reward model overoptimization, 2022. URL https://arxiv.org/abs/2210.10760. A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X.', 'Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or g/10.48550/arXiv.2406.04127. Google. Our next-generation model: Gemini 1.5, 2024.', 'URL https://blog.google/techno logy/ai/google-gemini-next-generation-model-february-2024. Y. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi- nese simpleqa: A chinese factuality evaluation for large language models.', 'arXiv preprint arXiv:2411.07140, 2024. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.', 'Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.', 'N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024.', 'URL https://doi.org/10.48550/arXiv.2403.07974. 17  S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation.', 'CoRR, abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485 50/arXiv.2409.12941. A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs, et al.', 'Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin.', 'CMMLU: Measur- ing massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, 2023. T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica.', 'From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe.', 'Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023. B. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL https://github.com/WildEval/ZeroEval. MAA. American invitational mathematics examination - aime.', 'In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math -competitions/american-invitational-mathematics-examination-aime. OpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/. OpenAI.', 'Learning to reason with llms, 2024b. URL https://openai.com/index/learnin g-to-reason-with-llms/. OpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing -simpleqa/. OpenAI.', 'Introducing SWE-bench verified we’re releasing a human-validated subset of swe- bench that more, 2024d. URL https://openai.com/index/introducing-swe-bench -verified/. Qwen. Qwq: Reflect deeply on the boundaries of the unknown, 2024a.', 'URL https://qwenlm .github.io/blog/qwq-32b-preview/. Qwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b log/qwen2.5. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman.', 'GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.', 'arXiv preprint arXiv:2402.03300, 2024. D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis.', 'Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017a. URL http://arxiv.org/abs/1712.01815.', '18  D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis.', 'Mastering the game of go without human knowledge. Nat., 550(7676):354–359, 2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270. C. Snell, J. Lee, K. Xu, and A. Kumar.', 'Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033 14. T. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations.', 'Nature, 2024. doi: 10.1038/s41586-023-06747-5. J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.', 'P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A label- free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, 2023. X. Wang, J. Wei, D. Schuurmans, Q.', 'Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X.', 'He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024. URL https://doi.org/10.48550/arXiv.2406.01574.', 'C. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024. H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q.', 'Du, W. Gao, Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL https://arxiv.org/abs/2408.08152.', 'J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. 19  Appendix A.', 'Contributions and Acknowledgments Core Contributors Daya Guo Dejian Yang Haowei Zhang Junxiao Song Ruoyu Zhang Runxin Xu Qihao Zhu Shirong Ma Peiyi Wang Xiao Bi Xiaokang Zhang Xingkai Yu Yu Wu Z.F.', 'Wu Zhibin Gou Zhihong Shao Zhuoshu Li Ziyi Gao Contributors Aixin Liu Bing Xue Bingxuan Wang Bochao Wu Bei Feng Chengda Lu Chenggang Zhao Chengqi Deng Chong Ruan Damai Dai Deli Chen Dongjie Ji Erhang Li Fangyun Lin Fucong Dai Fuli Luo* Guangbo Hao Guanting Chen Guowei Li H. Zhang Hanwei Xu Honghui Ding Huazuo Gao Hui Qu Hui Li Jianzhong Guo Jiashi Li Jingchang Chen Jingyang Yuan Jinhao Tu Junjie Qiu Junlong Li J.L.', 'Cai Jiaqi Ni Jian Liang Jin Chen Kai Dong Kai Hu* Kaichao You Kaige Gao Kang Guan Kexin Huang Kuai Yu Lean Wang Lecong Zhang Liang Zhao Litong Wang Liyue Zhang Lei Xu Leyi Xia Mingchuan Zhang Minghua Zhang Minghui Tang Mingxu Zhou Meng Li Miaojun Wang Mingming Li Ning Tian Panpan Huang Peng Zhang Qiancheng Wang Qinyu Chen Qiushi Du Ruiqi Ge* Ruisong Zhang Ruizhe Pan Runji Wang R.J. Chen R.L.', 'Jin 20  Ruyi Chen Shanghao Lu Shangyan Zhou Shanhuang Chen Shengfeng Ye Shiyu Wang Shuiping Yu Shunfeng Zhou Shuting Pan S.S. Li Shuang Zhou Shaoqing Wu Shengfeng Ye Tao Yun Tian Pei Tianyu Sun T. Wang Wangding Zeng Wen Liu Wenfeng Liang Wenjun Gao Wenqin Yu* Wentao Zhang W.L.', 'Xiao Wei An Xiaodong Liu Xiaohan Wang Xiaokang Chen Xiaotao Nie Xin Cheng Xin Liu Xin Xie Xingchao Liu Xinyu Yang Xinyuan Li Xuecheng Su Xuheng Lin X.Q.', 'Li Xiangyue Jin Xiaojin Shen Xiaosha Chen Xiaowen Sun Xiaoxiang Wang Xinnan Song Xinyi Zhou Xianzu Wang Xinxia Shan Y.K. Li Y.Q. Wang Y.X.', 'Wei Yang Zhang Yanhong Xu Yao Li Yao Zhao Yaofeng Sun Yaohui Wang Yi Yu Yichao Zhang Yifan Shi Yiliang Xiong Ying He Yishi Piao Yisong Wang Yixuan Tan Yiyang Ma* Yiyuan Liu Yongqiang Guo Yuan Ou Yuduan Wang Yue Gong Yuheng Zou Yujia He Yunfan Xiong Yuxiang Luo Yuxiang You Yuxuan Liu Yuyang Zhou Y.X.', 'Zhu Yanping Huang Yaohui Li Yi Zheng Yuchen Zhu Yunxian Ma Ying Tang Yukun Zha Yuting Yan Z.Z.', 'Ren Zehui Ren Zhangli Sha Zhe Fu Zhean Xu Zhenda Xie Zhengyan Zhang Zhewen Hao Zhicheng Ma Zhigang Yan Zhiyu Wu Zihui Gu 21  Zijia Zhu Zijun Liu* Zilin Li Ziwei Xie Ziyang Song Zizheng Pan Zhen Huang Zhipeng Xu Zhongyu Zhang Zhen Zhang Within each role, authors are listed alphabetically by the first name.', 'Names marked with * denote individuals who have departed from our team. 22', 'DualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles.', 'For detailed information on computation-communication overlap, please refer to the profile data.', 'Pipeline Bubbles and Memory Usage Comparison | Method    | Bubble                  | Parameter | Activation | |:---------:|:-----------------------:|:---------:|:----------:| | 1F1B      | (PP-1)(𝐹+𝐵)            | 1×        | PP         | | ZB1P      | (PP-1)(𝐹+𝐵-2𝑊)         | 1×        | PP         | | DualPipe  | (PP/2-1)(𝐹&amp;𝐵+𝐵-3𝑊)     | 2×        | PP+1       | 𝐹 denotes the execution time of a forward chunk, 𝐵 denotes the execution time of a full backward chunk, 𝑊 denotes the execution time of a \"backward for weights\" chunk, and 𝐹&amp;𝐵 denotes the execution time of two mutually overlapped forward and backward chunks.', '<h3>About</h3> A bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training <code>DualPipe was created and developed by Jiashi Li and Chengqi Deng and Wenfeng Liang.</code>  Here, we publicly share profiling data from our training and inference framework to help the community better understand the communication-computation overlap strategies and low-level implementation details.', 'The profiling data was captured using the PyTorch Profiler. After downloading, you can visualize it directly by navigating to chrome://tracing in the Chrome browser (or edge://tracing in the Edge browser).', 'Notice that we simulate an absolutely balanced MoE routing strategy for profiling. <h2>Training</h2> The training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe.', 'Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.', '<h2>Inference</h2> <h3>Prefilling</h3> For prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU.', 'In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.', '<h3>Decoding</h3> For decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU.', 'Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication.', 'However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished.', 'For more information about the all-to-all implementation, please refer to DeepEP. When using expert parallelism (EP), different experts are assigned to different GPUs.', 'Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced.', 'As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs.', 'Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.', 'To facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads.', \"Note that the exact method to predict the loads of experts is out of this repo's scope. A common method is to use moving average of historical statistics. <h2>The Algorithm</h2> The load balancing algorithm comes with two policies used for different cases.\", '<h2>Hierarchical Load Balancing</h2> When the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing.', 'We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced.', 'The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.', '<h3>Global Load Balancing</h3> In other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs.', 'This policy can be adopted in decoding stage with a larger expert-parallel size. The Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads.', 'It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications.', 'Key features and benefits of 3FS include: <ul> <li> Performance and Usability <ul> <li>Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.</li> <li>Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application code simple and easy to reason about.</li> <li>File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB).', 'The file interface is well known and used everywhere.', 'There is no need to learn a new storage API.</li> </ul> </li> <li> Diverse Workloads <ul> <li>Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.</li> <li>Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.</li> <li>Checkpointing Supports high-throughput parallel checkpointing for large-scale training.</li> <li>KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.</li> </ul> </li> </ul> <h2>Performance</h2>  <li>Peak throughput</li>  The following figure demonstrates the throughput of read stress test on a large 3FS cluster.', 'This cluster consists of 180 storage nodes, each equipped with 2×200Gbps InfiniBand NICs and sixteen 14TiB NVMe SSDs. Approximately 500+ client nodes were used for the read stress test, with each client node configured with 1x200Gbps InfiniBand NIC.', 'The final aggregate read throughput reached approximately 6.6 TiB/s with background traffic from training jobs. <li>GraySort</li>  We evaluated smallpond using the GraySort benchmark, which measures sort performance on large-scale datasets.', 'Our implementation adopts a two-phase approach: (1) partitioning data via shuffle using the prefix bits of keys, and (2) in-partition sorting. Both phases read/write data from/to 3FS.', 'The test cluster comprised 25 storage nodes (2 NUMA domains/node, 1 storage service/NUMA, 2×400Gbps NICs/node) and 50 compute nodes (2 NUMA domains, 192 physical cores, 2.2 TiB RAM, and 1×200 Gbps NIC/node).', 'Sorting 110.5 TiB of data across 8,192 partitions completed in 30 minutes and 14 seconds, achieving an average throughput of 3.66 TiB/min. <li>KVCache</li>  KVCache is a technique used to optimize the LLM inference process.', 'It avoids redundant computations by caching the key and value vectors of previous tokens in the decoder layers.', 'The top figure demonstrates the read throughput of all KVCache clients, highlighting both peak and average values, with peak throughput reaching up to 40 GiB/s.', 'The bottom figure presents the IOPS of removing ops from garbage collection (GC) during the same time period.', '<source name=\"https://medium.com/@visithkumarapperuma/deepseek-v3-a-game-changer-in-a-i-heres-why-it-matters-75591957ca07\"> author - Visith Kumarapperuma  Currently, the AI models from the Chinese startup Deepseek are causing quite a stir in the AI space.', 'Their latest reasoning model, Deepseek r1, shows better or equal performance to competitors. But above all, they achieved it with a fraction of the training and inference cost.', 'DeepSeek’s AI Assistant overtook ChatGPT to become the most downloaded free app on the U.S. App Store. This development has led to market concerns about A.I. investments to major U.S. tech companies. Impacting share prices of tech firms including Nvidia.', '<h2>So what made Deepseek such a big impact to A.I. ?</h2> The significance of Deepseek as a disruptor in the industry lies in its approach. Unlike other companies that pushed for better hardware, Deepseek improved the algorithms.', 'Thus achieving better results at a software level. Note that the following details are for the Deepseek V3 model. • Deepseek said it trained a model using a data centre of some 2,000 of Nvidia H800 GPUs.', '• Time duration 2 months with the cost of the *final training run being ~$5.5 million This ~$5.5M reflects the “rental” cost for the GPU hours needed to train DeepSeek‑V3. It does not include: 1. The capital expenditure for owning the hardware. 2.', 'Costs associated with prior research, ablation studies, or experiments on alternative architectures/algorithms/data.', '<h3>Deepseek made training more efficient (45 times more efficient)</h3> <ul> <li>Use 8-bit instead of 32-bit to save memory.</li> <li>Compress key value indices which eat up a lot of VRAM; they got 93% compression ratios.</li> <li>Do multi-token prediction instead of single-token prediction -&gt; doubled inference speeds</li> <li>The MOE model decomposes a big model into small models that can run on consumer-grade hardware.</li> </ul> <h2>Summary of how Deepseek v3 was so efficient at training the frontier model</h2>  <li>Model Architecture The model employs a Mixture-of-Experts (MoE) architecture, where only 37B parameters fire for each token out of the total 671B.', 'This sparse activation significantly reduces compute requirements compared to dense models. The model uses Multi-head Latent Attention (MLA).', 'This compresses the Key-Value cache, reducing memory usage and enabling more efficient training.</li> <li>FP8 Mixed Precision Training: They implemented an FP8 mixed precision training framework.', 'Which reduces memory usage and accelerates training compared to higher precision formats. Reduced memory footprint by up to 50% compared to traditional FP16/FP32 formats.', 'They use fine-grained quantisation strategies and increased accumulation precision to maintain accuracy.</li> <li>Load Balancing Strategy They pioneered an auxiliary loss-free strategy for load balancing in the MoE architecture.', 'This improved performance without the drawbacks of traditional auxiliary loss methods.</li> <li>Training Framework They developed a custom training framework called HAI-LLM with several optimisations: DualPipe algorithm for efficient pipeline parallelism.', 'This reduces pipeline bubbles and overlapping computation and communication. Efficient cross-node all-to-all communication kernels to fully utilise network bandwidth.', 'Careful memory optimisations to avoid using costly tensor parallelism.</li>  <h2>Breakdown of the costs of the Deepseek v3 model</h2> Deepseek’s flagship model v3 showcases an architecture with a 671B parameter MOE (Mixture of Agents) with 37B active parameters per token - Their success stems from breakthrough engineering: using MoE architecture, implementing FP8 mixed precision training, and developing a custom HAI-LLM framework.', '- Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5 Sonnet. - For writing and coding tasks, Claude 3.5 Sonnet maintains a slight lead.', '- Deepseek pre-trained this model on 14.8 trillion high-quality data, taking 2,788,000 GPU hours on the Nvidia h800s cluster, costing around only $6 million - the Llama 403b was trained on 11x of that, taking 30,840,000 GPU hours, also on 15 trillion tokens.', '<code>So how true is the claim of $5.5 million, or is it another marketing trick?</code>  <li>Underlying FLOP calculations Model Details:</li> <li>Active Parameters: 37B (using FP8 precision)</li> <li>FLOPs per token: Using the rule of thumb “6 FLOPs per parameter per token.” <code>37B×6 = 222B FLOPs per token</code></li> <li>Total Training Tokens: Approximately 14.8 trillion tokens</li> <li>Total FLOPs required: <code>222 B FLOPs/token×14.8 T tokens ≈ 3.3×10²⁴ FLOPs</code></li>  <h3>GPU FLOP Capacity (H800/H100):</h3> An H100 is roughly estimated to deliver about.', '3.958×10¹⁵ FLOPs (per second or per some standardised interval — here used as a comparative metric). Ideal (Perfect Efficiency) GPU hours.', '(Dividing total required FLOPs by per‑GPU capability gives) <code>3.3×10²⁴ / 3.958×10¹⁵ \\u200b≈ 8.33×10⁸ seconds⇒≈0.4 million GPU hour</code> Note: This “perfect efficiency” scenario is a lower bound. Real-world training is less efficient. 2.', 'Adjusting for Real‑World Inefficiencies (Comparison with Llama 3.1) Reference Model: Llama 3.1 (405B parameters, 15 T tokens) reportedly required 30.84 M GPU hours in practice.', 'Recalculating FLOPs for Llama 3.1: <code>Using the same math: 3.64×10²⁵ FLOPs required</code> Scaling Efficiency Using the ratio of FLOPs needed for DeepSeek‑V3 versus Llama 3.1. and assuming similar inefficiencies.', 'The estimate adjusts to roughly 2.79M GPU hours for DeepSeek‑V3 training. 3.', 'DeepSeek‑V3 Reported Training Breakdown According to the DeepSeek‑V3 paper Pre‑training Stage: - Per Trillion Tokens: 180K H800 GPU hours - Overall Pre‑training: Total of 2,664K GPU hours - This stage was completed in less than two months using a cluster of 2,048 H800 GPUs.', 'Context Length Extension: - Additional 119K GPU hours Post‑training: - An extra 5K GPU hours Total GPU Hours: <code>2,664 K+119 K+5 K≈2.788M GPU hours</code> 4.', 'Cost Estimation Assumed GPU Rental Price: $2 per GPU hour Total Rental Cost: <code>2.788M GPU hours×$2/hour≈$5.576 million</code> as stated in Deepseek paper During the pre‑training stage, training DeepSeek‑V3 on each trillion tokens requires only 180K H800 GPU hours… Consequently, our pre‑training stage is completed in less than two months and costs 2664K GPU hours.', 'Combined with 119K GPU hours for the context length extension and 5K GPU hours for post‑training, DeepSeek‑V3 costs only 2.788M GPU hours for its full training.', 'Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. 5.', 'Summary Theoretical (Perfect Efficiency) Estimate: ~0.4 M GPU hours (using idealised FLOP counts and assuming perfect hardware utilisation0 Adjusted (Real‑World) Estimate (via Llama 3.1 comparison): ~2.79 GPU hours DeepSeek‑V3 Reported Breakdown: Pre‑training: 2,664K GPU hours Context Extension: 119K GPU hours Post‑training: 5K GPU hours Total: ~2.788 M GPU hours <h3>Cost (at $2 per GPU hour): ~$5.576 million</h3>', '<source name=\"https://medium.com/@jjjy213/deepseek-v3-explained-fdac83ba280c\"/> author - Ataka jeong  <li>Introduction How could the DeepSeek-V3 model achieve incredible performance and economical training as an open source model?', 'In this paper review, we will explore the various features that were invented and applied to build the DeepSeek-V3 model.', 'The way the paper presents the model may seem complicated to people who are unfamiliar with the new conceptualization invented by DeepSeek. However, its core principle still resembles that of the standard Transformer and well-known LLMs.', 'It will be incredibly helpful to have general knowledge of previously released large language models like LLaMA. I will also add my own interpretation of the DeekSeek model in this story.', 'Let’s dive into the new features of model architecture step by step.</li> <li>Model Architecture First of all, we will investigate the core architecture of DeekSeek-V3 model. The DeekSeek-V3 model has inherited most parts of model from previous V2 model.', 'These parts of model were elaborated more in V2 paper, but it is worth noting the principle how V3 model was built upon.', 'While they used the structure of the ordinary transformer block like Llama, its attention and Feed-Forward Network were more sophisticated to boost the model performance. The overview of the Transformer block is as shown in the following diagram.', 'The two main components are Multi-Head Latent Attention(MLA) and DeepSeekMoE.</li> <li>2.1 Multi-Head Latent Attention(MLA) What is Multi-Head Latent Attention(MLA)? You might noticed that “Latent” is only additional word to conventional attention module.', 'MLA improved the speed and memory usage in the attention block by compressing the input vector. From a data analysis perspective, the data can be compressed into a lower dimension while preserving the information it contains.', 'One of the well-known techniques is Principal component analysis (PCA), which reduces the dimension of the data and maintains variance to retain its information.', 'In latent diffusion model, the input data is compressed by variational autoencoder and reconstructed in initial dimension. The Multi-Head Latent Attention(MLA) applied this principle to compress and decompress the input data.', 'By storing a compressed vector for the KV cache, the DeepSeek model can improve both speed by reducing data copying and memory efficiency by using a smaller compressed vector.', 'The weight matrix for compression is additionally required, because human cannot compress it manually, but AI should learn it how the compression should be done.', 'Applying RoPE to the compressed vector is not mathematically compatible, which was shown in detail in V2 paper, they used decoupled RoPE. As illustrated in the figure above, RoPE is applied to query and key, but also the query and key without RoPE.', 'The RoPE-applied query and key are then concatenated with their respective non-RoPE counterparts. Finally, the query and key are obtained as normal transformer block, then the computation of dot-product attention will not be different from original one.', 'But we could reach this point with a more economical KV cache thanks to the lower dimension of data.</li> <li>2.2 DeekSeekMoE Secondly, you can note that Feed-Forward Network is unusual as it was split into a lot of experts, rather than one large FFN.', 'They called it as DeekSeekMoE. Like humans in a group, the AI also needs to specialized in certain domain to improve the performance. Thus, the mixture of experts come into play here.', 'Each expert can specialize in certain domain, in this case, the group of tokens that they are familiar with, instead of coping with entire range of tokens alone.', 'Dependent on the input sequence(tokens), the certain experts are selected to be activated and they contribute to make output. Shared experts are generalist and are activated for all kind of tokens.', 'Then it might be interesting to know by what algorithm we can select the experts? We need to assign a vector to each expert which determines the range of tokens(domain) that experts can deal with well.', 'And we give score to each expert to check how similar the domain of expert and input token are. If the score is high, then we should select the expert and let them activated to make output. Well, it sounds quite simple. Let’s see the math behind it.', 'eᵢ is a centroid vector. It is learned during training and represents the type of input tokens the expert specialized in. Each expert’s centroid vector encodes the knowledge domain it specializes in. uₜ is input vector to FFN.', 'The dot product uₜᵀ eᵢ quantifies the similarity between the input vector uₜ\\u200b and the centroid (or domain) of expert eᵢ, effectively measuring the alignment of the input data with the expert’s specialized domain.', 'So, the sᵢ = Sigmoid(uₜᵀ eᵢ) represents the score for each i-th expert, determining whether the expert should be selected. By gating value gᵢ, which select Kᵣ experts with high score by Topk algorithm.', 'We add all outputs of selected experts and shared experts, then we arrive to the final output. 2.3 Multi-Token Prediction In a standard transformer, the model generates a token each time, and this new token is fed back into decoder as input.', 'Since this way restricts the efficiency and the speed of convergence during training, many researchers have made effort to come up with a method to generate multiple tokens each time. DeepSeek improved the conventional way of Multi-Token Prediction(MTP).', 'Instead of previous parallel MTP, DeepSeek decided sequential MTP. They construct independent MTP modules, where the previous output of the Transformer block is concatenated into the subsequent MTP module, as illustrated in the following figure.', 'As shown in the figure, the structure of MTP modules is akin to RNN model. But, unlike RNN, which preserve hidden states of nodes, the MTP modules send output of prediction to the subsequent module.', 'Even though a single Transformer block cannot generate multiple tokens, the entire system of MTP modules collectively enables multi-token prediction.', 'As it compares additional tokens per prediction, it provides more information for weight updates during training, leading to more efficient learning and faster convergence. The model can proactively learn and prepare for the additional tokens.', 'In actual training, DeepSeek opted to generate only one additional token, presumably due to the computational cost which is caused by using many MTP modules. It necessitate the compromise between the benefits of MTP and computational cost.', 'During inference, the MTP modules are discarded, generating only one token per prediction.</li> <li>Infrastructure 3.1 DualPipe Since the U.S. did not export great GPUs like the NVIDIA H100 to China, DeepSeek researchers had to devise innovative methods to accelerate model training using the weaker H800 GPUs.', 'Since they succeeded, NVIDIA’s stock price briefly plunged, as people believed that high-performance GPUs would no longer be necessary for training LLM.', 'Because the DeepSeek model was trained on 2048 H800 GPUs, communication between GPUs accounts for large portion of training time. Therefore, enhanching networking between GPUs has to play crucial role to reduce training time.', 'When we use many GPUs simultaneously, the GPUs have to wait for a certain amount of time until new data is copied from other GPU.', 'This waiting time, which causes training inefficiencies, is known as a “bubble,” and we should minimize it as much as possible. DeepSeek invented a innovative method to reduce bubble.', 'During model training, data flows through the model in forward and backward processes. In forward process, data goes from the input layer to the output layer.', 'On the other hand, during the backward process data moves from the output layer to the input later, updating weights based on the information to minimize the loss.', 'Prior to DeepSeek, researchers found that the backward process can be split into two processes, which are backward for input and backward for weight, in order to remove more bubbles.', 'The backward for input is computation of the gradient of the loss with respect to the input data, whereas the backward for weight calculates gradient of the loss with respect to the weight.', 'The backward for input must be completed ahead of the backward for weight, because it is necessary to compute the backward for weight.', 'Mathematically, the chain rule is applied to the calculation of backpropagation, where the backward for input is used for the calculation of backward for weight.', 'In such process, it is certain that an enormous number of communications between GPUs is required.', 'In order to reduce the number of communication, the DeepSeek’s DualPipe combines the forward process and the backward for input by initiating training data from two devices in the opposite directions as illustrated in following figure.</li>  The batch 0 is the initial data, which starts processing on the device 0 and continues on the subsequent devices.', 'In a conventional training plan, the device 7 remains idle, waiting for the batch 0 to be copied onto it. However, DualPipe makes the device 7 start training with other batch data in the opposite direction.', 'This allows us to combine them as a chunk and continuously copy them together on other devices to reduce communication between GPUs.', 'With weaker H800 GPUs, they couldn’t improve the speed of the GPUs, but reduce the communication between GPUs to accelerate the training.', '3.2 Mixed precision training Mixed precision training is already prevalent technique of LLM to improve training and memory efficiency while maintaining the model accuracy.', 'In mixed precision training, it is critical task to find out which parts of model are less significant for the model accuracy and reduce the precision that parts.', 'In DeepSeek-V3 model, the researchers have found that they should reduce precision in the parts of model where heavy computations are executed, such as matrix multiplication.', 'In contrast, they preserved high precision for matrix addition and storing data, which are relatively lightweight computation. The mixed precision training of DeepSeek is shown in the following figure.', 'While reducing the precision by the method above, overflow and underflow arise as a impediment. If the numerical values are quantized in lower precision like FP8 format, the values are clipped to the certain representable range.', 'While computation in lower precision, the values can easily exceed the range during the computation. Scaling the values can mitigate the overflow and underflow by adjusting the values and lead to more proper representation in limited range.', 'But, static scaling, which applies fixed scaling factor to all values, can still cause overflow and underflow for many values. To cope with this issue, DeepSeek implemented Fine-Grained Quantization.', 'In this method, the values are grouped, and each group has its own scaling factor. This approach allows the each group of values to have a more suitable scaling factor, by which the overflow and underflow can be averted.', 'Another issue of quantization is that the small errors can be accumulated and become more serious problem later.', 'In order to avoid that a lot of values with error are summed and their errors are accumulated, intermediate values are copied in high precision, if the number of values reaches the interval.', 'It means that some values are grouped, and their values are stored in high precision. Then, the errors of values aren’t accumulated on a large scale, because the small group of values don’t contribute to large error.', 'These two techniques to prevent quantization error are visualized in following figure. <li>Reinforcement Learning After supervised fine-tuning, DeepSeek additionally implemented reinforcement learning.', 'A reward model has to be built and trained for reinforcement learning, which gives feedback to the model and determine the direction of learning. The rule-based reward model(RM) and model-based reward model(RM) were employed.', 'The rule-based RM is applied to the questions with specific rules, such as math problems and LeetCode problems. In these domains, the specific rules are used to verify the correctness of the answers and the questions about logical reasoning are involved.', 'However, for many questions, the answer cannot be verified by a specific rule. In those cases where no rule is provided, the model-based RM determines, whether the answer matches the ground-truth answer.', 'Another innovative idea of DeepSeek is including the chain-of-thought to the reward, whereas conventional models only included final reward based on the answer. DeepSeek-V3 model, as V2 model did, adopted Group Relative POlicy Optimization (GRPO).', 'This GRPO algorithm maximizes the following objective by updating the policy model π.</li>  Maximize this objective by updating the weights of the model based on the reward. Advantage is defined as the normalized reward.', 'In LLM case, the policy model π is model itself, and θ is weights of the model. q is question and o is output of the model.', 'We can interpret the policy model(LLM) outputs a probability distribution over tokens, where the policy π(o|q) is a probability of output o given the question q. Therefore, the policy model is LLM itself.', 'If the output o is right answer, we should reinforce the probability of that model makes this output o. So we need to maximize π(o|q) by multiplying advantage(normalized reward).', 'If the output o is correct, the advantage (reward) will be a positive value and the policy will be reinforced. Otherwise, it will be negative and π(o|q) should be minimized.', 'Plus, we have a fine-tuned model as the initial base model and do not want it to go too far from this base model, which might cause model to forget basic language understanding and important knowledge that the model learned during pre-training and fine-tuning.', 'To implement this safety concerns, GRPO algorithm used KL divergence and epsilon parameter. The KL divergence measures the difference between current policy model and reference policy model(initial base model).', 'So the KL divergence term should be minimized to maximized the GRPO objective. And we pick minimum between the original policy and the clipped policy in (1-ε, 1+ε), by which the model cannot deviate too much from 1.', 'So, the current policy cannot differ a lot from the old policy, restricting the effect of reinforcement learning. This GRPO algorithm based on rule-based and model-based reward model enhances model performance and reasoning capability.', '<li>Conclusion DeepSeek-V3 model offered great opportunity for efficient training with cheaper GPUs. It is unclear that its performance exceeds the OpenAI model, but DeepSeek is way more economical to train and open-source model.', 'AI researchers can directly use DeekSeek models and they can also implement the innovative ideas and designs in their own model, because the new methods of DeepSeek and source code are opened.', 'Seemingly, the DeepSeek researchers have potential to come up with more advanced idea to improve the model performance and efficient training process.', 'In AI development, a lower training cost almost always implies better model accuracy later on, as the data and model can easily be scaled up at a lower cost.', 'I hope that the performance of a good AI model does not have to be undermined by the censorship and suppression of the Chinese government.</li>', \"We're a tiny team @deepseek-ai pushing our limits in AGI exploration.\", \"Starting this week , Feb 24, 2025 we'll open-source 5 repos – one daily drop – not because we've made grand claims, but simply as developers sharing our small-but-sincere progress with full transparency.\", 'These are humble building blocks of our online service: documented, deployed and battle-tested in production. No vaporware, just sincere code that moved our tiny yet ambitious dream forward. Why?', \"Because every line shared becomes collective momentum that accelerates the journey. Daily unlocks begin soon. No ivory towers - just pure garage-energy and community-driven innovation 🔧 Stay tuned – let's geek out in the open together.\", '<h2>Day 1 - FlashMLA</h2> Efficient MLA Decoding Kernel for Hopper GPUs Optimized for variable-length sequences, battle-tested in production 🔗 FlashMLA GitHub Repo ✅ BF16 support ✅ Paged KV cache (block size 64) ⚡ Performance: 3000 GB/s memory-bound | BF16 580 TFLOPS compute-bound on H800 <h2>Day 2 - DeepEP</h2> Excited to introduce DeepEP - the first open-source EP communication library for MoE model training and inference.', '🔗 DeepEP GitHub Repo ✅ Efficient and optimized all-to-all communication ✅ Both intranode and internode support with NVLink and RDMA ✅ High-throughput kernels for training and inference prefilling ✅ Low-latency kernels for inference decoding ✅ Native FP8 dispatch support ✅ Flexible GPU resource control for computation-communication overlapping <h2>Day 3 - DeepGEMM</h2> Introducing DeepGEMM - an FP8 GEMM library that supports both dense and MoE GEMMs, powering V3/R1 training and inference.', '🔗 DeepGEMM GitHub Repo ⚡ Up to 1350+ FP8 TFLOPS on Hopper GPUs ✅ No heavy dependency, as clean as a tutorial ✅ Fully Just-In-Time compiled ✅ Core logic at ~300 lines - yet outperforms expert-tuned kernels across most matrix sizes ✅ Supports dense layout and two MoE layouts <h2>Day 4 - Optimized Parallelism Strategies</h2> ✅ DualPipe - a bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training.', '🔗 GitHub Repo ✅ EPLB - an expert-parallel load balancer for V3/R1. 🔗 GitHub Repo 📊 Analyze computation-communication overlap in V3/R1.', '🔗 GitHub Repo <h2>Day 5 - 3FS, Thruster for All DeepSeek Data Access</h2> Fire-Flyer File System (3FS) - a parallel file system that utilizes the full bandwidth of modern SSDs and RDMA networks.', '⚡ 6.6 TiB/s aggregate read throughput in a 180-node cluster ⚡ 3.66 TiB/min throughput on GraySort benchmark in a 25-node cluster ⚡ 40+ GiB/s peak throughput per client node for KVCache lookup 🧬 Disaggregated architecture with strong consistency semantics ✅ Training data preprocessing, dataset loading, checkpoint saving/reloading, embedding vector search &amp; KVCache lookups for inference in V3/R1 📥 3FS → https://github.com/deepseek-ai/3FS ⛲ Smallpond - data processing framework on 3FS → https://github.com/deepseek-ai/smallpond <h2>Day 6 - One More Thing: DeepSeek-V3/R1 Inference System Overview</h2> Optimized throughput and latency via: 🔧 Cross-node EP-powered batch scaling 🔄 Computation-communication overlap ⚖️ Load balancing Production data of V3/R1 online services: ⚡ 73.7k/14.8k input/output tokens per second per H800 node 🚀 Cost profit margin 545%', '<h2>Design and implementation</h2> The 3FS system has four components: cluster manager, metadata service, storage service and client. All components are connected in an RDMA network (InfiniBand or RoCE).', 'Metadata and storage services send heartbeats to cluster manager. Cluster manager handles membership changes and distributes cluster configuration to other services and clients.', 'Multiple cluster managers are deployed and one of them is elected as the primary. Another manager is promoted as primary when the primary fails.', 'Cluster configuration is typically stored in a reliable distributed coordination service, such as ZooKeeper or etcd. In our production environment, we use the same key-value store as file metadata to reduce dependencies. File metadata operations (e.g.', 'open or create files/directories) are sent to metadata services, which implement the file system semantics. Metadata services are stateless, since file metadata are stored in a transactional key-value store (e.g. FoundationDB).', 'Clients can connect to any metadata service. Each storage service manages a few local SSDs and provides a chunk store interface. The storage service implements Chain Replication with Apportioned Queries (CRAQ) to ensure strong consistency.', 'CRAQ’s write-all-read-any approach helps to unleash the throughput of SSDs and RDMA network. A 3FS file is split into equally sized chunks, which are replicated over multiple SSDs. Two clients are developed for applications: FUSE client and native client.', 'Most applications use FUSE client, which has a low adoption barrier. Performance-critical applications are integrated with the native client. <h2>File system interfaces</h2> Object store is becoming a popular option for data analytics and machine learning.', 'However, file system semantics and a unified namespace where files are organized in directories provide greater flexibility for applications.', '<ul> <li> <em>Atomic directory manipulation</em> An object store can approximate hierarchical directory structures by using slashes (/) in object keys.', 'However, it doesn’t natively support operations like atomically moving files/directories, or recursively deleting entire directories.', 'Actually a common pattern in our internal applications involves creating a temporary directory, writing files to it, and then moving the directory to its final location.', 'When handling a large number of small files, the recursive delete for directories is crucial. Without it, applications have to traverse each directory and remove files one by one.', '</li> <li> <em>Symbolic and hard links</em> Our applications utilize symbolic and hard links to create lightweight snapshots of dynamically updated datasets, where new data is appended as individual files.', '</li> <li> <em>Familiar interface</em> The file interface is well known and used everywhere. There is no need to learn a new storage API. Many datasets are stored as CSV/Parquet files.', 'Adapting file-based data loaders to use the 3FS FUSE client or native client is straightforward.', '</li> </ul> <h3>Limitations of FUSE</h3> FUSE (Filesystem in Userspace) simplifies file system client development by redirecting I/O operations to user-space processes through the FUSE kernel module.', 'It creates the illusion that applications are accessing the remote file system as if it were a local file system.', 'However, it has performance limitations: <ul> <li> <em>Memory copy overhead</em> The user-space file system daemon cannot access application memory. Data transfer between kernel and user spaces consumes memory bandwidth and increases end-to-end latency.', '</li> <li> <em>Primitive multi-threading support</em> When an application initiates I/O requests, FUSE places these requests into a multi-threaded shared queue, protected by a spin lock.', 'The user-space file system daemon then retrieves and processes requests from this queue. Due to lock contention, FUSE’s I/O processing capability fails to scale with the number of threads.', 'Our benchmark results indicate that FUSE only handles approximately 400K 4KiB reads per second. Further increasing concurrency does not improve performance as lock contention intensifies.', '<code>perf</code> profiling reveals that the kernel-space spin lock consumes a significant amount of CPU time. </li> </ul> Most applications, e.g.', 'data analytics, perform large block writes on 3FS or they can buffer data in memory and flush it to 3FS when write buffer is full.', 'However, FUSE on Linux 5.x does not support concurrent writes to the same file<a href=\"https://elixir.bootlin.com/linux/v5.4.284/source/fs/fuse/file.c#L1573\">^1</a>.', 'Applications overcome this limitation by writing to multiple files concurrently, maximizing the total throughput. Read operations exhibit more complex patterns.', 'Some training jobs require random access to dataset samples, with read sizes varying from a few kilobytes to several megabytes per sample. And samples are typically not 4K-aligned in files.', 'Data loaders are specifically designed to fetch batches of samples. But they perform poorly when handling small random reads on FUSE-mounted 3FS. Bandwidth of SSDs and RDMA network are not fully utilized.', '<h3>Asynchronous zero-copy API</h3> Implementing the file system client as a VFS kernel module avoids performance issues mentioned above. But kernel module development is significantly more challenging than user-space system programming.', 'Bugs are difficult to diagnose and can lead to catastrophic failures in production environments. For example, machines may crash and leave no log message for debugging.', 'When upgrading a kernel module, all processes using the file system must be stopped cleanly; otherwise, a machine restart is required. For these reasons, we have chosen to implement a native client within the FUSE daemon.', 'This client offers an interface that supports asynchronous zero-copy I/O operations. File meta operations are still handled by FUSE daemon (e.g. open/close/stat files).', 'Applications call <code>open()</code> to obtain a file descriptor (fd) and register it via native API. They can then perform I/O operations on the file with native client.', 'This approach ensures consistency in metadata operations with the POSIX API, making it easier to migrate existing code. The asynchronous, zero-copy API is inspired by Linux <code>io_uring</code>.', 'Below are the key data structures in the API: <ul> <li> <em>Iov</em> A large memory region for zero-copy read/write operations, shared between the user process and the native client. InfiniBand memory registration is managed by the client.', 'In native API, all read data will be read into Iov, and all write data should be written to Iov before calling the API. </li> <li> <em>Ior</em> A small shared ring buffer for communication between user process and native client.', 'The usage of Ior is similar to Linux <code>io_uring</code>, where the user process enqueues read/write requests, and the native client dequeues these requests for completion.', 'The requests are executed in batches, with their sizes controlled by the <code>io_depth</code> parameter. Multiple batches are processed in parallel, whether from different rings or the same ring.', 'However, multiple rings are still recommended for multi-threaded applications, as sharing a ring requires synchronization, which can impact performance. </li> </ul> Within the native client, multiple threads are spawned to fetch I/O requests from the Iors.', 'These requests are batched and dispatched to storage services, reducing RPC overhead caused by small read requests.', '<h2>File metadata store</h2> <h3>Location of file chunks</h3> 3FS divides file data into equally sized chunks and stripes them across multiple replication chains (replication chains and chain tables are defined in Section <a href=\"#data-placement\">Data placement</a>).', 'Users can specify the chain table, chunk size, and stripe size for files on a per-directory basis. Each chunk is independently stored on multiple storage services, with its chunk ID generated by concatenating the file’s inode id and chunk index.', 'When creating a new file, the metadata service employs a round-robin strategy to select consecutive replication chains from the designated chain table, based on the stripe size. Next, a random seed is generated to shuffle the selected chains.', 'This allocation strategy ensures balanced data distribution across chains and SSDs. When an application opens a file, the client contacts the meta service to obtain the file’s data layout information.', 'Then the client can independently compute chunk IDs and chains for data operations, minimizing the involvement of the meta service in the critical path.', '<h3>File metadata on transactional key-value store</h3> 3FS uses FoundationDB as its distributed storage system for metadata. FoundationDB provides a key-value store interface and supports transactions with Serializable Snapshot Isolation (SSI).', '3FS stores all metadata as key-value pairs in FoundationDB. Meta services follow a stateless architecture, greatly enhancing maintainability by allowing administrators to seamlessly upgrade or restart services without disruption.', 'When clients experience request failures or timeouts, they can automatically fail over to other available services. The file system metadata primarily consists of two core structures: inodes and directory entries.', 'Inodes store attribute information for files, directories, and symbolic links, each identified by a globally unique 64-bit identifier that increments monotonically.', 'Inode keys are constructed by concatenating the \"INOD\" prefix with the inode id, which is encoded in little-endian byte order to spread inodes over multiple FoundationDB nodes.', 'The inode values vary by its type: <ul> <li> All inode types contain basic attributes: ownership, permissions, access/modification/change times.', '</li> <li> Additional attributes for file inodes: file length, chunk size, selected range in chain table, shuffle seed.', '</li> <li> Additional attributes for directory inodes: the parent directory’s inode id, default layout configurations for subdirectories/files (chain table, chunk size, stripe size).', 'The parent’s inode id is required to detect loops when moving directories.', 'When moving <code>dir_a/dir_b</code> to <code>dir_c/</code>, we need to ensure that <code>dir_c</code> is not a descendant of <code>dir_b</code>, which can be achieved by checking all ancestors of <code>dir_c</code> upward.', '</li> <li> Additional attributes for symbolic link inodes: target path string. </li> </ul> Directory entry keys are composed of a \"DENT\" prefix, the parent inode ID, and the entry name. Directory entry values store the target inode id and inode type.', 'All entries within a directory naturally form a contiguous key range, allowing efficient directory listing via range queries.', 'The meta operations leverage FoundationDB’s transactions: <ul> <li> Read-only transactions used for metadata queries: fstat, lookup, listdir etc. </li> <li> Read-write transactions used for metadata updates: create, link, unlink, rename etc.', '</li> </ul> For write transactions, FoundationDB tracks the read/write key sets to form conflict detection sets. When concurrent transaction conflicts are detected, the meta service automatically retries the transaction.', 'This design enables multiple meta services to process requests in parallel while maintaining file system metadata consistency.', '<h3>Dynamic file attributes</h3> On most local file systems, deleting an opened file is deferred until all associated file descriptors are closed. Consequently, it is necessary to track all file descriptors of the file.', 'Training jobs open a large number of files during startup. Storing all file descriptors would impose heavy load on meta service and FoundationDB.', 'Since training jobs do not depend on this feature, 3FS does not track file descriptors opened in read-only mode.', '3FS maintains a file session for each file descriptor (fd) opened in write mode since deleting write opened files may lead to unreclaimable garbage chunks from concurrent writes.', 'When a file with active write sessions is deleted, meta service delays the deletion until all its fds are closed.', 'To prevent lingering sessions from offline clients, the 3FS meta service periodically checks client liveness and cleans up sessions of offline clients. The file length is stored in the inode.', 'For files being actively updated, the length stored in inode may diverge from the actual length. Clients periodically (5 seconds by default) report to meta service maximum write position of each file opened in write mode.', 'If this position exceeds the length in inode and there is no concurrent truncate operation, this position is adopted as the new file length.', 'Due to the possibility of concurrent writes from multiple clients, the method described above ensures only eventual consistency for file lengths.', 'When processing close/fsync operations, the meta service obtains the precise file length by querying the ID and length of the last chunk from the storage service.', 'Since file data is striped across multiple chains, this operation incurs non-negligible overhead. Concurrent updates to the same file’s length by multiple meta services may cause transaction conflicts and lead to repeated file length computation.', 'To mitigate this, meta service distributes file length update tasks across multiple meta services using inode IDs and the rendezvous hash algorithm. Our production environments use a large stripe size: 200.', 'For small files, the number of chains containing file chunks is well below this number. The number of potentially used chains is stored in file inode and used as a hint when updating the length.', 'It starts with an initial value of 16 and is doubled each time additional file chunks are written to more chains. This allows us to avoid querying all 200 chains when updating lengths of small files.', 'This optimization can also be extended to the deletion of small files. <h2>Chunk storage system</h2> The design goal of chunk storage system is to achieve the highest bandwidth possible even when there are storage medium failures.', 'The read/write throughput of 3FS should scale linearly with the number of SSDs and bisection network bandwidth between clients and storage services. Applications access storage services in a locality-oblivious manner.', '<h3>Data placement</h3> Each file chunk is replicated over a chain of storage targets using chain replication with apportioned queries (CRAQ). In CRAQ write requests are sent to the head target and propagated along a chain.', 'Read requests can be sent to any of the storage target. Usually the read traffic is evenly distributed among all targets in a chain for better load balance. Multiple storage targets are created on each SSD and the targets join different chains.', 'Suppose there are 6 nodes: A, B, C, D, E, F. Each node has 1 SSD. Create 5 storage targets on each SSD: 1, 2, ... 5. Then there are 30 targets in total: A1, A2, A3, ..., F5. If each chunk has 3 replicas, a chain table is constructed as follows.', '| Chain | Version | Target 1 (head) | Target 2 | Target 3 (tail) | | :---: | :-----: | :-------------: | :------: | :-------------: | |   1   |    1    |      <code>A1</code>       |   <code>B1</code>   |      <code>C1</code>       | |   2   |    1    |      <code>D1</code>       |   <code>E1</code>   |      <code>F1</code>       | |   3   |    1    |      <code>A2</code>       |   <code>B2</code>   |      <code>C2</code>       | |   4   |    1    |      <code>D2</code>       |   <code>E2</code>   |      <code>F2</code>       | |   5   |    1    |      <code>A3</code>       |   <code>B3</code>   |      <code>C3</code>       | |   6   |    1    |      <code>D3</code>       |   <code>E3</code>   |      <code>F3</code>       | |   7   |    1    |      <code>A4</code>       |   <code>B4</code>   |      <code>C4</code>       | |   8   |    1    |      <code>D4</code>       |   <code>E4</code>   |      <code>F4</code>       | |   9   |    1    |      <code>A5</code>       |   <code>B5</code>   |      <code>C5</code>       | |  10   |    1    |      <code>D5</code>       |   <code>E5</code>   |      <code>F5</code>       | Each chain has a version number.', 'The version number is incremented if the chain is changed (e.g. a storage target is offline). Only the primary cluster manager makes changes to chain tables. A few chain tables can be constructed to support different data placement requirements.', 'For example, two chain tables can be created, one for batch/offline jobs and another for online services. The two tables consist of storage targets on mutually exclusive nodes and SSDs. Logically, the state of each chain changes independently.', 'Each chain can be included in multiple chain tables. The concept of chain table is created to let metadata service pick a table for each file and stripe file chunks across chains in the table.', '<h3>Balanced traffic during recovery</h3> Suppose read traffic is evenly distributed among all storage targets in the above chain table.', 'When A fails its read requests would be redirected to B and C. Under heavy load the read bandwidth of B, C is immediately saturated and B, C become the bottleneck of the entire system.', 'Replacing a failed SSD and syncing data to the new SSD can take several hours. The read throughput is impaired during this period. To reduce the performance impact, we can have more SSDs share the redirected traffic.', 'In the following chain table, A is paired with every other SSDs. When A fails, each of the other SSDs receives 1/5 of A’s read traffic.', '| Chain | Version | Target 1 (head) | Target 2 | Target 3 (tail) | | :---: | :-----: | :-------------: | :------: | :-------------: | |   1   |    1    |      <code>B1</code>       |   <code>E1</code>   |      <code>F1</code>       | |   2   |    1    |      <code>A1</code>       |   <code>B2</code>   |      <code>D1</code>       | |   3   |    1    |      <code>A2</code>       |   <code>D2</code>   |      <code>F2</code>       | |   4   |    1    |      <code>C1</code>       |   <code>D3</code>   |      <code>E2</code>       | |   5   |    1    |      <code>A3</code>       |   <code>C2</code>   |      <code>F3</code>       | |   6   |    1    |      <code>A4</code>       |   <code>B3</code>   |      <code>E3</code>       | |   7   |    1    |      <code>B4</code>       |   <code>C3</code>   |      <code>F4</code>       | |   8   |    1    |      <code>B5</code>       |   <code>C4</code>   |      <code>E4</code>       | |   9   |    1    |      <code>A5</code>       |   <code>C5</code>   |      <code>D4</code>       | |  10   |    1    |      <code>D5</code>       |   <code>E5</code>   |      <code>F5</code>       | To achieve maximum read throughput during recovery, the load balance problem can be formulated as a balanced incomplete block design.', 'The optimal solution is obtained by using integer programming solver. <h3>Data replication</h3> CRAQ is a write-all-read-any replication protocol optimized for read-heavy workloads.', 'Utilizing read bandwidth of all replicas is critical to achieve highest read throughput in an all-flash storage system.', 'When a write request is received by a storage service, it goes through the following steps:  <li> The service checks if the chain version in write request matches with the latest known version; reject the request if it’s not.', 'The write request could be sent by a client or a predecessor in the chain. </li> <li> The service issues RDMA Read operations to pull write data. If the client/predecessor fails, the RDMA Read operations may time out and the write is aborted.', '</li> <li> Once the write data is fetched into local memory buffer, a lock for the chunk to be updated is acquired from a lock manager. Concurrent writes to the same chunk are blocked. All writes are serialized at the head target.', '</li> <li> The service reads the committed version of the chunk into memory, applies the update, and stores the updated chunk as a pending version. A storage target may store two versions of a chunk: a committed version and a pending version.', 'Each version has a monotonically-increasing version number. The version numbers of committed version and pending versions are <code>v</code> and <code>u</code> respectively, and satisfy <code>u = v + 1</code>.', '</li> <li> If the service is the tail, the committed version is atomically replaced by the pending version and an acknowledgment message is sent to the predecessor. Otherwise, the write request is forwarded to the successor.', 'When the committed version is updated, the current chain version is stored as a field in the chunk metadata.', '</li> <li> When an acknowledgment message arrives at a storage service, the service replaces the committed version with the pending version and continues to propagate the message to its predecessor. The local chunk lock is then released.', '</li>  Suppose there are 3 targets in the chain: <code>A, B, C</code>. A write request has just entered step 5 at <code>A</code>. <code>A</code> forwards the request to successor <code>B</code>.', 'Then <code>B</code> instantly fails and the forwarded write request is lost. When cluster manager detects <code>B</code>’s failure, it marks <code>B</code> as offline and moves it to the end of chain and broadcasts the updated chain table.', 'Once <code>A</code> receives the latest chain table, it forwards the write request to the new successor <code>C</code>. <code>C</code> may not receive the latest chain table yet and rejects the request.', 'But <code>A</code> can keep forwarding the request to <code>C</code>. Eventually <code>C</code> gets the latest chain table and accepts the request.', 'When a read request arrives at a storage service:  <li> When the service only has a committed version of the chunk, this version is returned to the client. </li> <li> Unlike CRAQ, our implementation does not issue version query to the tail target.', 'When there are both committed and pending versions, the service replies a special status code to notify the client. The client may wait for a short interval and retry. Or the client can issue a relaxed read request to get the pending version.', '</li>  <h3>Failure detection</h3> The cluster manager relies on heartbeats to detect fail-stop failures. Cluster manager declares a service failed if it does not receive heartbeats from it for a configurable interval (e.g. T seconds).', 'A service stops processing requests and exits if it cannot communicate with cluster manager for T/2 seconds. The heartbeat can be seen as a request to *renew a lease* granted by the manager. The metadata services are stateless.', 'The list of online meta services provided by cluster manager is a simple service discovery mechanism that helps clients create connections to metadata services. If one meta service is down, the clients may switch to any other metadata service.', 'Cluster manager plays a more critical role in membership changes of storage services. It maintains a global view of chain tables and storage targets’ states. Each storage target has a public state and a local state.', 'Public state indicates if it’s ready to serve read requests and if write requests would be propagated to it. Public states are stored with chain tables and distributed to services and clients.', '| Public State | Read | Write | Notes                                           | | :----------- | :--: | :---: | :---------------------------------------------- | | serving      |  Y   |   Y   | service alive and serving client requests       | | syncing      |  N   |   Y   | service alive and data recovery is in progress  | | waiting      |  N   |   N   | service alive and data recovery not started yet | | lastsrv      |  N   |   N   | service down and it was the last serving target | | offline      |  N   |   N   | service down or storage medium failure          | Local state is only known by storage services and cluster manager, and it’s stored in the memory of cluster manager.', 'If a storage target has medium failure, the related service sets the target’s local state to offline in heartbeat. If a storage service is down, storage targets managed by the service are marked offline.', '| Local State | Notes                                                | | :---------- | :--------------------------------------------------- | | up-to-date  | service alive and serving client requests            | | online      | service alive and target in syncing or waiting state | | offline     | service down or storage medium failure               | A storage target can change from one public state to another in response to the latest local state.', 'The local state plays the role of a triggering event. The cluster manager periodically scans every chain and updates the public states of targets on the chain according to a state-transition table.', '<ul> <li> The chain version is incremented if the chain is updated. </li> <li> If a storage target is marked offline, it’s moved to the end of chain.', '</li> <li> If a storage service finds public state of any local storage target is lastsrv or offline, it exits immediately. The service may be isolated from the cluster manager by network partition error.', '</li> <li> Once the data recovery of a storage target in syncing state is completed, the storage service set the target’s local state to up-to-date in subsequent heartbeat messages sent to cluster manager.', '</li> </ul> | Local State | Current Public State | Predecessor’s Public State | Next Public State | | :---------- | :------------------- | :------------------------- | :---------------- | | up-to-date  | serving              | (any)                      | serving           | |             | syncing              | (any)                      | serving           | |             | waiting              | (any)                      | waiting           | |             | lastsrv              | (any)                      | serving           | |             | offline              | (any)                      | waiting           | | online      | serving              | (any)                      | serving           | |             | syncing              | serving                    | syncing           | |             |                      | not serving                | waiting           | |             | waiting              | serving                    | syncing           | |             |                      | not serving                | waiting           | |             | lastsrv              | (any)                      | serving           | |             | offline              | (any)                      | waiting           | | offline     | serving              | has no predecessor         | lastsrv           | |             |                      | has predecessor            | offline           | |             | syncing              | (any)                      | offline           | |             | waiting              | (any)                      | offline           | |             | lastsrv              | (any)                      | lastsrv           | |             | offline              | (any)                      | offline           | <h3>Data recovery</h3> When a storage service exits (e.g.', 'process crashes or restarts during upgrade), or a storage medium failure occurs, all related storage targets will be marked as offline and moved to the end of chains by cluster manager.', 'Once the service restarts, each target on the service enters into the recovery process independently. The entire recovery process overlaps with normal activity and minimizes any interruption.', 'When a previously offline storage service starts:  <li> The service periodically pulls latest chain tables from cluster manager. But it does not send heartbeats until all its storage targets have been marked offline in the latest chain tables.', 'This ensures all its targets would go through the data recovery process. </li> <li> When a write request arrives during recovery, the request is always a full-chunk-replace write.', 'The local committed version is updated and any existing pending version is abandoned. Since current service is the tail, an acknowledgment message is sent to the predecessor.', 'The full state of the predecessor is copied to the returning service through a continuous stream of full-chunk-replace writes.', '</li> <li> Before the data recovery of a storage target starts, the predecessor sends a dump-chunkmeta request to the returning service.', 'Then the service iterates the local chunk metadata store to collect the ids, chain versions and committed/pending version numbers of all chunks on the target, and replies the collected metadata to the predecessor.', '</li> <li> When a sync-done message arrives, the service knows that the storage target is up-to-date. It sets local state of the target to up-to-date in heartbeat messages sent to cluster manager.', '</li>  When a storage service finds a previously offline successor is online:  <li> The service starts to forward normal write requests to the successor.', 'Clients may only update a portion of the chunk, but the forwarded write requests should contain the whole chunk, i.e. a full-chunk-replace write. </li> <li> The service sends a dump-chunkmeta request to the successor.', 'Once the metadata of all chunks on the successor target are received, it collects the chunk metadata on its local target. Then it compares the two copies of chunk metadata to decide which chunks should be transferred.', '</li> <li> The selected chunks are transferred to the successor by issuing full-chunk-replace write requests. </li> <li> The chunk lock is first acquired for each chunk.', '</li> <li> The chain version, committed version number and chunk content are read and transferred to successor by sending a full-chunk-replace request. </li> <li> The chunk lock is released. </li>  4.', 'When all required chunks have been transferred, a sync-done message is sent to the successor. The rules used to decide which chunks should be transferred are: <ul> <li> If a chunk only exists on the local target, it should be transferred.', '</li> <li> If a chunk only exists on the remote target, it should be removed. </li> <li> If the chain version of local chunk replica is greater than that of the remote chunk replica, it should be transferred.', '</li> <li> If the chain versions of local/remote chunk replicas are the same but local committed version number does not equal to the remote pending version number, it should be transferred.', '</li> <li> Otherwise, two chunk replicas are either the same or being updated by in-progress write requests. </li> </ul> <h3>Chunks and the metadata</h3> File chunks are stored in the chunk engine.', 'On each SSD, the persistent storage of the chunk engine consists of a fixed number of data files for storing chunk data, and a RocksDB instance for maintaining chunk metadata and other system information.', 'Additionally, the chunk engine maintains an in-memory cache of chunk metadata to enhance query performance. A chunk allocator is implemented for fast allocation of new chunks.', 'The chunk engine interface provides thread-safe access through the following operations:  <li> <em>open/close</em> Initializes the engine by loading metadata from RocksDB and reconstructing chunk allocator states.', '</li> <li> <em>get</em> Retrieves chunk metadata and reference-counted handle through a hashmap cache, enabling concurrent access with O(1) average complexity.', '</li> <li> <em>update</em> Implements copy-on-write (COW) semantics by allocating new chunks before modifying data. Old chunks remain readable until all handles are released.', '</li> <li> <em>commit</em> Commit the updated chunk metadata to RocksDB via write batches to ensure atomic updates; synchronously refresh the chunk metadata cache. </li>  The chunk data will ultimately be stored on physical blocks.', 'Physical block sizes range from 64KiB to 64MiB in increments of powers of two, totaling 11 distinct sizes. The allocator will assign physical blocks whose sizes most closely match the actual chunk size.', 'A resource pool is constructed for each physical block size, with each pool containing 256 physical files. The usage status of physical blocks is maintained in memory using bitmaps. When a physical block is reclaimed, its bitmap flag is set to 0.', 'The actual storage space of the block remains preserved and will be prioritized for subsequent allocations.', 'When no available physical blocks remain, <code>fallocate()</code> will be used to allocate a contiguous large space in physical files, creating 256 new physical blocks - this approach helps reduce disk fragmentation.', 'When performing write operations on a chunk, the allocator first assigns a new physical block. The system then reads existing chunk data into a buffer, applies the update, and writes the updated buffer to the newly allocated block.', \"An optimized process is implemented for appends, where data is directly added in-place at the end of the existing block. A new copy of metadata is constructed from the new block's location and existing chunk metadata.\", 'Subsequently, both the new chunk metadata and statuses of new and old physical blocks are atomically updated in RocksDB.']\n"
          ]
        }
      ],
      "source": [
        "print(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mXYv-0b584WC"
      },
      "outputs": [],
      "source": [
        "dataset = []\n",
        "for i in range(len(qa_pairs_per_chunk)):\n",
        "    dataset.append({ \n",
        "        \"Instruction\": qa_pairs_per_chunk[i][\"question\"],  # ✅ No [0]\n",
        "        \"Answer\": qa_pairs_per_chunk[i][\"answer\"]       # ✅ No [0]\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOFa4fR2s04h",
        "outputId": "09b92e92-53cd-49ca-be77-364e29bfd22f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "575\n"
          ]
        }
      ],
      "source": [
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uJpKy-FY_1Cq"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"output.json\", \"w\") as file:\n",
        "    json.dump(dataset, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONwh1XPYAfzl",
        "outputId": "420e6f81-bc3a-4f6a-ee41-ce06ff41ff86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "print(len(documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU available. Training will run on CPU.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
        "else:\n",
        "    print(\"No GPU available. Training will run on CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Train dataset saved with 460 samples.\n",
            "✅ Test dataset saved with 115 samples.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Ensure reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Shuffle dataset\n",
        "random.shuffle(dataset)\n",
        "\n",
        "# Define split ratio (80% train, 20% test)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(dataset) * split_ratio)\n",
        "\n",
        "# Create train and test datasets\n",
        "train_dataset = dataset[:split_index]\n",
        "test_dataset = dataset[split_index:]\n",
        "\n",
        "# Save train dataset\n",
        "with open(\"train_dataset.json\", \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(train_dataset, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "# Save test dataset\n",
        "with open(\"test_dataset.json\", \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(test_dataset, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"✅ Train dataset saved with {len(train_dataset)} samples.\")\n",
        "print(f\"✅ Test dataset saved with {len(test_dataset)} samples.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
