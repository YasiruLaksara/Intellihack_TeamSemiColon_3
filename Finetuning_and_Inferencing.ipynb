{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2oy39bBSaeq",
        "outputId": "103357eb-8a45-46fc-b590-1b00051519b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rHWwb8cuyCsJ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "    !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggcjaI-1zkmB",
        "outputId": "a45c29b0-6520-4c13-80c0-9657d09e829e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from datasets import load_dataset\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Model configuration\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXTL2Vlg95Mk"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWF66KygyGcG",
        "outputId": "390df654-d883-49db-fdbc-22f9b42c48ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.3.9: Fast Qwen2 patching. Transformers: 4.48.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token=\"\"  # Replace with your token\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNbSpik-99sC"
      },
      "source": [
        "# using LORA Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kvkg8rAyKvi",
        "outputId": "02a4f76d-e86c-4703-9a93-12ce5239be08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.3.9 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,                       # LoRA rank\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6MdSPDk-EvU"
      },
      "source": [
        "# defining alpaca Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LaLg4PvLyP0c"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"Instruction\"]\n",
        "    outputs = examples[\"Answer\"]\n",
        "\n",
        "    texts = []\n",
        "    for instruction, output in zip(instructions, outputs):\n",
        "        text = alpaca_prompt.format(instruction, \"\", output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "\n",
        "    return { \"text\": texts }\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZW6Ah9I-Mpd"
      },
      "source": [
        "# load and split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "7cc8aa0d80a2444295042fb124c4cbc4",
            "3aa369565c8b49be9c78eb88b2d28b23",
            "7b80cbacc24f413baf02a88fc6043fac",
            "7620fdc8b798431a884c3f2e520e3db4",
            "355c837c137149b9b2eea0ba3f33d7df",
            "ccb62ce90e8a4bc3b37dbda22483a73d",
            "b64afa1b6c6a45da890b935dedd6a2ef",
            "0dda57e28b21478c96b8e782cf5d55cf",
            "ad6ea5b3df434bc5b88ed6b5d8780e01",
            "6eb3fb9726bf4822865d3e5b51ef9c85",
            "d38dd0740c924a23b04bd824363822dc",
            "71bc85155e5a4ae8a0ae037034a3e6ab",
            "7ca6dad475944ba5a556bc68c987d3c1",
            "84218c043677456f9299390a9aa3c865",
            "88c7da6cdb9c43aeb2f4a9d1e1704937",
            "f70ec2bb692c46f4ac6866b61718f0b2",
            "5aaf84a0a9e44eccb581021a489742b8",
            "b25bd95f54134bb0a9b53c30a6f6f0c1",
            "82997670bb684437aa9c1779c6592835",
            "43ca1befec1b4f43b13211a1ada52e69",
            "3292f898e38644f1b67d4d5c5d2d5365",
            "9f5965b5c00945e98948197b0ca0312b"
          ]
        },
        "id": "9ORr6HqAyUOu",
        "outputId": "5b260d40-3261-42c7-bdc9-204abb45ff6f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cc8aa0d80a2444295042fb124c4cbc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/460 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71bc85155e5a4ae8a0ae037034a3e6ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/115 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/output.json\")\n",
        "\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=3407)\n",
        "train_dataset = dataset[\"train\"]\n",
        "eval_dataset = dataset[\"test\"]\n",
        "\n",
        "train_dataset = train_dataset.map(formatting_prompts_func, batched=True, num_proc=2)\n",
        "eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True, num_proc=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WubdQ4eNyXDH",
        "outputId": "5dc2abe5-66ba-4643-d935-f39d589cbe8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 460\n",
            "Validation dataset size: 115\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BatEM055_OgL"
      },
      "source": [
        "# Supervised Fine-Tuning Trainer  initializing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "7c06b5d7d995455bb81d730c4d49b6d8",
            "36d688a13c27418c8561318eebeac0ad",
            "1559a1491cd5494fb717e362750a172e",
            "3debcacf3611464c8930f3a4b3eae6c4",
            "810dd1448c3d4c14af814becf36c8dfa",
            "4e74e3dcff3c473b9a2d057e806346f2",
            "ce10a91e3a554283a70216c1de6b46a7",
            "37736284b13542688de644a828a3eee1",
            "1094f598f9c04839a5b5325e151888a1",
            "981a237c2c22430f8a0b89d3de7561fb",
            "caab6a309aa847bda1cae15bd9cdcb2b",
            "6c78db8e89624a1aae3367586928764a",
            "04c18e0d922d4ce188895eca5fbb090f",
            "62a9a9a0ea2543079de5826e44af85af",
            "dce0b15af79c4be086fbe809ecf14d96",
            "363f8929f3344718b1088dbd8e5887da",
            "60baf0e09eec4564b93dafb84fde71d7",
            "a1f52768fa3849c2aa45b23b932a3bc9",
            "c1b7606920d3497ca4e0a116925cf453",
            "2141a172feb04386968d23bbf2f0898e",
            "af4ed2280a5742939b24c8de23b306ee",
            "1e149eda7488413b95f4bfb60f1b7abf"
          ]
        },
        "id": "L6YOqAxGyeYg",
        "outputId": "98803f3f-20e5-4960-e3ba-347b8f337b24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c06b5d7d995455bb81d730c4d49b6d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing to [\"text\"] (num_proc=3):   0%|          | 0/460 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c78db8e89624a1aae3367586928764a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing to [\"text\"] (num_proc=3):   0%|          | 0/115 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = eval_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 3,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 8,\n",
        "        warmup_steps = 20,\n",
        "        max_steps = 160,\n",
        "        learning_rate = 1e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=20,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVlo_M9A-UNt"
      },
      "source": [
        "# start fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "EpnLzNOAynbJ",
        "outputId": "1db12c6b-14a1-46ce-e5b4-bdaebaeae134"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 460 | Num Epochs = 12 | Total steps = 160\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 8 x 1) = 32\n",
            " \"-____-\"     Trainable parameters = 29,933,568/1,830,055,936 (1.64% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/160 12:08, Epoch 10/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.680300</td>\n",
              "      <td>2.013773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.239000</td>\n",
              "      <td>1.190110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.007400</td>\n",
              "      <td>1.062270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.937300</td>\n",
              "      <td>1.013381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.851800</td>\n",
              "      <td>0.989729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.800700</td>\n",
              "      <td>0.981624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.773800</td>\n",
              "      <td>0.981477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.769300</td>\n",
              "      <td>0.981282</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=160, training_loss=1.3132228642702102, metrics={'train_runtime': 734.2443, 'train_samples_per_second': 6.973, 'train_steps_per_second': 0.218, 'total_flos': 4917818704232448.0, 'train_loss': 1.3132228642702102})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upo8dSng0nV0",
        "outputId": "536f6966-1fb9-4a82-b662-1b25e052a045"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 2.4G\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 6.69 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:02<00:00, 17.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model/pytorch_model-00001-of-00002.bin...\n",
            "Unsloth: Saving model/pytorch_model-00002-of-00002.bin...\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Converting qwen2 model. Can use fast conversion = False.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at model into f16 GGUF format.\n",
            "The output location will be /content/model/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: model\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {2048, 151936}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.32.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.32.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.32.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.32.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.32.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.32.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.32.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.32.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.32.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.32.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.32.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.32.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.33.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.33.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.33.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.33.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.33.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.33.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.33.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.33.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.33.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.33.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.33.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.33.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.34.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.34.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.34.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.34.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.34.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.34.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.34.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.34.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.34.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.34.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.34.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.34.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.35.attn_q.bias,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.35.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.35.attn_k.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.35.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.35.attn_v.bias,        torch.float16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.35.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.35.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.35.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.35.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 11008}\n",
            "INFO:hf-to-gguf:blk.35.ffn_down.weight,    torch.float16 --> F16, shape = {11008, 2048}\n",
            "INFO:hf-to-gguf:blk.35.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.35.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 32768\n",
            "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 11008\n",
            "INFO:hf-to-gguf:gguf: head count = 16\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 2\n",
            "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "2025-03-09 15:12:29.419603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741533149.447993   19341 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741533149.459087   19341 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "INFO:gguf.vocab:Adding 151387 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type eos to 151645\n",
            "INFO:gguf.vocab:Setting special token type pad to 151654\n",
            "INFO:gguf.vocab:Setting add_bos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- messages[0]['content'] }}\n",
            "    {%- else %}\n",
            "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
            "    {%- endif %}\n",
            "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
            "    {%- else %}\n",
            "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {{- '\\n' + message.content }}\n",
            "        {%- endif %}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- if tool_call.function is defined %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
            "            {{- tool_call.name }}\n",
            "            {{- '\", \"arguments\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- '}\\n</tool_call>' }}\n",
            "        {%- endfor %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/model/unsloth.F16.gguf: n_tensors = 434, total_size = 6.2G\n",
            "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.17G/6.17G [01:11<00:00, 86.6Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/model/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 4857 (0fd7ca7a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/model/unsloth.F16.gguf' to '/content/model/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 26 key-value pairs and 434 tensors from /content/model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3b Instruct Unsloth Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct-unsloth-bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = qwen2.5\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          qwen2.block_count u32              = 36\n",
            "llama_model_loader: - kv   8:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  15:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ä  Ä \", \"Ä Ä  Ä Ä \", \"i n\", \"Ä  t\",...\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151654\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  181 tensors\n",
            "llama_model_loader: - type  f16:  253 tensors\n",
            "[   1/ 434]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   2/ 434]                    token_embd.weight - [ 2048, 151936,     1,     1], type =    f16, converting to q6_K .. size =   593.50 MiB ->   243.43 MiB\n",
            "[   3/ 434]                    blk.0.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[   4/ 434]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[   5/ 434]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   6/ 434]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   7/ 434]                    blk.0.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   8/ 434]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   9/ 434]                    blk.0.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  10/ 434]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[  11/ 434]                blk.0.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[  12/ 434]                blk.0.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  13/ 434]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  14/ 434]                  blk.0.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  15/ 434]                    blk.1.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  16/ 434]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  17/ 434]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  18/ 434]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  19/ 434]                    blk.1.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  20/ 434]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  21/ 434]                    blk.1.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  22/ 434]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[  23/ 434]                blk.1.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[  24/ 434]                blk.1.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  25/ 434]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  26/ 434]                  blk.1.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  27/ 434]                    blk.2.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  28/ 434]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  29/ 434]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  30/ 434]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  31/ 434]                    blk.2.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  32/ 434]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  33/ 434]                    blk.2.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  34/ 434]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[  35/ 434]                blk.2.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[  36/ 434]                blk.2.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  37/ 434]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  38/ 434]                  blk.2.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  39/ 434]                    blk.3.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  40/ 434]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  41/ 434]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  42/ 434]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  43/ 434]                    blk.3.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  44/ 434]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  45/ 434]                    blk.3.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  46/ 434]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[  47/ 434]                blk.3.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[  48/ 434]                blk.3.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  49/ 434]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  50/ 434]                  blk.3.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  51/ 434]                    blk.4.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  52/ 434]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  53/ 434]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  54/ 434]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  55/ 434]                    blk.4.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  56/ 434]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  57/ 434]                    blk.4.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  58/ 434]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  59/ 434]                blk.4.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  60/ 434]                blk.4.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  61/ 434]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  62/ 434]                  blk.4.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  63/ 434]                    blk.5.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  64/ 434]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  65/ 434]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  66/ 434]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  67/ 434]                    blk.5.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  68/ 434]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  69/ 434]                    blk.5.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  70/ 434]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  71/ 434]                blk.5.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  72/ 434]                blk.5.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  73/ 434]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  74/ 434]                  blk.5.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  75/ 434]                    blk.6.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  76/ 434]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  77/ 434]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  78/ 434]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  79/ 434]                    blk.6.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  80/ 434]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  81/ 434]                    blk.6.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  82/ 434]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[  83/ 434]                blk.6.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[  84/ 434]                blk.6.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  85/ 434]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  86/ 434]                  blk.6.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  87/ 434]                    blk.7.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  88/ 434]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  89/ 434]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  90/ 434]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  91/ 434]                    blk.7.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  92/ 434]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  93/ 434]                    blk.7.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  94/ 434]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  95/ 434]                blk.7.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  96/ 434]                blk.7.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  97/ 434]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  98/ 434]                  blk.7.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[  99/ 434]                    blk.8.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 100/ 434]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 101/ 434]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 102/ 434]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 103/ 434]                    blk.8.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 104/ 434]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 105/ 434]                    blk.8.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 106/ 434]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 107/ 434]                blk.8.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 108/ 434]                blk.8.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 109/ 434]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 110/ 434]                  blk.8.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 111/ 434]                    blk.9.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 112/ 434]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 113/ 434]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 114/ 434]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 115/ 434]                    blk.9.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 116/ 434]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 117/ 434]                    blk.9.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 118/ 434]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 119/ 434]                blk.9.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 120/ 434]                blk.9.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 121/ 434]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 122/ 434]                  blk.9.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 123/ 434]                   blk.10.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 124/ 434]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 125/ 434]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 126/ 434]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 127/ 434]                   blk.10.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 128/ 434]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 129/ 434]                   blk.10.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 130/ 434]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 131/ 434]               blk.10.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 132/ 434]               blk.10.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 133/ 434]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 134/ 434]                 blk.10.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 135/ 434]                   blk.11.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 136/ 434]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 137/ 434]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 138/ 434]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 139/ 434]                   blk.11.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 140/ 434]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 141/ 434]                   blk.11.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 142/ 434]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 143/ 434]               blk.11.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 144/ 434]               blk.11.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 145/ 434]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 146/ 434]                 blk.11.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 147/ 434]                   blk.12.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 148/ 434]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 149/ 434]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 150/ 434]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 151/ 434]                   blk.12.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 152/ 434]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 153/ 434]                   blk.12.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 154/ 434]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 155/ 434]               blk.12.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 156/ 434]               blk.12.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 157/ 434]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 158/ 434]                 blk.12.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 159/ 434]                   blk.13.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 160/ 434]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 161/ 434]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 162/ 434]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 163/ 434]                   blk.13.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 164/ 434]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 165/ 434]                   blk.13.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 166/ 434]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 167/ 434]               blk.13.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 168/ 434]               blk.13.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 169/ 434]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 170/ 434]                 blk.13.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 171/ 434]                   blk.14.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 172/ 434]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 173/ 434]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 174/ 434]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 175/ 434]                   blk.14.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 176/ 434]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 177/ 434]                   blk.14.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 178/ 434]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 179/ 434]               blk.14.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 180/ 434]               blk.14.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 181/ 434]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 182/ 434]                 blk.14.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 183/ 434]                   blk.15.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 184/ 434]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 185/ 434]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 186/ 434]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 187/ 434]                   blk.15.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 188/ 434]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 189/ 434]                   blk.15.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 190/ 434]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 191/ 434]               blk.15.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 192/ 434]               blk.15.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 193/ 434]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 194/ 434]                 blk.15.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 195/ 434]                   blk.16.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 196/ 434]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 197/ 434]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 198/ 434]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 199/ 434]                   blk.16.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 200/ 434]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 201/ 434]                   blk.16.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 202/ 434]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 203/ 434]               blk.16.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 204/ 434]               blk.16.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 205/ 434]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 206/ 434]                 blk.16.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 207/ 434]                   blk.17.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 208/ 434]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 209/ 434]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 210/ 434]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 211/ 434]                   blk.17.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 212/ 434]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 213/ 434]                   blk.17.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 214/ 434]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 215/ 434]               blk.17.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 216/ 434]               blk.17.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 217/ 434]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 218/ 434]                 blk.17.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 219/ 434]                   blk.18.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 220/ 434]                 blk.18.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 221/ 434]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 222/ 434]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 223/ 434]                   blk.18.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 224/ 434]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 225/ 434]                   blk.18.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 226/ 434]                 blk.18.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 227/ 434]               blk.18.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 228/ 434]               blk.18.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 229/ 434]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 230/ 434]                 blk.18.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 231/ 434]                   blk.19.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 232/ 434]                 blk.19.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 233/ 434]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 234/ 434]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 235/ 434]                   blk.19.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 236/ 434]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 237/ 434]                   blk.19.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 238/ 434]                 blk.19.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 239/ 434]               blk.19.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 240/ 434]               blk.19.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 241/ 434]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 242/ 434]                 blk.19.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 243/ 434]                   blk.20.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 244/ 434]                 blk.20.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 245/ 434]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 246/ 434]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 247/ 434]                   blk.20.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 248/ 434]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 249/ 434]                   blk.20.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 250/ 434]                 blk.20.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 251/ 434]               blk.20.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 252/ 434]               blk.20.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 253/ 434]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 254/ 434]                 blk.20.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 255/ 434]                   blk.21.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 256/ 434]                 blk.21.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 257/ 434]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 258/ 434]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 259/ 434]                   blk.21.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 260/ 434]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 261/ 434]                   blk.21.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 262/ 434]                 blk.21.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 263/ 434]               blk.21.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 264/ 434]               blk.21.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 265/ 434]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 266/ 434]                 blk.21.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 267/ 434]                   blk.22.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 268/ 434]                 blk.22.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 269/ 434]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 270/ 434]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 271/ 434]                   blk.22.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 272/ 434]                 blk.22.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 273/ 434]                   blk.22.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 274/ 434]                 blk.22.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 275/ 434]               blk.22.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 276/ 434]               blk.22.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 277/ 434]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 278/ 434]                 blk.22.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 279/ 434]                   blk.23.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 280/ 434]                 blk.23.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 281/ 434]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 282/ 434]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 283/ 434]                   blk.23.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 284/ 434]                 blk.23.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 285/ 434]                   blk.23.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 286/ 434]                 blk.23.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 287/ 434]               blk.23.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 288/ 434]               blk.23.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 289/ 434]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 290/ 434]                 blk.23.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 291/ 434]                   blk.24.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 292/ 434]                 blk.24.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 293/ 434]              blk.24.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 294/ 434]            blk.24.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 295/ 434]                   blk.24.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 296/ 434]                 blk.24.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 297/ 434]                   blk.24.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 298/ 434]                 blk.24.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 299/ 434]               blk.24.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 300/ 434]               blk.24.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 301/ 434]               blk.24.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 302/ 434]                 blk.24.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 303/ 434]                   blk.25.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 304/ 434]                 blk.25.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 305/ 434]              blk.25.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 306/ 434]            blk.25.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 307/ 434]                   blk.25.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 308/ 434]                 blk.25.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 309/ 434]                   blk.25.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 310/ 434]                 blk.25.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 311/ 434]               blk.25.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 312/ 434]               blk.25.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 313/ 434]               blk.25.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 314/ 434]                 blk.25.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 315/ 434]                   blk.26.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 316/ 434]                 blk.26.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 317/ 434]              blk.26.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 318/ 434]            blk.26.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 319/ 434]                   blk.26.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 320/ 434]                 blk.26.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 321/ 434]                   blk.26.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 322/ 434]                 blk.26.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 323/ 434]               blk.26.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 324/ 434]               blk.26.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 325/ 434]               blk.26.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 326/ 434]                 blk.26.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 327/ 434]                   blk.27.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 328/ 434]                 blk.27.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 329/ 434]              blk.27.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 330/ 434]            blk.27.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 331/ 434]                   blk.27.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 332/ 434]                 blk.27.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 333/ 434]                   blk.27.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 334/ 434]                 blk.27.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 335/ 434]               blk.27.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 336/ 434]               blk.27.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 337/ 434]               blk.27.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 338/ 434]                 blk.27.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 339/ 434]                   blk.28.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 340/ 434]                 blk.28.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 341/ 434]              blk.28.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 342/ 434]            blk.28.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 343/ 434]                   blk.28.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 344/ 434]                 blk.28.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 345/ 434]                   blk.28.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 346/ 434]                 blk.28.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 347/ 434]               blk.28.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 348/ 434]               blk.28.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 349/ 434]               blk.28.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 350/ 434]                 blk.28.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 351/ 434]                   blk.29.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 352/ 434]                 blk.29.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 353/ 434]              blk.29.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 354/ 434]            blk.29.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 355/ 434]                   blk.29.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 356/ 434]                 blk.29.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 357/ 434]                   blk.29.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 358/ 434]                 blk.29.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 359/ 434]               blk.29.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 360/ 434]               blk.29.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 361/ 434]               blk.29.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 362/ 434]                 blk.29.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 363/ 434]                   blk.30.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 364/ 434]                 blk.30.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 365/ 434]              blk.30.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 366/ 434]            blk.30.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 367/ 434]                   blk.30.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 368/ 434]                 blk.30.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 369/ 434]                   blk.30.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 370/ 434]                 blk.30.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 371/ 434]               blk.30.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 372/ 434]               blk.30.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 373/ 434]               blk.30.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 374/ 434]                 blk.30.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 375/ 434]                   blk.31.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 376/ 434]                 blk.31.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 377/ 434]              blk.31.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 378/ 434]            blk.31.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 379/ 434]                   blk.31.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 380/ 434]                 blk.31.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 381/ 434]                   blk.31.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 382/ 434]                 blk.31.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 383/ 434]               blk.31.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 384/ 434]               blk.31.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 385/ 434]               blk.31.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 386/ 434]                 blk.31.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 387/ 434]                   blk.32.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 388/ 434]                 blk.32.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 389/ 434]              blk.32.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 390/ 434]            blk.32.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 391/ 434]                   blk.32.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 392/ 434]                 blk.32.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 393/ 434]                   blk.32.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 394/ 434]                 blk.32.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 395/ 434]               blk.32.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 396/ 434]               blk.32.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 397/ 434]               blk.32.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 398/ 434]                 blk.32.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 399/ 434]                   blk.33.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 400/ 434]                 blk.33.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 401/ 434]              blk.33.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 402/ 434]            blk.33.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 403/ 434]                   blk.33.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 404/ 434]                 blk.33.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 405/ 434]                   blk.33.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 406/ 434]                 blk.33.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 407/ 434]               blk.33.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 408/ 434]               blk.33.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 409/ 434]               blk.33.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 410/ 434]                 blk.33.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 411/ 434]                   blk.34.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 412/ 434]                 blk.34.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 413/ 434]              blk.34.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 414/ 434]            blk.34.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 415/ 434]                   blk.34.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 416/ 434]                 blk.34.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 417/ 434]                   blk.34.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 418/ 434]                 blk.34.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 419/ 434]               blk.34.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 420/ 434]               blk.34.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 421/ 434]               blk.34.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 422/ 434]                 blk.34.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 423/ 434]                   blk.35.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 424/ 434]                 blk.35.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 425/ 434]              blk.35.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 426/ 434]            blk.35.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 427/ 434]                   blk.35.attn_q.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 428/ 434]                 blk.35.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 429/ 434]                   blk.35.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 430/ 434]                 blk.35.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 431/ 434]               blk.35.ffn_down.weight - [11008,  2048,     1,     1], type =    f16, converting to q6_K .. size =    43.00 MiB ->    17.64 MiB\n",
            "[ 432/ 434]               blk.35.ffn_gate.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "[ 433/ 434]               blk.35.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 434/ 434]                 blk.35.ffn_up.weight - [ 2048, 11008,     1,     1], type =    f16, converting to q4_K .. size =    43.00 MiB ->    12.09 MiB\n",
            "llama_model_quantize_impl: model size  =  5886.42 MB\n",
            "llama_model_quantize_impl: quant size  =  1834.82 MB\n",
            "\n",
            "main: quantize time = 353171.64 ms\n",
            "main:    total time = 353171.64 ms\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained_gguf(\"model\",tokenizer,quantization_method = \"q4_k_m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbFuGvRxgzWH",
        "outputId": "66121f0d-6e02-42ec-9968-2ac33eb24c4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 14G\n",
            "-rw-r--r-- 1 root root  605 Mar  9 15:08 added_tokens.json\n",
            "-rw-r--r-- 1 root root  808 Mar  9 15:08 config.json\n",
            "-rw-r--r-- 1 root root  266 Mar  9 15:08 generation_config.json\n",
            "-rw-r--r-- 1 root root 1.6M Mar  9 15:08 merges.txt\n",
            "-rw-r--r-- 1 root root 4.7G Mar  9 15:09 pytorch_model-00001-of-00002.bin\n",
            "-rw-r--r-- 1 root root 1.2G Mar  9 15:09 pytorch_model-00002-of-00002.bin\n",
            "-rw-r--r-- 1 root root  35K Mar  9 15:09 pytorch_model.bin.index.json\n",
            "-rw-r--r-- 1 root root  614 Mar  9 15:08 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 7.2K Mar  9 15:08 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Mar  9 15:08 tokenizer.json\n",
            "-rw-r--r-- 1 root root 5.8G Mar  9 15:13 unsloth.F16.gguf\n",
            "-rw-r--r-- 1 root root 1.8G Mar  9 15:19 unsloth.Q4_K_M.gguf\n",
            "-rw-r--r-- 1 root root 2.7M Mar  9 15:08 vocab.json\n"
          ]
        }
      ],
      "source": [
        "!ls -lh /content/model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rQUykL09m76"
      },
      "source": [
        "# inference script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ1ySvUayn6r",
        "outputId": "17f02309-f1c8-4be5-c866-399a5a1124a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nwhat is HAI-LLM ?\\n\\n### Input:\\n\\n\\n### Response:\\nHigh-Performance AI Language Model<|im_end|>']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"what is HAI-LLM ?\", # instruction\n",
        "        \"\", # input  - leave this blank\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoBWVuRI9u5y"
      },
      "source": [
        "# Drive upload gguf file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "sdbVNXk11Kit",
        "outputId": "b8749440-c11e-45c3-8ea0-c0877a1bd5e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/ModelFolder/unsloth.Q4_K_M.gguf'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil\n",
        "\n",
        "\n",
        "model_path = '/content/model/unsloth.Q4_K_M.gguf'\n",
        "drive_folder = '/content/drive/My Drive/ModelFolder/'\n",
        "\n",
        "shutil.move(model_path, drive_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwdKOKO_iZNr",
        "outputId": "ae54d442-edf3-4c49-d271-1f53b2c700e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Evaluation dataset saved at: /content/drive/MyDrive/eval_dataset.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# âœ… Convert the dataset to a list\n",
        "eval_data_list = eval_dataset.to_list()\n",
        "\n",
        "# âœ… Define the save path in Google Drive\n",
        "eval_dataset_path = \"/content/drive/MyDrive/eval_dataset.json\"\n",
        "\n",
        "# âœ… Save as JSON\n",
        "with open(eval_dataset_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(eval_data_list, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"âœ… Evaluation dataset saved at: {eval_dataset_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1Ugd1cwnH0V"
      },
      "source": [
        "# Obtaining Sample Responses from the Finetuned Model For the evaluation dataset Instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSACaLSznHjJ",
        "outputId": "0b926186-cbf1-4e81-d9a4-f56f2c756001"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Loaded 115 evaluation samples.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "# âœ… Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# âœ… Load Evaluation Dataset\n",
        "eval_dataset_path = \"/content/drive/MyDrive/eval_dataset.json\"\n",
        "\n",
        "with open(eval_dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    eval_data = json.load(file)\n",
        "\n",
        "print(f\"âœ… Loaded {len(eval_data)} evaluation samples.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NSkgHJlHnsvq"
      },
      "outputs": [],
      "source": [
        "# âœ… Define the Alpaca Prompt Format\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Shp_NUZnvQQ",
        "outputId": "e619831a-437e-4e22-e671-aef1d1be1f19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Responses generated for all evaluation samples!\n"
          ]
        }
      ],
      "source": [
        "# âœ… Generate responses for each evaluation sample\n",
        "for item in eval_data:\n",
        "    instruction = item[\"Instruction\"]\n",
        "\n",
        "    # âœ… Format the input prompt\n",
        "    inputs = tokenizer(\n",
        "        [alpaca_prompt.format(instruction, \"\", \"\")],  # Empty input/output\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # âœ… Generate response\n",
        "    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    # âœ… Clean up response\n",
        "    response = response.replace(EOS_TOKEN, \"\").strip()\n",
        "\n",
        "    # âœ… Store the generated response in the dataset\n",
        "    item[\"Generated_Response\"] = response\n",
        "\n",
        "print(\"âœ… Responses generated for all evaluation samples!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaUUpIMuokGZ",
        "outputId": "150eb44b-b499-42f9-f461-0756c915c892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Evaluation results saved at: /content/drive/MyDrive/eval_results.json\n"
          ]
        }
      ],
      "source": [
        "# âœ… Define save path\n",
        "eval_results_path = \"/content/drive/MyDrive/eval_results.json\"\n",
        "\n",
        "# âœ… Save the updated dataset\n",
        "with open(eval_results_path, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(eval_data, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"âœ… Evaluation results saved at: {eval_results_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04c18e0d922d4ce188895eca5fbb090f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60baf0e09eec4564b93dafb84fde71d7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a1f52768fa3849c2aa45b23b932a3bc9",
            "value": "Tokenizingâ€‡toâ€‡[&quot;text&quot;]â€‡(num_proc=3):â€‡100%"
          }
        },
        "0dda57e28b21478c96b8e782cf5d55cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1094f598f9c04839a5b5325e151888a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1559a1491cd5494fb717e362750a172e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37736284b13542688de644a828a3eee1",
            "max": 460,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1094f598f9c04839a5b5325e151888a1",
            "value": 460
          }
        },
        "1e149eda7488413b95f4bfb60f1b7abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2141a172feb04386968d23bbf2f0898e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3292f898e38644f1b67d4d5c5d2d5365": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "355c837c137149b9b2eea0ba3f33d7df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "363f8929f3344718b1088dbd8e5887da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36d688a13c27418c8561318eebeac0ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e74e3dcff3c473b9a2d057e806346f2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ce10a91e3a554283a70216c1de6b46a7",
            "value": "Tokenizingâ€‡toâ€‡[&quot;text&quot;]â€‡(num_proc=3):â€‡100%"
          }
        },
        "37736284b13542688de644a828a3eee1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aa369565c8b49be9c78eb88b2d28b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccb62ce90e8a4bc3b37dbda22483a73d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b64afa1b6c6a45da890b935dedd6a2ef",
            "value": "Mapâ€‡(num_proc=2):â€‡100%"
          }
        },
        "3debcacf3611464c8930f3a4b3eae6c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_981a237c2c22430f8a0b89d3de7561fb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_caab6a309aa847bda1cae15bd9cdcb2b",
            "value": "â€‡460/460â€‡[00:03&lt;00:00,â€‡91.61â€‡examples/s]"
          }
        },
        "43ca1befec1b4f43b13211a1ada52e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e74e3dcff3c473b9a2d057e806346f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aaf84a0a9e44eccb581021a489742b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60baf0e09eec4564b93dafb84fde71d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62a9a9a0ea2543079de5826e44af85af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1b7606920d3497ca4e0a116925cf453",
            "max": 115,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2141a172feb04386968d23bbf2f0898e",
            "value": 115
          }
        },
        "6c78db8e89624a1aae3367586928764a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04c18e0d922d4ce188895eca5fbb090f",
              "IPY_MODEL_62a9a9a0ea2543079de5826e44af85af",
              "IPY_MODEL_dce0b15af79c4be086fbe809ecf14d96"
            ],
            "layout": "IPY_MODEL_363f8929f3344718b1088dbd8e5887da"
          }
        },
        "6eb3fb9726bf4822865d3e5b51ef9c85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71bc85155e5a4ae8a0ae037034a3e6ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ca6dad475944ba5a556bc68c987d3c1",
              "IPY_MODEL_84218c043677456f9299390a9aa3c865",
              "IPY_MODEL_88c7da6cdb9c43aeb2f4a9d1e1704937"
            ],
            "layout": "IPY_MODEL_f70ec2bb692c46f4ac6866b61718f0b2"
          }
        },
        "7620fdc8b798431a884c3f2e520e3db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6eb3fb9726bf4822865d3e5b51ef9c85",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d38dd0740c924a23b04bd824363822dc",
            "value": "â€‡460/460â€‡[00:00&lt;00:00,â€‡666.59â€‡examples/s]"
          }
        },
        "7b80cbacc24f413baf02a88fc6043fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dda57e28b21478c96b8e782cf5d55cf",
            "max": 460,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad6ea5b3df434bc5b88ed6b5d8780e01",
            "value": 460
          }
        },
        "7c06b5d7d995455bb81d730c4d49b6d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36d688a13c27418c8561318eebeac0ad",
              "IPY_MODEL_1559a1491cd5494fb717e362750a172e",
              "IPY_MODEL_3debcacf3611464c8930f3a4b3eae6c4"
            ],
            "layout": "IPY_MODEL_810dd1448c3d4c14af814becf36c8dfa"
          }
        },
        "7ca6dad475944ba5a556bc68c987d3c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aaf84a0a9e44eccb581021a489742b8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b25bd95f54134bb0a9b53c30a6f6f0c1",
            "value": "Mapâ€‡(num_proc=2):â€‡100%"
          }
        },
        "7cc8aa0d80a2444295042fb124c4cbc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3aa369565c8b49be9c78eb88b2d28b23",
              "IPY_MODEL_7b80cbacc24f413baf02a88fc6043fac",
              "IPY_MODEL_7620fdc8b798431a884c3f2e520e3db4"
            ],
            "layout": "IPY_MODEL_355c837c137149b9b2eea0ba3f33d7df"
          }
        },
        "810dd1448c3d4c14af814becf36c8dfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82997670bb684437aa9c1779c6592835": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84218c043677456f9299390a9aa3c865": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82997670bb684437aa9c1779c6592835",
            "max": 115,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43ca1befec1b4f43b13211a1ada52e69",
            "value": 115
          }
        },
        "88c7da6cdb9c43aeb2f4a9d1e1704937": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3292f898e38644f1b67d4d5c5d2d5365",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9f5965b5c00945e98948197b0ca0312b",
            "value": "â€‡115/115â€‡[00:00&lt;00:00,â€‡192.45â€‡examples/s]"
          }
        },
        "981a237c2c22430f8a0b89d3de7561fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f5965b5c00945e98948197b0ca0312b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1f52768fa3849c2aa45b23b932a3bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad6ea5b3df434bc5b88ed6b5d8780e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af4ed2280a5742939b24c8de23b306ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b25bd95f54134bb0a9b53c30a6f6f0c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b64afa1b6c6a45da890b935dedd6a2ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1b7606920d3497ca4e0a116925cf453": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caab6a309aa847bda1cae15bd9cdcb2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ccb62ce90e8a4bc3b37dbda22483a73d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce10a91e3a554283a70216c1de6b46a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d38dd0740c924a23b04bd824363822dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dce0b15af79c4be086fbe809ecf14d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af4ed2280a5742939b24c8de23b306ee",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1e149eda7488413b95f4bfb60f1b7abf",
            "value": "â€‡115/115â€‡[00:02&lt;00:00,â€‡44.72â€‡examples/s]"
          }
        },
        "f70ec2bb692c46f4ac6866b61718f0b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
